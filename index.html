<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <link rel="icon" href="assets/images/artificial-intelligence.png" type="image/icon type">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1.0, shrink-to-fit=no">
  <link href="assets/images/favicon.png" rel="icon" />
  <title>GB Documentation | graph_brain documentation</title>
  <meta name="description" content="Graph Neural Network Library">
  <meta name="author" content="harnishdesign.net">


  <!-- Stylesheet
============================== -->
  <!-- Bootstrap -->
  <link rel="stylesheet" type="text/css" href="assets/vendor/bootstrap/css/bootstrap.min.css" />
  <!-- Font Awesome Icon -->
  <link rel="stylesheet" type="text/css" href="assets/vendor/font-awesome/css/all.min.css" />
  <!-- Magnific Popup -->
  <link rel="stylesheet" type="text/css" href="assets/vendor/magnific-popup/magnific-popup.min.css" />
  <!-- Highlight Syntax -->
  <link rel="stylesheet" type="text/css" href="assets/vendor/highlight.js/styles/github.css" />
  <!-- Custom Stylesheet -->
  <link rel="stylesheet" type="text/css" href="assets/css/stylesheet.css" />
</head>

<body data-spy="scroll" data-target=".idocs-navigation" data-offset="125">

  <!-- Preloader -->
  <div class="preloader">
    <div class="lds-ellipsis">
      <div></div>
      <div></div>
      <div></div>
      <div></div>
    </div>
  </div>
  <!-- Preloader End -->

  <!-- Document Wrapper   
=============================== -->
  <div id="main-wrapper">

    <!-- Header
  ============================ -->
    <header id="header" class="sticky-top">
      <!-- Navbar -->
      <nav class="primary-menu navbar navbar-expand-lg navbar-dropdown-dark">
        <div class="container-fluid">
          <!-- Sidebar Toggler -->
          <button id="sidebarCollapse" class="navbar-toggler d-block d-md-none" type="button"><span></span><span
              class="w-75"></span><span class="w-50"></span></button>

          <!-- Logo -->
          <a class="logo ml-md-3" href="index.html" title="GB documentation"> <strong
              style="color: black ; font-size:1.5vw; line-height: 21px">Graph<br> Brain</strong> </a>
          <span class="Logo"><img src="assets/images/artificial-intelligence.png" alt=""></span>
          <!-- Logo End -->

          <!-- Navbar Toggler -->
          <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse"
            data-target="#header-nav"><span></span><span></span><span></span></button>

          <div id="header-nav" class="collapse navbar-collapse justify-content-end">
             
                
            
            
          </div>
          <ul class="social-icons social-icons-sm ml-lg-2 mr-2">
            <li><a data-toggle="tooltip" href="https://github.com/Ritaj19Zamel/Layers" target="_blank" title=""
              data-original-title="GitHub"><i class="fab fa-github"></i></a></li>
          </ul>
        </div>
      </nav>
      <!-- Navbar End -->
    </header>
    <!-- Header End -->

    <!-- Content
  ============================ -->
    <div id="content" role="main">

      <!-- Sidebar Navigation
	============================ -->
      <div class="idocs-navigation bg-light">
        <ul class="nav flex-column ">
          <li class="nav-item"><a class="nav-link active" href="#idocs_start">Getting Started</a>
   
          </li>
          <li class="nav-item"><a class="nav-link" href="#idocs_installation">Installation</a></li>
          <li class="nav-item"><a class="nav-link" href="#idocs_layers">Layers</a>
          <li class="nav-item"><a class="nav-link" href="#idocs_conv">Convolutional Layers</a></li>
          <li class="nav-item"><a class="nav-link" href="#idocs_aggr">Aggregation Operators</a></li>
          <li class="nav-item"><a class="nav-link" href="#idocs_norm">Normalization Layers</a></li>
          <li class="nav-item"><a class="nav-link" href="#idocs_pool">Pooling Layers</a></li>
          <li class="nav-item"><a class="nav-link" href="#idocs_un_pool">Unpooling Layers</a></li>
         
        </ul>
      </div>

      <!-- Docs Content
	============================ -->
      <div class="idocs-content">
        <div class="container">

          <!-- Getting Started
		============================ -->
          <section id="idocs_start">
            <div class="header_section">
              <div class="header_overlay">
                <div id="particles-js">
                  <h1>Graph Brain</h1>
                  <a href="https://github.com/Ritaj19Zamel/Layers" target="_blank">GitHub</a>
                  <!-- <p>Graph Brain is a library help in writing and training Graph Neural Networks (GNNs) for a wide range of applications related to structured data.</p> -->
                  <!-- <p class="lead">Thank you so much for purchasing our item from themeforest.</p> -->
                </div>
              </div>
      
            </div>
          
            <hr>
            <div class="row">
              <p style="color: black;"><strong><mark>Graph Brain</mark> is a library that helps in writing and training Graph Neural Networks (GNNs) for a wide range of applications related to structured data.</strong></p>
              <div class="col-sm-6 col-lg-4">
                
                <ul class="list-unstyled"> 
                  <li><strong>Version:</strong> 1.0</li>
                  <li><strong>Author:</strong> <a href="#" target="_blank">Binary Brains
                      Team</a></li>
                 
                </ul>
              </div>
              <div class="col-sm-6 col-lg-4">
                <ul class="list-unstyled">
                  <li><strong class="font-weight-700">Created:</strong> 1 Nov, 2022</li>
                  
                  
                </ul>
              </div>
            </div>
            
          </section>

          <hr class="divider">

          <!-- Installation
		============================ -->
          <section id="idocs_installation">
            <h2>Installation</h2>
            <p>Graph Brain is tested and supported on the following 64-bit systems:</p>
            <ul><li>Python 3.7–3.10</li>
              <li>macOS 10.12.6</li>
              <li>Ubuntu 16.04 or later</li>
              <li>Windows 7 or later</li>
            </ul>
            <p class="lead"><strong>We have outsourced a lot of functionality of Graph Brain to other packages, which needs to be installed in advance.</strong></p>
            <ol>
              <li>Install Tensorflow Package.</li>
              <p>pip install Tensorflow</p>
              <li>Install Numpy Package.</li>
              <p>pip install Numpy</p>
              <li>Install <a href="https://www.dgl.ai/pages/start.html" target="_blank">DGL</a></li>
              
              <li>Then, install Graph Binary with pip.</li>
              <p>pip install graph_brain</p>
            </ol>
          </section>

          <hr class="divider">






          <!-- Layers
		============================ -->
          <section id="idocs_layers">
            <h2>Layers</h2>
            <p class="lead mb-5">Graph Brain Layers</p>
            <p class="lead">
              <ul class="simple">
                <li>
                  <p>
                    <a class="reference internal" href="#idocs_conv">Convolutional Layers</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#idocs_aggr">Aggregation Operators</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#idocs_norm">Normalization Layers</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#idocs_pool">Pooling Layer</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#idocs_un_pool">UnPooling Layers</a>
                  </p>
                </li>
              </ul>
            </p>
          </section>


          <!-- Convolutional Layers
		============================ -->
          <section id="idocs_conv">
            <h2>Convolutional Layers</h2>
            <p class="lead">
              <ul class="simple">
                <li>
                  <p>
                    <a class="reference internal" href="#msg">Message Passing</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#chxonv">ChebConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#sagconv">SAGEConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#graphconv">GraphConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#gravconv">GravNetConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#gatconv">GATConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#ginconv">GINConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#sgconv">SGConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#appconv">APPNP</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#edgeconv">EdgeConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#leconv">LEConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#rgcconv">RelGraphConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#cg_conv">CGConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#sign_conv">SignedConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#film_conv">FiLMConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#agnn_conv">AGNNConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#lg_conv">LGConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#cluster_conv">ClusterGCNConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#wl_cont_conv">WLConvContinuous</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#res_gated_conv">ResGatedGraphConv</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#hyper_graph_conv">HypergraphConv</a>
                  </p>
                </li>
              </ul>
              </p>

                              
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="msgpass">MessagePassing</span>( aggr: Optional[Union[str, List[str], Aggregation]] = 'add', *, aggr_kwargs: Optional[Dict[str, Any]] = None, flow: str = 'source_to_target', node_dim: int = -2, decomposed_layers: int = 1, **kwargs) </p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#msg_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="msg_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR MESSAGR_PASSING</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            import os
                            import os.path as osp
                            import re
                            from collections import OrderedDict
                            from itertools import chain
                            from uuid import uuid1
                            from collections import OrderedDict
                            import importlib
                            import sys
                            from getpass import getuser
                            from importlib.util import module_from_spec, spec_from_file_location
                            from tempfile import NamedTemporaryFile as TempFile
                            from tempfile import gettempdir
                            import hooks
                            import jit
                            from tf_typing import *
                            import makedirs
                            from Inspector import *
                            from base import Aggregation
                            from Multi import MultiAggregation
                            from basic import *
                            from typing import (
                                Any,
                                Callable,
                                Dict,
                                List,
                                Optional,
                                Set,
                                Union,
                                get_type_hints,
                            )
                            from resolver import *
                            from tensorflow.sparse import SparseTensor
                            from uuid import uuid1
                            from hooks import RemovableHandle
                            from inspect import Parameter
                            FUSE_AGGRS = {'add', 'sum', 'mean', 'min', 'max'}

                            def tf_index_select(input_, dim, indices):
                                """
                                input_(tensor): input tensor
                                dim(int): dimension
                                indices(list): selected indices list
                                """
                                shape = input_.get_shape().as_list()
                                if dim == -1:
                                    dim = len(shape)-1
                                shape[dim] = 1
                                
                                tmp = []
                                for idx in indices:
                                    begin = [0]*len(shape)
                                    begin[dim] = idx
                                    tmp.append(tf.slice(input_, begin, shape))
                                res = tf.concat(tmp, axis=dim)
                                
                                return res
                            class MessagePassing(tf.keras.layers.Layer):

                                special_args: Set[str] = {'edge_index', 'adj_t', 'edge_index_i', 'edge_index_j', 'size',
                                    'size_i', 'size_j', 'ptr', 'index', 'dim_size'}

                                def __init__(self,aggr: Optional[Union[str, List[str], Aggregation]] = "add",*,
                                    aggr_kwargs: Optional[Dict[str, Any]] = None,flow: str = "source_to_target",
                                    node_dim: int = -2,decomposed_layers: int = 1):
                                    super().__init__()

                                    if aggr is None:
                                        self.aggr = None
                                        self.aggr_module = None
                                    elif isinstance(aggr, (str, Aggregation)):
                                        self.aggr = str(aggr)
                                        self.aggr_module = aggregation_resolver(aggr, **(aggr_kwargs or {}))
                                    elif isinstance(aggr, (tuple, list)):
                                        self.aggr = [str(x) for x in aggr]
                                        self.aggr_module = MultiAggregation(aggr, **(aggr_kwargs or {}))
                                    else:
                                        raise ValueError(
                                            f"Only strings, list, tuples and instances of"
                                            f"`tf.base.Aggregation` are "
                                            f"valid aggregation schemes (got '{type(aggr)}').")

                                    self.flow = flow

                                    if flow not in ['source_to_target', 'target_to_source']:
                                        raise ValueError(f"Expected 'flow' to be either 'source_to_target'"
                                                        f" or 'target_to_source' (got '{flow}')")

                                    self.node_dim = node_dim
                                    self.decomposed_layers = decomposed_layers

                                    self.inspector = Inspector(self)
                                    self.inspector.inspect(self.message)
                                    self.inspector.inspect(self.aggregate, pop_first=True)
                                    self.inspector.params['aggregate'].pop('aggr', None)
                                    self.inspector.inspect(self.message_and_aggregate, pop_first=True)
                                    self.inspector.inspect(self.update, pop_first=True)
                                    self.inspector.inspect(self.edge_update)

                                    self.__user_args__ = self.inspector.keys(
                                        ['message', 'aggregate', 'update']).difference(self.special_args)
                                    self.__fused_user_args__ = self.inspector.keys(
                                        ['message_and_aggregate', 'update']).difference(self.special_args)
                                    self.__edge_user_args__ = self.inspector.keys(
                                        ['edge_update']).difference(self.special_args)

                                    # Support for "fused" message passing.
                                    self.fuse = self.inspector.implements('message_and_aggregate')
                                    if self.aggr is not None:
                                        self.fuse &= isinstance(self.aggr, str) and self.aggr in FUSE_AGGRS

                                    # Support for explainability.
                                    self._explain = False
                                    self._edge_mask = None
                                    self._loop_mask = None
                                    self._apply_sigmoid = True

                                    # Hooks:
                                    self._propagate_forward_pre_hooks = OrderedDict()
                                    self._propagate_forward_hooks = OrderedDict()
                                    self._message_forward_pre_hooks = OrderedDict()
                                    self._message_forward_hooks = OrderedDict()
                                    self._aggregate_forward_pre_hooks = OrderedDict()
                                    self._aggregate_forward_hooks = OrderedDict()
                                    self._message_and_aggregate_forward_pre_hooks = OrderedDict()
                                    self._message_and_aggregate_forward_hooks = OrderedDict()
                                    self._edge_update_forward_pre_hooks = OrderedDict()
                                    self._edge_update_forward_hooks = OrderedDict()
                                    
                                def __check_input__(self , edge_index , size):
                                    the_size : List[Optional[int]] = [None , None]

                                    if isinstance(edge_index , tf.Tensor):
                                        int_dtypes = (tf.uint8 , tf.int8 , tf.int32 , tf.int64)

                                        if edge_index.dtype not in int_dtypes:
                                            raise ValueError(f"Expected 'edge_index' to be of integer "
                                                                f"type (got '{edge_index.dtype}')")
                                        if np.ndim(edge_index) != 2 :
                                            raise ValueError(f"Expected 'edge_index' to be two-dimensional"
                                                            f" (got {np.ndim(edge_index)} dimensions)")
                                        if edge_index.get_shape()[0] != 2 :
                                            raise ValueError(f"Expected 'edge_index' to have size '2' in "
                                                                f"the first dimension (got "
                                                                f"'{tf.size(edge_index).numpy()}')")

                                        if size is not None:
                                            the_size[0] = size[0]
                                            the_size[1] = size[1]
                                        return the_size

                                    elif isinstance(edge_index, SparseTensor):
                                        if self.flow == 'target_to_source':
                                                raise ValueError(
                                                    ('Flow direction "target_to_source" is invalid for '
                                                    'message propagation via `tensorflow.SparseTensor`. If '
                                                    'you really want to make use of a reverse message '
                                                    'passing flow, pass in the transposed sparse tensor to '
                                                    'the message passing module, e.g., `adj_t.t()`.'))       
                                        the_size[0] = tf.size(edge_index[1]).numpy()
                                        the_size[1] = tf.size(edge_index[0]).numpy()  
                                        return the_size 

                                    raise ValueError(
                                            ('`MessagePassing.propagate` only supports integer tensors of '
                                            'shape `[2, num_messages]` or `tensorflow.SparseTensor` for '
                                            'argument `edge_index`.'))

                                def __set_size__(self, size: List[Optional[int]], dim: int, src: tf.Tensor):
                                    the_size = size[dim]
                                    if the_size is None:
                                        size[dim] = tf.shape(src).numpy()[0]
                                    elif the_size != tf.shape(src).numpy()[0]:
                                        raise ValueError(
                                            (f'Encountered tensor with size {tf.shape(src).numpy()[0]} in '
                                            f'dimension {self.node_dim}, but expected size {the_size}.'))
                                def __lift__(self, src, edge_index, dim):
                                        if isinstance(edge_index, tf.Tensor):
                                            try:
                                                index = edge_index.numpy()[dim]
                                                return tf_index_select(src,self.node_dim, index)
                                            except (IndexError, RuntimeError) as e:
                                                if 'CUDA' in str(e):
                                                    raise ValueError(
                                                        f"Encountered a CUDA error. Please ensure that all "
                                                        f"indices in 'edge_index' point to valid indices "
                                                        f"in the interval [0, {src.get_shape()[self.node_dim]}) in "
                                                        f"your node feature matrix and try again.")

                                                if tf.size(index) > 0 and tf.reduce_min(index) < 0:
                                                    raise ValueError(
                                                        f"Found negative indices in 'edge_index' (got "
                                                        f"{int(tf.reduce_min(index))}). Please ensure that all "
                                                        f"indices in 'edge_index' point to valid indices "
                                                        f"in the interval [0, {src.get_shape()[self.node_dim]}) in "
                                                        f"your node feature matrix and try again.")

                                                if (tf.size(index) > 0
                                                        and tf.reduce_max(index) >= src.get_shape()[self.node_dim]):
                                                    raise ValueError(
                                                        f"Found indices in 'edge_index' that are larger "
                                                        f"than {int(src.get_shape()[self.node_dim]) - 1} (got "
                                                        f"{int(tf.reduce_max(index))}). Please ensure that all "
                                                        f"indices in 'edge_index' point to valid indices "
                                                        f"in the interval [0, {int(src.get_shape()[self.node_dim])}) in "
                                                        f"your node feature matrix and try again.")

                                                raise e
                            ################################## there is problem in convert gather_csr to tensorflow ##################################
                            #                     elif isinstance(edge_index, SparseTensor):
                            #                                 if dim == 1:
                            #                                     rowptr = edge_index.storage.rowptr()
                            #                                     rowptr = expand_left(rowptr, dim=self.node_dim, dims=src.dim())
                            #                                     return gather_csr(src, rowptr)
                            #                                 elif dim == 0:
                            #                                     col = edge_index.storage.col()
                            #                                     return src.index_select(self.node_dim, col)

                            #                             raise ValueError(
                            #                                 ('`MessagePassing.propagate` only supports integer tensors of '
                            #                                  'shape `[2, num_messages]` or `torch_sparse.SparseTensor` for '
                            #                                  'argument `edge_index`.'))
                            ###############################################################################################################################

                                def __collect__(self, args, edge_index, size, kwargs):
                                    i, j = (1, 0) if self.flow == 'source_to_target' else (0, 1)

                                    out = {}
                                    for arg in args:
                                        if arg[-2:] not in ['_i', '_j']:
                                            out[arg] = kwargs.get(arg, Parameter.empty)
                                        else:
                                            dim = j if arg[-2:] == '_j' else i
                                            data = kwargs.get(arg[:-2], Parameter.empty)

                                            if isinstance(data, (tuple, list)):
                                                assert len(data) == 2
                                                if isinstance(data[1 - dim], tf.Tensor):
                                                    self.__set_size__(size, 1 - dim, data[1 - dim])
                                                data = data[dim]

                                            if isinstance(data, tf.Tensor):
                                                self.__set_size__(size, dim, data)
                                                data = self.__lift__(data, edge_index, dim)

                                            out[arg] = data

                                    if isinstance(edge_index, tf.Tensor):
                                        out['adj_t'] = None
                                        out['edge_index'] = edge_index
                                        out['edge_index_i'] = edge_index[i]
                                        out['edge_index_j'] = edge_index[j]
                                        out['ptr'] = None

                                ################################## there is problem in convert torch.storage to tensorflow ##################################

                                #     elif isinstance(edge_index, tf.SparseTensor):
                                #         out['adj_t'] = edge_index
                                #         out['edge_index'] = None
                                #         out['edge_index_i'] = edge_index.storage.row()
                                #         out['edge_index_j'] = edge_index.storage.col()
                                #         out['ptr'] = edge_index.storage.rowptr()
                                #         if out.get('edge_weight', None) is None:
                                #             out['edge_weight'] = edge_index.storage.value()
                                #         if out.get('edge_attr', None) is None:
                                #             out['edge_attr'] = edge_index.storage.value()
                                #         if out.get('edge_type', None) is None:
                                #             out['edge_type'] = edge_index.storage.value()

                                ###############################################################################################################################

                                    out['index'] = out['edge_index_i']
                                    out['size'] = size
                                    out['size_i'] = size[i] if size[i] is not None else size[j]
                                    out['size_j'] = size[j] if size[j] is not None else size[i]
                                    out['dim_size'] = out['size_i']

                                    return out

                                def propagate(self, edge_index:Union[tf.Tensor, tf.SparseTensor] , size: Optional[Tuple[int, int]] = None, **kwargs):

                                        decomposed_layers = 1 if self.explain else self.decomposed_layers

                                        for hook in self._propagate_forward_pre_hooks.values():
                                            res = hook(self, (edge_index, size, kwargs))
                                            if res is not None:
                                                edge_index, size, kwargs = res

                                        size = self.__check_input__(edge_index, size)


                                        if (isinstance(edge_index, tf.SparseTensor) and self.fuse
                                                and not self.explain):
                                            coll_dict = self.__collect__(self.__fused_user_args__, edge_index,
                                                                        size, kwargs)

                                            msg_aggr_kwargs = self.inspector.distribute(
                                                'message_and_aggregate', coll_dict)
                                            for hook in self._message_and_aggregate_forward_pre_hooks.values():
                                                res = hook(self, (edge_index, msg_aggr_kwargs))
                                                if res is not None:
                                                    edge_index, msg_aggr_kwargs = res
                                            out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs)
                                            for hook in self._message_and_aggregate_forward_hooks.values():
                                                res = hook(self, (edge_index, msg_aggr_kwargs), out)
                                                if res is not None:
                                                    out = res

                                            update_kwargs = self.inspector.distribute('update', coll_dict)
                                            out = self.update(out, **update_kwargs)

                                        else:  # Otherwise, run both functions in separation.
                                            if decomposed_layers > 1:
                                                user_args = self.__user_args__
                                                decomp_args = {a[:-2] for a in user_args if a[-2:] == '_j'}
                                                decomp_kwargs = {

                                                    a: np.array_split((kwargs[a]), decomposed_layers)
                                                    for a in decomp_args
                                                }
                                                decomp_out = []

                                            for i in range(decomposed_layers):
                                                if decomposed_layers > 1:
                                                    for arg in decomp_args:
                                                        kwargs[arg] = decomp_kwargs[arg][i]

                                                coll_dict = self.__collect__(self.__user_args__, edge_index,
                                                                            size, kwargs)

                                                msg_kwargs = self.inspector.distribute('message', coll_dict)
                                                for hook in self._message_forward_pre_hooks.values():
                                                    res = hook(self, (msg_kwargs, ))
                                                    if res is not None:
                                                        msg_kwargs = res[0] if isinstance(res, tuple) else res
                                                out = self.message(**msg_kwargs)
                                                for hook in self._message_forward_hooks.values():
                                                    res = hook(self, (msg_kwargs, ), out)
                                                    if res is not None:
                                                        out = res

                                                if self.explain:
                                                    explain_msg_kwargs = self.inspector.distribute(
                                                        'explain_message', coll_dict)
                                                    out = self.explain_message(out, **explain_msg_kwargs)

                                                aggr_kwargs = self.inspector.distribute('aggregate', coll_dict)
                                                for hook in self._aggregate_forward_pre_hooks.values():
                                                    res = hook(self, (aggr_kwargs, ))
                                                    if res is not None:
                                                        aggr_kwargs = res[0] if isinstance(res, tuple) else res

                                                out = self.aggregate(out, **aggr_kwargs)

                                                for hook in self._aggregate_forward_hooks.values():
                                                    res = hook(self, (aggr_kwargs, ), out)
                                                    if res is not None:
                                                        out = res

                                                update_kwargs = self.inspector.distribute('update', coll_dict)
                                                out = self.update(out, **update_kwargs)

                                                if decomposed_layers > 1:
                                                    decomp_out.append(out)

                                            if decomposed_layers > 1:
                                                out = tf.concat(decomp_out, axis = -1)

                                        for hook in self._propagate_forward_hooks.values():
                                            res = hook(self, (edge_index, size, kwargs), out)
                                            if res is not None:
                                                out = res

                                        return out
                                def edge_updater(self, edge_index:Union[tf.Tensor,SparseTensor] , **kwargs):
                                        r"""The initial call to compute or update features for each edge in the
                                        graph.
                                        Args:
                                            edge_index (Tensor or SparseTensor): A :obj:`torch.LongTensor` or a
                                                :obj:`torch_sparse.SparseTensor` that defines the underlying
                                                graph connectivity/message passing flow.
                                                See :meth:`propagate` for more information.
                                            **kwargs: Any additional data which is needed to compute or update
                                                features for each edge in the graph.
                                        """
                                        for hook in self._edge_update_forward_pre_hooks.values():
                                            res = hook(self, (edge_index, kwargs))
                                            if res is not None:
                                                edge_index, kwargs = res

                                        size = self.__check_input__(edge_index, size=None)

                                        coll_dict = self.__collect__(self.__edge_user_args__, edge_index, size,
                                                                    kwargs)

                                        edge_kwargs = self.inspector.distribute('edge_update', coll_dict)
                                        out = self.edge_update(**edge_kwargs)

                                        for hook in self._edge_update_forward_hooks.values():
                                            res = hook(self, (edge_index, kwargs), out)
                                            if res is not None:
                                                out = res

                                        return out
                                def message(self, x_j: tf.Tensor) -> tf.Tensor:
                                        return x_j
                                @property
                                def explain(self) -> bool:
                                        return self._explain
                                @explain.setter
                                def explain(self, explain: bool):
                                        if explain:
                                            methods = ['message', 'explain_message', 'aggregate', 'update']
                                        else:
                                            methods = ['message', 'aggregate', 'update']

                                        self._explain = explain
                                        self.inspector.inspect(self.explain_message, pop_first=True)
                                        self.__user_args__ = self.inspector.keys(methods).difference(
                                            self.special_args)
                                def explain_message(self, inputs: tf.Tensor, size_i: int) -> tf.Tensor:
                                        edge_mask = self._edge_mask

                                        if edge_mask is None:
                                            raise ValueError(f"Could not find a pre-defined 'edge_mask' as "
                                                            f"part of {self.__class__.__name__}.")

                                        if self._apply_sigmoid:
                                            edge_mask = edge_mask.sigmoid()
                                        if inputs.size(self.node_dim) != edge_mask.size(0):
                                            edge_mask = edge_mask[self._loop_mask]
                                            loop = edge_mask.new_ones(size_i)
                                            edge_mask = tf.concat([edge_mask, loop], dim=0)
                                        assert inputs.size(self.node_dim) == edge_mask.size(0)

                                        size = [1] * inputs.dim()
                                        size[self.node_dim] = -1
                                        return inputs * edge_mask.view(size)
                                    
                                def aggregate(self, inputs: tf.Tensor, index: tf.Tensor,
                                                  ptr: Optional[tf.Tensor] = None,
                                                  dim_size: Optional[int] = None) -> tf.Tensor:
                                    
                                        r"""Aggregates messages from neighbors as
                                        :math:`\square_{j \in \mathcal{N}(i)}`.
                                        Takes in the output of message computation as first argument and any
                                        argument which was initially passed to :meth:`propagate`.
                                        By default, this function will delegate its call to the underlying
                                        :class:`~torch_geometric.nn.aggr.Aggregation` module to reduce messages
                                        as specified in :meth:`__init__` by the :obj:`aggr` argument.
                                        """
                                        return self.aggr_module(inputs, index, ptr=ptr, dim_size=dim_size,
                                                                dim=self.node_dim)
                                def message_and_aggregate(self, adj_t: tf.sparse.SparseTensor) -> tf.Tensor:
                                    r"""Fuses computations of :func:`message` and :func:`aggregate` into a
                                    single function.
                                    If applicable, this saves both time and memory since messages do not
                                    explicitly need to be materialized.
                                    This function will only gets called in case it is implemented and
                                    propagation takes place based on a :obj:`torch_sparse.SparseTensor`.
                                    """
                                    raise NotImplementedError
                                def update(self, inputs: tf.Tensor) -> tf.Tensor:
                                    r"""Updates node embeddings in analogy to
                                    :math:`\gamma_{\mathbf{\Theta}}` for each node
                                    :math:`i \in \mathcal{V}`.
                                    Takes in the output of aggregation as first argument and any argument
                                    which was initially passed to :meth:`propagate`.
                                    """
                                    return inputs
                                def edge_update(self) -> tf.Tensor:
                                    r"""Computes or updates features for each edge in the graph.
                                    This function can take any argument as input which was initially passed
                                    to :meth:`edge_updater`.
                                    Furthermore, tensors passed to :meth:`edge_updater` can be mapped to
                                    the respective nodes :math:`i` and :math:`j` by appending :obj:`_i` or
                                    :obj:`_j` to the variable name, *.e.g.* :obj:`x_i` and :obj:`x_j`.
                                    """
                                    raise NotImplementedError
                                def register_propagate_forward_pre_hook(self,
                                                                        hook: Callable) -> RemovableHandle:
                                    r"""Registers a forward pre-hook on the module.
                                    The hook will be called every time before :meth:`propagate` is invoked.
                                    It should have the following signature:
                                    .. code-block:: python
                                        hook(module, inputs) -> None or modified input
                                    The hook can modify the input.
                                    Input keyword arguments are passed to the hook as a dictionary in
                                    :obj:`inputs[-1]`.
                                    Returns a :class:`torch.utils.hooks.RemovableHandle` that can be used
                                    to remove the added hook by calling :obj:`handle.remove()`.
                                    """
                                    handle = RemovableHandle(self._propagate_forward_pre_hooks)
                                    self._propagate_forward_pre_hooks[handle.id] = hook
                                    return handle
                                def register_propagate_forward_hook(self,
                                                                    hook: Callable) -> RemovableHandle:
                                    r"""Registers a forward hook on the module.
                                    The hook will be called every time after :meth:`propagate` has computed
                                    an output.
                                    It should have the following signature:
                                    .. code-block:: python
                                        hook(module, inputs, output) -> None or modified output
                                    The hook can modify the output.
                                    Input keyword arguments are passed to the hook as a dictionary in
                                    :obj:`inputs[-1]`.
                                    Returns a :class:`torch.utils.hooks.RemovableHandle` that can be used
                                    to remove the added hook by calling :obj:`handle.remove()`.
                                    """
                                    handle = RemovableHandle(self._propagate_forward_hooks)
                                    self._propagate_forward_hooks[handle.id] = hook
                                    return handle
                                def register_message_forward_pre_hook(self,
                                                                      hook: Callable) -> RemovableHandle:
                                    r"""Registers a forward pre-hook on the module.
                                    The hook will be called every time before :meth:`message` is invoked.
                                    See :meth:`register_propagate_forward_pre_hook` for more information.
                                    """
                                    handle = RemovableHandle(self._message_forward_pre_hooks)
                                    self._message_forward_pre_hooks[handle.id] = hook
                                    return handle
                                def register_message_forward_hook(self, hook: Callable) -> RemovableHandle:
                                    r"""Registers a forward hook on the module.
                                    The hook will be called every time after :meth:`message` has computed
                                    an output.
                                    See :meth:`register_propagate_forward_hook` for more information.
                                    """
                                    handle = RemovableHandle(self._message_forward_hooks)
                                    self._message_forward_hooks[handle.id] = hook
                                    return handle
                                def register_aggregate_forward_pre_hook(self,
                                                                        hook: Callable) -> RemovableHandle:
                                    r"""Registers a forward pre-hook on the module.
                                    The hook will be called every time before :meth:`aggregate` is invoked.
                                    See :meth:`register_propagate_forward_pre_hook` for more information.
                                    """
                                    handle = RemovableHandle(self._aggregate_forward_pre_hooks)
                                    self._aggregate_forward_pre_hooks[handle.id] = hook
                                    return handle
                                def register_aggregate_forward_hook(self,
                                                                    hook: Callable) -> RemovableHandle:
                                    r"""Registers a forward hook on the module.
                                    The hook will be called every time after :meth:`aggregate` has computed
                                    an output.
                                    See :meth:`register_propagate_forward_hook` for more information.
                                    """
                                    handle = RemovableHandle(self._aggregate_forward_hooks)
                                    self._aggregate_forward_hooks[handle.id] = hook
                                    return handle
                                def register_message_and_aggregate_forward_pre_hook(
                                        self, hook: Callable) -> RemovableHandle:
                                    r"""Registers a forward pre-hook on the module.
                                    The hook will be called every time before :meth:`message_and_aggregate`
                                    is invoked.
                                    See :meth:`register_propagate_forward_pre_hook` for more information.
                                    """
                                    handle = RemovableHandle(self._message_and_aggregate_forward_pre_hooks)
                                    self._message_and_aggregate_forward_pre_hooks[handle.id] = hook
                                    return handle
                                def register_message_and_aggregate_forward_hook(
                                        self, hook: Callable) -> RemovableHandle:
                                    r"""Registers a forward hook on the module.
                                    The hook will be called every time after :meth:`message_and_aggregate`
                                    has computed an output.
                                    See :meth:`register_propagate_forward_hook` for more information.
                                    """
                                    handle = RemovableHandle(self._message_and_aggregate_forward_hooks)
                                    self._message_and_aggregate_forward_hooks[handle.id] = hook
                                    return handle
                                def register_edge_update_forward_pre_hook(
                                        self, hook: Callable) -> RemovableHandle:
                                    r"""Registers a forward pre-hook on the module.
                                    The hook will be called every time before :meth:`edge_update` is
                                    invoked. See :meth:`register_propagate_forward_pre_hook` for more
                                    information.
                                    """
                                    handle = RemovableHandle(self._edge_update_forward_pre_hooks)
                                    self._edge_update_forward_pre_hooks[handle.id] = hook
                                    return handle
                                def register_edge_update_forward_hook(self,
                                                                      hook: Callable) -> RemovableHandle:
                                    r"""Registers a forward hook on the module.
                                    The hook will be called every time after :meth:`edge_update` has
                                    computed an output.
                                    See :meth:`register_propagate_forward_hook` for more information.
                                    """
                                    handle = RemovableHandle(self._edge_update_forward_hooks)
                                    self._edge_update_forward_hooks[handle.id] = hook
                                    return handle
                                def jittable(self, typing: Optional[str] = None):
                                    r"""Analyzes the :class:`MessagePassing` instance and produces a new
                                    jittable module.
                                    Args:
                                        typing (string, optional): If given, will generate a concrete
                                            instance with :meth:`forward` types based on :obj:`typing`,
                                            *e.g.*: :obj:`"(Tensor, Optional[Tensor]) -> Tensor"`.
                                    """
                                    try:
                                        from jinja2 import Template
                                    except ImportError:
                                        raise ModuleNotFoundError(
                                            "No module named 'jinja2' found on this machine. "
                                            "Run 'pip install jinja2' to install the library.")

                                    source = inspect.getsource(self.__class__)

                                    # Find and parse `propagate()` types to format `{arg1: type1, ...}`.
                                    if hasattr(self, 'propagate_type'):
                                        prop_types = {
                                            k: sanitize(str(v))
                                            for k, v in self.propagate_type.items()
                                        }
                                    else:
                                        match = re.search(r'#\s*propagate_type:\s*\((.*)\)', source)
                                        if match is None:
                                            raise TypeError(
                                                'TorchScript support requires the definition of the types '
                                                'passed to `propagate()`. Please specify them via\n\n'
                                                'propagate_type = {"arg1": type1, "arg2": type2, ... }\n\n'
                                                'or via\n\n'
                                                '# propagate_type: (arg1: type1, arg2: type2, ...)\n\n'
                                                'inside the `MessagePassing` module.')
                                        prop_types = split_types_repr(match.group(1))
                                        prop_types = dict([re.split(r'\s*:\s*', t) for t in prop_types])

                                    # Find and parse `edge_updater` types to format `{arg1: type1, ...}`.
                                    if 'edge_update' in self.__class__.__dict__.keys():
                                        if hasattr(self, 'edge_updater_type'):
                                            edge_updater_types = {
                                                k: sanitize(str(v))
                                                for k, v in self.edge_updater.items()
                                            }
                                        else:
                                            match = re.search(r'#\s*edge_updater_type:\s*\((.*)\)', source)
                                            if match is None:
                                                raise TypeError(
                                                    'TorchScript support requires the definition of the '
                                                    'types passed to `edge_updater()`. Please specify '
                                                    'them via\n\n edge_updater_type = {"arg1": type1, '
                                                    '"arg2": type2, ... }\n\n or via\n\n'
                                                    '# edge_updater_type: (arg1: type1, arg2: type2, ...)'
                                                    '\n\ninside the `MessagePassing` module.')
                                            edge_updater_types = split_types_repr(match.group(1))
                                            edge_updater_types = dict(
                                                [re.split(r'\s*:\s*', t) for t in edge_updater_types])
                                    else:
                                        edge_updater_types = {}

                                    type_hints = get_type_hints(self.__class__.update)
                                    prop_return_type = type_hints.get('return', 'Tensor')
                                    if str(prop_return_type)[:6] == '<class':
                                        prop_return_type = prop_return_type.__name__

                                    type_hints = get_type_hints(self.__class__.edge_update)
                                    edge_updater_return_type = type_hints.get('return', 'Tensor')
                                    if str(edge_updater_return_type)[:6] == '<class':
                                        edge_updater_return_type = edge_updater_return_type.__name__

                                    # Parse `__collect__()` types to format `{arg:1, type1, ...}`.
                                    collect_types = self.inspector.types(
                                        ['message', 'aggregate', 'update'])

                                    # Parse `__collect__()` types to format `{arg:1, type1, ...}`,
                                    # specific to the argument used for edge updates.
                                    edge_collect_types = self.inspector.types(['edge_update'])

                                    # Collect `forward()` header, body and @overload types.
                                    forward_types = parse_types(self.forward)
                                    forward_types = [resolve_types(*types) for types in forward_types]
                                    forward_types = list(chain.from_iterable(forward_types))

                                    keep_annotation = len(forward_types) < 2
                                    forward_header = func_header_repr(self.forward, keep_annotation)
                                    forward_body = func_body_repr(self.forward, keep_annotation)

                                    if keep_annotation:
                                        forward_types = []
                                    elif typing is not None:
                                        forward_types = []
                                        forward_body = 8 * ' ' + f'# type: {typing}\n{forward_body}'

                                    root = os.path.dirname(osp.realpath(__file__))
                                    with open(osp.join(root, 'message_passing.jinja'), 'r') as f:
                                        template = Template(f.read())

                                    uid = uuid1().hex[:6]
                                    cls_name = f'{self.__class__.__name__}Jittable_{uid}'
                                    jit_module_repr = template.render(
                                        uid=uid,
                                        module=str(self.__class__.__module__),
                                        cls_name=cls_name,
                                        parent_cls_name=self.__class__.__name__,
                                        prop_types=prop_types,
                                        prop_return_type=prop_return_type,
                                        fuse=self.fuse,
                                        collect_types=collect_types,
                                        user_args=self.__user_args__,
                                        edge_user_args=self.__edge_user_args__,
                                        forward_header=forward_header,
                                        forward_types=forward_types,
                                        forward_body=forward_body,
                                        msg_args=self.inspector.keys(['message']),
                                        aggr_args=self.inspector.keys(['aggregate']),
                                        msg_and_aggr_args=self.inspector.keys(['message_and_aggregate']),
                                        update_args=self.inspector.keys(['update']),
                                        edge_collect_types=edge_collect_types,
                                        edge_update_args=self.inspector.keys(['edge_update']),
                                        edge_updater_types=edge_updater_types,
                                        edge_updater_return_type=edge_updater_return_type,
                                        check_input=inspect.getsource(self.__check_input__)[:-1],
                                    )
                                    # Instantiate a class from the rendered JIT module representation.
                                    cls = class_from_module_repr(cls_name, jit_module_repr)
                                    module = cls.__new__(cls)
                                    module.__dict__ = self.__dict__.copy()
                                    module.jittable = None
                                    return module

                                def __repr__(self) -> str:
                                        if hasattr(self, 'in_channels') and hasattr(self, 'out_channels'):
                                            return (f'{self.__class__.__name__}({self.in_channels}, '
                                                    f'{self.out_channels})')
                                        return f'{self.__class__.__name__}()'
                            
        
                                  </code>
                                  
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Base class for creating message passing layers of the form</p>
                  <img class="equationclass" src="assets\images\equation/msgpass.PNG" alt="batch">
                  <p>where <strong>□</strong>denotes a differentiable, permutation invariant function.</p>
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>aggr </strong>
                         – The aggregation scheme to use, e.g., <mark>"add"</mark>, <mark>"sum"</mark>, <mark>"mean"</mark>, <mark>"min"</mark>, <mark>"max"</mark> or <mark>"mul"</mark>. In addition, can be any <mark>Aggregation</mark> module (or any string that automatically resolves to it). If given as a list, will make use of multiple aggregations in which different outputs will get concatenated in the last dimension.
                      </p>
                      </li>
                      <li>
                        <p><strong>aggr_kwargs </strong>
                         – Arguments passed to the respective aggregation function in case it gets automatically resolved.
                      </p>
                      </li>
                      <li>
                        <p><strong>flow </strong>
                          – The flow direction of message passing (<mark>"source_to_target"</mark> or <mark>"target_to_source"</mark>). 
                      </p>
                      </li>
                      <li>
                        <p><strong>node_dim  </strong>
                          – The axis along which to propagate.
                      </p>
                      </li>
                      <li>
                        <p><strong>decomposed_layers  </strong>
                          – The number of feature decomposition layers, as introduced in the <a href="https://arxiv.org/abs/2104.03058" target="_blank">“Optimizing Memory Efficiency of Graph Neural Networks on Edge Computing Platforms”</a> paper.Feature decomposition reduces the peak memory usage by slicing the feature dimensions into separated feature decomposition layers during GNN aggregation. This method can accelerate GNN execution on CPU-based platforms (e.g., 2-3x speedup on the <mark>Reddit</mark> dataset) for common GNN models such as <mark>GCN</mark>, <mark>GraphSAGE</mark>, <mark>GIN</mark>, etc. However, this method is not applicable to all GNN operators available, in particular for operators in which message computation can not easily be decomposed, e.g. in attention-based GNNs. The selection of the optimal value of <mark>decomposed_layers</mark> depends both on the specific graph dataset and available hardware resources. A value of <mark>2</mark> is suitable in most cases. Although the peak memory usage is directly associated with the granularity of feature decomposition, the same is not necessarily true for execution speedups. (default: <mark>1</mark>) 
                      </p>
                      </li>
                      
                    </ul>
                  </dd>
                </dl>
               
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="chxonv">ChebConv</span>( in_feats, out_feats, k, activation=<function relu>, bias=True) </p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#chconv_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="chconv_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR CHEB_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import numpy as np
                            import tensorflow as tf
                            from tensorflow.keras import layers
                            
                            from .... import broadcast_nodes
                            from .... import function as fn
                            from ....base import dgl_warning
                            
                            
                            class ChebConv(layers.Layer):
                                r"""Chebyshev Spectral Graph Convolution layer from `Convolutional
                                Neural Networks on Graphs with Fast Localized Spectral Filtering
                                <https://arxiv.org/pdf/1606.09375.pdf>`__
                                .. math::
                                    h_i^{l+1} &= \sum_{k=0}^{K-1} W^{k, l}z_i^{k, l}
                                    Z^{0, l} &= H^{l}
                                    Z^{1, l} &= \tilde{L} \cdot H^{l}
                                    Z^{k, l} &= 2 \cdot \tilde{L} \cdot Z^{k-1, l} - Z^{k-2, l}
                                    \tilde{L} &= 2\left(I - \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}\right)/\lambda_{max} - I
                                where :math:`\tilde{A}` is :math:`A` + :math:`I`, :math:`W` is learnable weight.
                                Parameters
                                ----------
                                in_feats: int
                                    Dimension of input features; i.e, the number of dimensions of :math:`h_i^{(l)}`.
                                out_feats: int
                                    Dimension of output features :math:`h_i^{(l+1)}`.
                                k : int
                                    Chebyshev filter size :math:`K`.
                                activation : function, optional
                                    Activation function. Default ``ReLu``.
                                bias : bool, optional
                                    If True, adds a learnable bias to the output. Default: ``True``.
                                Example
                                -------
                                >>> import dgl
                                >>> import numpy as np
                                >>> import tensorflow as tf
                                >>> from dgl.nn import ChebConv
                                >>> with tf.device("CPU:0"):
                                ...     g = dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))
                                ...     feat = tf.ones((6, 10))
                                ...     conv = ChebConv(10, 2, 2)
                                ...     res = conv(g, feat)
                                ...     res
                                <tf.Tensor: shape=(6, 2), dtype=float32, numpy=
                                array([[ 0.6163, -0.1809],
                                        [ 0.6163, -0.1809],
                                        [ 0.6163, -0.1809],
                                        [ 0.9698, -1.5053],
                                        [ 0.3664,  0.7556],
                                        [-0.2370,  3.0164]], dtype=float32)>
                                """
                            
                                def __init__(
                                    self, in_feats, out_feats, k, activation=tf.nn.relu, bias=True
                                ):
                                    super(ChebConv, self).__init__()
                                    self._k = k
                                    self._in_feats = in_feats
                                    self._out_feats = out_feats
                                    self.activation = activation
                                    self.linear = layers.Dense(out_feats, use_bias=bias)
                            
                                def call(self, graph, feat, lambda_max=None):
                                    r"""Compute ChebNet layer.
                                    Parameters
                                    ----------
                                    graph : DGLGraph
                                        The graph.
                                    feat : tf.Tensor
                                        The input feature of shape :math:`(N, D_{in})` where :math:`D_{in}`
                                        is size of input feature, :math:`N` is the number of nodes.
                                    lambda_max : list or tensor or None, optional.
                                        A list(tensor) with length :math:`B`, stores the largest eigenvalue
                                        of the normalized laplacian of each individual graph in ``graph``,
                                        where :math:`B` is the batch size of the input graph. Default: None.
                                        If None, this method would set the default value to 2.
                                        One can use :func:`dgl.laplacian_lambda_max` to compute this value.
                                    Returns
                                    -------
                                    tf.Tensor
                                        The output feature of shape :math:`(N, D_{out})` where :math:`D_{out}`
                                        is size of output feature.
                                    """
                            
                                    def unnLaplacian(feat, D_invsqrt, graph):
                                        """Operation Feat * D^-1/2 A D^-1/2"""
                                        graph.ndata["h"] = feat * D_invsqrt
                                        graph.update_all(fn.copy_u("h", "m"), fn.sum("m", "h"))
                                        return graph.ndata.pop("h") * D_invsqrt
                            
                                    with graph.local_scope():
                                        in_degrees = tf.clip_by_value(
                                            tf.cast(graph.in_degrees(), tf.float32),
                                            clip_value_min=1,
                                            clip_value_max=np.inf,
                                        )
                                        D_invsqrt = tf.expand_dims(tf.pow(in_degrees, -0.5), axis=-1)
                            
                                        if lambda_max is None:
                                            dgl_warning(
                                                "lambda_max is not provided, using default value of 2.  "
                                                "Please use dgl.laplacian_lambda_max to compute the eigenvalues."
                                            )
                                            lambda_max = [2] * graph.batch_size
                            
                                        if isinstance(lambda_max, list):
                                            lambda_max = tf.constant(lambda_max, dtype=tf.float32)
                                        if lambda_max.ndim == 1:
                                            lambda_max = tf.expand_dims(
                                                lambda_max, axis=-1
                                            )  # (B,) to (B, 1)
                            
                                        # broadcast from (B, 1) to (N, 1)
                                        lambda_max = broadcast_nodes(graph, lambda_max)
                                        re_norm = 2.0 / lambda_max
                            
                                        # X_0 is the raw feature, Xt refers to the concatenation of X_0, X_1, ... X_t
                                        Xt = X_0 = feat
                            
                                        # X_1(f)
                                        if self._k > 1:
                                            h = unnLaplacian(X_0, D_invsqrt, graph)
                                            X_1 = -re_norm * h + X_0 * (re_norm - 1)
                                            # Concatenate Xt and X_1
                                            Xt = tf.concat((Xt, X_1), 1)
                            
                                        # Xi(x), i = 2...k
                                        for _ in range(2, self._k):
                                            h = unnLaplacian(X_1, D_invsqrt, graph)
                                            X_i = -2 * re_norm * h + X_1 * 2 * (re_norm - 1) - X_0
                                            # Concatenate Xt and X_i
                                            Xt = tf.concat((Xt, X_i), 1)
                                            X_1, X_0 = X_i, X_1
                            
                                        # linear projection
                                        h = self.linear(Xt)
                            
                                        # activation
                                        if self.activation:
                                            h = self.activation(h)
                            
                                    return h
                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>The chebyshev spectral graph convolutional operator from the <a href="https://arxiv.org/abs/1606.09375" target="_blank">“Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering”</a> paper</p>
                  <img class="equationclass" src="assets\images\equation/chebconv.PNG" alt="batch">
                  <p>where<strong>A<sup>~</sup> is A + I, W is learnable weight.</strong></p>
                  
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_feats </strong>
                         – Dimension of input features; i.e, the number of dimensions of h<sup>(l)</sup><sub>i</sub>.
                      </p>
                      </li>
                      <li>
                        <p><strong>out_feats   </strong>
                         –  Dimension of output features h<sup>(l)</sup><sub>i</sub>.
                      </p>
                      </li>
                      <li>
                        <p><strong>K </strong>
                          – Chebyshev filter size<strong>K </strong> . 
                      </p>
                      </li>
                      <li>
                        <p><strong>activation   </strong>
                          – Activation function. Default<mark>"ReLu"</mark>
                      </p>
                      </li>
                      <li>
                        <p><strong>bias   </strong>
                          – If <mark>True</mark>, adds a learnable bias to the output.
                      </p>
                      </li>
                      
                    </ul>
                  </dd>
                </dl>
               
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="sagconv">SAGEConv</span>( in_feats, out_feats, aggregator_type, feat_drop=0.0, bias=True, norm=None, activation=None) </p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#sage_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="sage_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR SAGE_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            from tensorflow.keras import layers
                            
                            from .... import function as fn
                            from ....base import DGLError
                            from ....utils import expand_as_pair, check_eq_shape
                            
                            
                            class SAGEConv(layers.Layer):
                                r"""GraphSAGE layer from `Inductive Representation Learning on
                                Large Graphs <https://arxiv.org/pdf/1706.02216.pdf>`__
                                .. math::
                                    h_{\mathcal{N}(i)}^{(l+1)} &= \mathrm{aggregate}
                                    \left(\{h_{j}^{l}, \forall j \in \mathcal{N}(i) \}\right)
                                    h_{i}^{(l+1)} &= \sigma \left(W \cdot \mathrm{concat}
                                    (h_{i}^{l}, h_{\mathcal{N}(i)}^{l+1}) \right)
                                    h_{i}^{(l+1)} &= \mathrm{norm}(h_{i}^{(l+1)})
                                Parameters
                                ----------
                                in_feats : int, or pair of ints
                                    Input feature size; i.e, the number of dimensions of :math:`h_i^{(l)}`.
                                    GATConv can be applied on homogeneous graph and unidirectional
                                    `bipartite graph <https://docs.dgl.ai/generated/dgl.bipartite.html?highlight=bipartite>`__.
                                    If the layer applies on a unidirectional bipartite graph, ``in_feats``
                                    specifies the input feature size on both the source and destination nodes.  If
                                    a scalar is given, the source and destination node feature size would take the
                                    same value.
                                    If aggregator type is ``gcn``, the feature size of source and destination nodes
                                    are required to be the same.
                                out_feats : int
                                    Output feature size; i.e, the number of dimensions of :math:`h_i^{(l+1)}`.
                                aggregator_type : str
                                    Aggregator type to use (``mean``, ``gcn``, ``pool``, ``lstm``).
                                feat_drop : float
                                    Dropout rate on features, default: ``0``.
                                bias : bool
                                    If True, adds a learnable bias to the output. Default: ``True``.
                                norm : callable activation function/layer or None, optional
                                    If not None, applies normalization to the updated node features.
                                activation : callable activation function/layer or None, optional
                                    If not None, applies an activation function to the updated node features.
                                    Default: ``None``.
                                Examples
                                --------
                                >>> import dgl
                                >>> import numpy as np
                                >>> import tensorflow as tf
                                >>> from dgl.nn import SAGEConv
                                >>>
                                >>> # Case 1: Homogeneous graph
                                >>> with tf.device("CPU:0"):
                                >>>     g = dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))
                                >>>     g = dgl.add_self_loop(g)
                                >>>     feat = tf.ones((6, 10))
                                >>>     conv = SAGEConv(10, 2, 'pool')
                                >>>     res = conv(g, feat)
                                >>>     res
                                <tf.Tensor: shape=(6, 2), dtype=float32, numpy=
                                array([[-3.6633523 , -0.90711546],
                                    [-3.6633523 , -0.90711546],
                                    [-3.6633523 , -0.90711546],
                                    [-3.6633523 , -0.90711546],
                                    [-3.6633523 , -0.90711546],
                                    [-3.6633523 , -0.90711546]], dtype=float32)>
                                >>> # Case 2: Unidirectional bipartite graph
                                >>> with tf.device("CPU:0"):
                                >>>     u = [0, 1, 0, 0, 1]
                                >>>     v = [0, 1, 2, 3, 2]
                                >>>     g = dgl.bipartite((u, v))
                                >>>     u_fea = tf.convert_to_tensor(np.random.rand(2, 5))
                                >>>     v_fea = tf.convert_to_tensor(np.random.rand(4, 5))
                                >>>     conv = SAGEConv((5, 10), 2, 'mean')
                                >>>     res = conv(g, (u_fea, v_fea))
                                >>>     res
                                <tf.Tensor: shape=(4, 2), dtype=float32, numpy=
                                array([[-0.59453356, -0.4055441 ],
                                    [-0.47459763, -0.717764  ],
                                    [ 0.3221837 , -0.29876417],
                                    [-0.63356155,  0.09390211]], dtype=float32)>
                                """
                                def __init__(self,
                                             in_feats,
                                             out_feats,
                                             aggregator_type,
                                             feat_drop=0.,
                                             bias=True,
                                             norm=None,
                                             activation=None):
                                    super(SAGEConv, self).__init__()
                                    valid_aggre_types = {'mean', 'gcn', 'pool', 'lstm'}
                                    if aggregator_type not in valid_aggre_types:
                                        raise DGLError(
                                            'Invalid aggregator_type. Must be one of {}. '
                                            'But got {!r} instead.'.format(valid_aggre_types, aggregator_type)
                                        )
                            
                                    self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)
                                    self._out_feats = out_feats
                                    self._aggre_type = aggregator_type
                                    self.norm = norm
                                    self.feat_drop = layers.Dropout(feat_drop)
                                    self.activation = activation
                                    # aggregator type: mean/pool/lstm/gcn
                                    if aggregator_type == 'pool':
                                        self.fc_pool = layers.Dense(self._in_src_feats)
                                    if aggregator_type == 'lstm':
                                        self.lstm = layers.LSTM(units=self._in_src_feats)
                                    if aggregator_type != 'gcn':
                                        self.fc_self = layers.Dense(out_feats, use_bias=bias)
                                    self.fc_neigh = layers.Dense(out_feats, use_bias=bias)
                            
                                def _lstm_reducer(self, nodes):
                                    """LSTM reducer
                                    NOTE(zihao): lstm reducer with default schedule (degree bucketing)
                                    is slow, we could accelerate this with degree padding in the future.
                                    """
                                    m = nodes.mailbox['m']  # (B, L, D)
                                    rst = self.lstm(m)
                                    return {'neigh': rst}
                            
                                def call(self, graph, feat):
                                    r"""Compute GraphSAGE layer.
                                    Parameters
                                    ----------
                                    graph : DGLGraph
                                        The graph.
                                    feat : tf.Tensor or pair of tf.Tensor
                                        If a tf.Tensor is given, it represents the input feature of shape
                                        :math:`(N, D_{in})`
                                        where :math:`D_{in}` is size of input feature, :math:`N` is the number of nodes.
                                        If a pair of tf.Tensor is given, the pair must contain two tensors of shape
                                        :math:`(N_{in}, D_{in_{src}})` and :math:`(N_{out}, D_{in_{dst}})`.
                                    Returns
                                    -------
                                    tf.Tensor
                                        The output feature of shape :math:`(N, D_{out})` where :math:`D_{out}`
                                        is size of output feature.
                                    """
                                    with graph.local_scope():
                                        if isinstance(feat, tuple):
                                            feat_src = self.feat_drop(feat[0])
                                            feat_dst = self.feat_drop(feat[1])
                                        else:
                                            feat_src = feat_dst = self.feat_drop(feat)
                                            if graph.is_block:
                                                feat_dst = feat_src[:graph.number_of_dst_nodes()]
                            
                                        h_self = feat_dst
                            
                                        # Handle the case of graphs without edges
                                        if graph.number_of_edges() == 0:
                                            graph.dstdata['neigh'] = tf.cast(tf.zeros(
                                                (graph.number_of_dst_nodes(), self._in_src_feats)), tf.float32)
                            
                                        if self._aggre_type == 'mean':
                                            graph.srcdata['h'] = feat_src
                                            graph.update_all(fn.copy_src('h', 'm'), fn.mean('m', 'neigh'))
                                            h_neigh = graph.dstdata['neigh']
                                        elif self._aggre_type == 'gcn':
                                            check_eq_shape(feat)
                                            graph.srcdata['h'] = feat_src
                                            graph.dstdata['h'] = feat_dst       # same as above if homogeneous
                                            graph.update_all(fn.copy_src('h', 'm'), fn.sum('m', 'neigh'))
                                            # divide in_degrees
                                            degs = tf.cast(graph.in_degrees(), tf.float32)
                                            h_neigh = (graph.dstdata['neigh'] + graph.dstdata['h']
                                                       ) / (tf.expand_dims(degs, -1) + 1)
                                        elif self._aggre_type == 'pool':
                                            graph.srcdata['h'] = tf.nn.relu(self.fc_pool(feat_src))
                                            graph.update_all(fn.copy_src('h', 'm'), fn.max('m', 'neigh'))
                                            h_neigh = graph.dstdata['neigh']
                                        elif self._aggre_type == 'lstm':
                                            graph.srcdata['h'] = feat_src
                                            graph.update_all(fn.copy_src('h', 'm'), self._lstm_reducer)
                                            h_neigh = graph.dstdata['neigh']
                                        else:
                                            raise KeyError(
                                                'Aggregator type {} not recognized.'.format(self._aggre_type))
                                        # GraphSAGE GCN does not require fc_self.
                                        if self._aggre_type == 'gcn':
                                            rst = self.fc_neigh(h_neigh)
                                        else:
                                            rst = self.fc_self(h_self) + self.fc_neigh(h_neigh)
                                        # activation
                                        if self.activation is not None:
                                            rst = self.activation(rst)
                                        # normalization
                                        if self.norm is not None:
                                            rst = self.norm(rst)
                                        return rst
                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>GraphSAGE layer from <a href="https://arxiv.org/abs/1706.02216" target="_blank">“Inductive Representation Learning on Large Graphs”</a> paper</p>
                  <img class="equationclass" src="assets\images\equation/sage_conv.jpeg" alt="batch">
                  
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_feats </strong>
                         – Dimension of input features; i.e, the number of dimensions of h<sup>(l)</sup><sub>i</sub>.GATConv can be applied on homogeneous graph and unidirectional bipartite graph. If the layer applies on a unidirectional bipartite graph, <mark>in_feats</mark> specifies the input feature size on both the source and destination nodes. If a scalar is given, the source and destination node feature size would take the same value.
                         If aggregator type is <mark>gcn</mark>, the feature size of source and destination nodes are required to be the same.
                      </p>
                      </li>
                      <li>
                        <p><strong>out_feats   </strong>
                         –  Dimension of output features h<sup>(l+1)</sup><sub>i</sub>.
                      </p>
                      </li>
                      <li>
                        <p><strong>aggregator_type </strong>
                          – Aggregator type to use (<mark>mean</mark>, <mark>gcn</mark>, <mark>pool</mark>, <mark>lstm</mark>). 
                      </p>
                      </li>
                      <li>
                        <p><strong>feat_drop   </strong>
                          – Dropout rate on features, default:<mark>0</mark>
                      </p>
                      </li>
                      <li>
                        <p><strong>bias   </strong>
                          – If <mark>True</mark>, adds a learnable bias to the output.
                      </p>
                      </li>
                      <li>
                        <p><strong>norm </strong>
                          – If not None, applies normalization to the updated node features.
                      </p>
                      </li>
                      <li>
                        <p><strong>activation  </strong>
                          – If not None, applies an activation function to the updated node features. Default: <mark>None</mark>.
                      </p>
                      </li>
                      
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="graphconv">GraphConv</span>( in_feats, out_feats, norm='both', weight=True, bias=True, activation=None, allow_zero_in_degree=False) </p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#graph_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="graph_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR GRAPH_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            from tensorflow.keras import layers
                            import numpy as np
                            
                            from .... import function as fn
                            from ....base import DGLError
                            from ....utils import expand_as_pair
                            
                            # pylint: disable=W0235
                            
                            
                            class GraphConv(layers.Layer):
                                r"""Graph convolution from `Semi-Supervised Classification with Graph Convolutional Networks
                                <https://arxiv.org/abs/1609.02907>`__
                                Mathematically it is defined as follows:
                                .. math::
                                  h_i^{(l+1)} = \sigma(b^{(l)} + \sum_{j\in\mathcal{N}(i)}\frac{1}{c_{ij}}h_j^{(l)}W^{(l)})
                                where :math:`\mathcal{N}(i)` is the set of neighbors of node :math:`i`,
                                :math:`c_{ij}` is the product of the square root of node degrees
                                (i.e.,  :math:`c_{ij} = \sqrt{|\mathcal{N}(i)|}\sqrt{|\mathcal{N}(j)|}`),
                                and :math:`\sigma` is an activation function.
                                Parameters
                                ----------
                                in_feats : int
                                    Input feature size; i.e, the number of dimensions of :math:`h_j^{(l)}`.
                                out_feats : int
                                    Output feature size; i.e., the number of dimensions of :math:`h_i^{(l+1)}`.
                                norm : str, optional
                                    How to apply the normalizer.  Can be one of the following values:
                                    * ``right``, to divide the aggregated messages by each node's in-degrees,
                                      which is equivalent to averaging the received messages.
                                    * ``none``, where no normalization is applied.
                                    * ``both`` (default), where the messages are scaled with :math:`1/c_{ji}` above, equivalent
                                      to symmetric normalization.
                                    * ``left``, to divide the messages sent out from each node by its out-degrees,
                                      equivalent to random walk normalization.
                                weight : bool, optional
                                    If True, apply a linear layer. Otherwise, aggregating the messages
                                    without a weight matrix.
                                bias : bool, optional
                                    If True, adds a learnable bias to the output. Default: ``True``.
                                activation : callable activation function/layer or None, optional
                                    If not None, applies an activation function to the updated node features.
                                    Default: ``None``.
                                allow_zero_in_degree : bool, optional
                                    If there are 0-in-degree nodes in the graph, output for those nodes will be invalid
                                    since no message will be passed to those nodes. This is harmful for some applications
                                    causing silent performance regression. This module will raise a DGLError if it detects
                                    0-in-degree nodes in input graph. By setting ``True``, it will suppress the check
                                    and let the users handle it by themselves. Default: ``False``.
                                Attributes
                                ----------
                                weight : torch.Tensor
                                    The learnable weight tensor.
                                bias : torch.Tensor
                                    The learnable bias tensor.
                                Note
                                ----
                                Zero in-degree nodes will lead to invalid output value. This is because no message
                                will be passed to those nodes, the aggregation function will be appied on empty input.
                                A common practice to avoid this is to add a self-loop for each node in the graph if
                                it is homogeneous, which can be achieved by:
                                >>> g = ... # a DGLGraph
                                >>> g = dgl.add_self_loop(g)
                                Calling ``add_self_loop`` will not work for some graphs, for example, heterogeneous graph
                                since the edge type can not be decided for self_loop edges. Set ``allow_zero_in_degree``
                                to ``True`` for those cases to unblock the code and handle zero-in-degree nodes manually.
                                A common practise to handle this is to filter out the nodes with zero-in-degree when use
                                after conv.
                                Examples
                                --------
                                >>> import dgl
                                >>> import numpy as np
                                >>> import tensorflow as tf
                                >>> from dgl.nn import GraphConv
                                >>> # Case 1: Homogeneous graph
                                >>> with tf.device("CPU:0"):
                                ...     g = dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))
                                ...     g = dgl.add_self_loop(g)
                                ...     feat = tf.ones((6, 10))
                                ...     conv = GraphConv(10, 2, norm='both', weight=True, bias=True)
                                ...     res = conv(g, feat)
                                >>> print(res)
                                <tf.Tensor: shape=(6, 2), dtype=float32, numpy=
                                array([[ 0.6208475 , -0.4896223 ],
                                    [ 0.68356586, -0.5390842 ],
                                    [ 0.6208475 , -0.4896223 ],
                                    [ 0.7859846 , -0.61985517],
                                    [ 0.8251371 , -0.65073216],
                                    [ 0.48335412, -0.38119012]], dtype=float32)>
                                >>> # allow_zero_in_degree example
                                >>> with tf.device("CPU:0"):
                                ...     g = dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))
                                ...     conv = GraphConv(10, 2, norm='both', weight=True, bias=True, allow_zero_in_degree=True)
                                ...     res = conv(g, feat)
                                >>> print(res)
                                    <tf.Tensor: shape=(6, 2), dtype=float32, numpy=
                                    array([[ 0.6208475 , -0.4896223 ],
                                        [ 0.68356586, -0.5390842 ],
                                        [ 0.6208475 , -0.4896223 ],
                                        [ 0.7859846 , -0.61985517],
                                        [ 0.8251371 , -0.65073216],
                                        [ 0., 0.]], dtype=float32)>
                                >>> # Case 2: Unidirectional bipartite graph
                                >>> u = [0, 1, 0, 0, 1]
                                >>> v = [0, 1, 2, 3, 2]
                                >>> with tf.device("CPU:0"):
                                ...     g = dgl.bipartite((u, v))
                                ...     u_fea = tf.convert_to_tensor(np.random.rand(2, 5))
                                ...     v_fea = tf.convert_to_tensor(np.random.rand(4, 5))
                                ...     conv = GraphConv(5, 2, norm='both', weight=True, bias=True)
                                ...     res = conv(g, (u_fea, v_fea))
                                >>> res
                                <tf.Tensor: shape=(4, 2), dtype=float32, numpy=
                                array([[ 1.3607183, -0.1636453],
                                    [ 1.6665325, -0.2004239],
                                    [ 2.1405895, -0.2574358],
                                    [ 1.3607183, -0.1636453]], dtype=float32)>
                                """
                                def __init__(self,
                                             in_feats,
                                             out_feats,
                                             norm='both',
                                             weight=True,
                                             bias=True,
                                             activation=None,
                                             allow_zero_in_degree=False):
                                    super(GraphConv, self).__init__()
                                    if norm not in ('none', 'both', 'right', 'left'):
                                        raise DGLError('Invalid norm value. Must be either "none", "both", "right" or "left".'
                                                       ' But got "{}".'.format(norm))
                                    self._in_feats = in_feats
                                    self._out_feats = out_feats
                                    self._norm = norm
                                    self._allow_zero_in_degree = allow_zero_in_degree
                            
                                    if weight:
                                        xinit = tf.keras.initializers.glorot_uniform()
                                        self.weight = tf.Variable(initial_value=xinit(
                                            shape=(in_feats, out_feats), dtype='float32'), trainable=True)
                                    else:
                                        self.weight = None
                            
                                    if bias:
                                        zeroinit = tf.keras.initializers.zeros()
                                        self.bias = tf.Variable(initial_value=zeroinit(
                                            shape=(out_feats), dtype='float32'), trainable=True)
                                    else:
                                        self.bias = None
                            
                                    self._activation = activation
                            
                                def set_allow_zero_in_degree(self, set_value):
                                    r"""Set allow_zero_in_degree flag.
                                    Parameters
                                    ----------
                                    set_value : bool
                                        The value to be set to the flag.
                                    """
                                    self._allow_zero_in_degree = set_value
                            
                                def call(self, graph, feat, weight=None):
                                    r"""Compute graph convolution.
                                    Parameters
                                    ----------
                                    graph : DGLGraph
                                        The graph.
                                    feat : torch.Tensor or pair of torch.Tensor
                                        If a torch.Tensor is given, it represents the input feature of shape
                                        :math:`(N, D_{in})`
                                        where :math:`D_{in}` is size of input feature, :math:`N` is the number of nodes.
                                        If a pair of torch.Tensor is given, which is the case for bipartite graph, the pair
                                        must contain two tensors of shape :math:`(N_{in}, D_{in_{src}})` and
                                        :math:`(N_{out}, D_{in_{dst}})`.
                                    weight : torch.Tensor, optional
                                        Optional external weight tensor.
                                    Returns
                                    -------
                                    torch.Tensor
                                        The output feature
                                    Raises
                                    ------
                                    DGLError
                                        If there are 0-in-degree nodes in the input graph, it will raise DGLError
                                        since no message will be passed to those nodes. This will cause invalid output.
                                        The error can be ignored by setting ``allow_zero_in_degree`` parameter to ``True``.
                                    Note
                                    ----
                                    * Input shape: :math:`(N, *, \text{in_feats})` where * means any number of additional
                                      dimensions, :math:`N` is the number of nodes.
                                    * Output shape: :math:`(N, *, \text{out_feats})` where all but the last dimension are
                                      the same shape as the input.
                                    * Weight shape: :math:`(\text{in_feats}, \text{out_feats})`.
                                    """
                                    with graph.local_scope():
                                        if not self._allow_zero_in_degree:
                                            if  tf.math.count_nonzero(graph.in_degrees() == 0) > 0:
                                                raise DGLError('There are 0-in-degree nodes in the graph, '
                                                               'output for those nodes will be invalid. '
                                                               'This is harmful for some applications, '
                                                               'causing silent performance regression. '
                                                               'Adding self-loop on the input graph by '
                                                               'calling `g = dgl.add_self_loop(g)` will resolve '
                                                               'the issue. Setting ``allow_zero_in_degree`` '
                                                               'to be `True` when constructing this module will '
                                                               'suppress the check and let the code run.')
                            
                                        feat_src, feat_dst = expand_as_pair(feat, graph)
                                        if self._norm in ['both', 'left']:
                                            degs = tf.clip_by_value(tf.cast(graph.out_degrees(), tf.float32),
                                                                    clip_value_min=1,
                                                                    clip_value_max=np.inf)
                                            if self._norm == 'both':
                                                norm = tf.pow(degs, -0.5)
                                            else:
                                                norm = 1.0 / degs
                                            shp = norm.shape + (1,) * (feat_dst.ndim - 1)
                                            norm = tf.reshape(norm, shp)
                                            feat_src = feat_src * norm
                            
                                        if weight is not None:
                                            if self.weight is not None:
                                                raise DGLError('External weight is provided while at the same time the'
                                                               ' module has defined its own weight parameter. Please'
                                                               ' create the module with flag weight=False.')
                                        else:
                                            weight = self.weight
                            
                                        if self._in_feats > self._out_feats:
                                            # mult W first to reduce the feature size for aggregation.
                                            if weight is not None:
                                                feat_src = tf.matmul(feat_src, weight)
                                            graph.srcdata['h'] = feat_src
                                            graph.update_all(fn.copy_src(src='h', out='m'),
                                                             fn.sum(msg='m', out='h'))
                                            rst = graph.dstdata['h']
                                        else:
                                            # aggregate first then mult W
                                            graph.srcdata['h'] = feat_src
                                            graph.update_all(fn.copy_src(src='h', out='m'),
                                                             fn.sum(msg='m', out='h'))
                                            rst = graph.dstdata['h']
                                            if weight is not None:
                                                rst = tf.matmul(rst, weight)
                            
                                        if self._norm in ['both', 'right']:
                                            degs = tf.clip_by_value(tf.cast(graph.in_degrees(), tf.float32),
                                                                    clip_value_min=1,
                                                                    clip_value_max=np.inf)
                                            if self._norm == 'both':
                                                norm = tf.pow(degs, -0.5)
                                            else:
                                                norm = 1.0 / degs
                                            shp = norm.shape + (1,) * (feat_dst.ndim - 1)
                                            norm = tf.reshape(norm, shp)
                                            rst = rst * norm
                            
                                        if self.bias is not None:
                                            rst = rst + self.bias
                            
                                        if self._activation is not None:
                                            rst = self._activation(rst)
                            
                                        return rst
                            
                                def extra_repr(self):
                                    """Set the extra representation of the module,
                                    which will come into effect when printing the model.
                                    """
                                    summary = 'in={_in_feats}, out={_out_feats}'
                                    summary += ', normalization={_norm}'
                                    if '_activation' in self.__dict__:
                                        summary += ', activation={_activation}'
                                    return summary.format(**self.__dict__)
                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Graph convolution from<a href="https://arxiv.org/abs/1609.02907" target="_blank">“Semi-Supervised Classification with Graph Convolutional Networks”</a> paper</p>
                  <img class="equationclass" src="assets\images\equation/graph_conv.PNG" alt="batch">
                  
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_feats </strong>
                         – Dimension of input features; i.e, the number of dimensions of h<sup>(l)</sup><sub>j</sub>.
                      </p>
                      </li>
                      <li>
                        <p><strong>out_feats   </strong>
                         –  Dimension of output features h<sup>(l+1)</sup><sub>i</sub>.
                      </p>
                      </li>
                      <li>
                        <p><strong>norm  </strong>
                          – How to apply the normalizer. Can be one of the following values:
                          <ul>
                            <li><mark>right</mark>: to divide the aggregated messages by each node’s in-degrees, which is equivalent to averaging the received messages.</li>
                            <li><mark>none</mark>: where no normalization is applied.</li>
                            <li><mark>both</mark>: (default), where the messages are scaled with 1/c<sub>ji</sub> above, equivalent to symmetric normalization.</li>
                            <li><mark>left</mark>: to divide the messages sent out from each node by its out-degrees, equivalent to random walk normalization.</li>
                          </ul>
                      </p>
                      </li>
                      <li>
                        <p><strong>weight </strong>
                          – If <mark>True</mark>, apply a linear layer. Otherwise, aggregating the messages without a weight matrix.
                      </p>
                      </li>
                      <li>
                        <p><strong>bias   </strong>
                          – If <mark>True</mark>, adds a learnable bias to the output.
                      </p>
                      </li>
                      <li>
                        <p><strong>norm </strong>
                          – If not None, applies normalization to the updated node features.
                      </p>
                      </li>
                      <li>
                        <p><strong>allow_zero_in_degree   </strong>
                          – If there are 0-in-degree nodes in the graph, output for those nodes will be invalid since no message will be passed to those nodes. This is harmful for some applications causing silent performance regression. This module will raise a DGLError if it detects 0-in-degree nodes in input graph. By setting <mark>True</mark>, it will suppress the check and let the users handle it by themselves. Default: <mark>False</mark>.
                      </p>
                      </li>
                      
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="gravconv">GravNetConv</span>( in_channels: int, out_channels: int, space_dimensions: int, propagate_dimensions: int, k: int, num_workers: Optional[int] = None, **kwargs) </p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#grav_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="grav_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR GRAVENT_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import warnings
                            from typing import Optional, Union
                            from tf_typing import OptTensor, PairOptTensor, PairTensor
                            import tensorflow as tf
                            from message_passing import MessagePassing

                            try:
                                from tf_cluster.knn import knn
                            except ImportError:
                                knn = None

                            class GravNetConv(MessagePassing):
                                def __init__(self, in_channels: int, out_channels: int,
                                            space_dimensions: int, propagate_dimensions: int, k: int,
                                            num_workers: Optional[int] = None, **kwargs):
                                        
                                    super().__init__(aggr=['mean'], flow='source_to_target',
                                                    **kwargs)

                                    if knn is None:
                                        raise ImportError('`GravNetConv` requires `torch-cluster`.')

                                    if num_workers is not None:
                                        warnings.warn(
                                            "'num_workers' attribute in '{self.__class__.__name__}' is "
                                            "deprecated and will be removed in a future release")

                                    self.in_channels = in_channels
                                    self.out_channels = out_channels
                                    self.k = k
                                    self.lin_s = tf.keras.layers.Dense(space_dimensions, input_shape=(in_channels,), activation=None)
                                    self.lin_h = tf.keras.layers.Dense(propagate_dimensions, input_shape=(in_channels,), activation=None)
                                    

                                    self.lin_out1 = tf.keras.layers.Dense(out_channels, input_shape=(in_channels,), activation=None,use_bias=False)
                                    self.lin_out2 = tf.keras.layers.Dense(out_channels, input_shape=(2 * propagate_dimensions,), activation=None)

                                def __call__(
                                        self, x: Union[tf.Tensor, PairTensor],
                                        batch: Union[OptTensor, Optional[PairTensor]] = None) -> tf.Tensor:
                                    # type: (Tensor, OptTensor) -> tf.Tensor  # noqa
                                    # type: (PairTensor, Optional[PairTensor]) -> tf.Tensor  # noqa
                                  
                                        is_bipartite: bool = True
                                        if isinstance(x, tf.Tensor):
                                            x: PairTensor = (x, x)
                                            is_bipartite = False

                                        if x[0].numpy().ndim != 2:
                                            raise ValueError("Static graphs not supported in 'GravNetConv'")

                                        b: PairOptTensor = (None, None)
                                        if isinstance(batch, tf.Tensor):
                                            b = (batch, batch)
                                        elif isinstance(batch, tuple):
                                            assert batch is not None
                                            b = (batch[0], batch[1])

                                        h_l: tf.Tensor = self.lin_h(x[0])

                                        s_l: tf.Tensor = self.lin_s(x[0])
                                        s_r: tf.Tensor = self.lin_s(x[1]) if is_bipartite else s_l

                                        edge_index = tf.reverse(knn(s_l, s_r, self.k, b[0], b[1]),[0])

                                        edge_weight = tf.math.reduce_sum(tf.math.pow((s_l.numpy()[edge_index.numpy()[0]] - s_r.numpy()[edge_index.numpy()[1]]),2),axis = -1)
                                        edge_weight = tf.math.exp(-10. * edge_weight)  # 10 gives a better spread
                                        # propagate_type: (x: OptPairTensor, edge_weight: OptTensor)
                                        out = self.propagate(edge_index, x=(h_l, None),
                                                            edge_weight=edge_weight,
                                                            size=(s_l.get_shape()[0], s_r.get_shape()[0]))

                                        return self.lin_out1(x[1]) + self.lin_out2(out)
                                def message(self, x_j: tf.Tensor, edge_weight: tf.Tensor) -> tf.Tensor:
                                        return x_j * tf.expand_dims(edge_weight,axis = 1)

                                def __repr__(self) -> str:
                                        return (f'{self.__class__.__name__}({self.in_channels}, '
                                            f'{self.out_channels}, k={self.k})')




                            # g = GravNetConv(3,2,2,3,1)


                            # x_j = tf.constant([[1,.7,6],[2,3,4],[4,5,6]])
                            # edge_weight = tf.constant([1,0,2])
                            # g(x_j,edge_weight)

                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>The GravNet operator from the<a href="https://arxiv.org/abs/1902.07987" target="_blank">“Learning representations of irregular particle-detector geometry with distance-weighted graph networks”</a> paper</p>
                  <p> where the graph is dynamically constructed using nearest neighbors. The neighbors are constructed in a learnable low-dimensional projection of the feature space. A second projection of the input feature space is then propagated from the neighbors to each vertex using distance weights that are derived by applying a Gaussian function to the distances.</p>
                  
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_channels  </strong>
                         – Size of each input sample, or <mark>-1</mark> to derive the size from the first input(s) to the forward method.
                      </p>
                      </li>
                      <li>
                        <p><strong>out_channels    </strong>
                         –  The number of output channels.
                      </p>
                      </li>
                      <li>
                        <p><strong>space_dimensions   </strong>
                          – The dimensionality of the space used to construct the neighbors; referred to as <strong>S</strong> in the paper.
                      </p>
                      </li>
                      <li>
                        <p><strong>propagate_dimensions </strong>
                          – The number of features to be propagated between the vertices; referred to as <strong>F<sub>LR</sub></strong>  in the paper.
                      </p>
                      </li>
                      <li>
                        <p><strong>K   </strong>
                          – The number of nearest neighbors.
                      </p>
                      </li> 
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="gatconv">GATConv</span>( in_feats, out_feats, num_heads, feat_drop=0.0, attn_drop=0.0, negative_slope=0.2, residual=False, activation=None, allow_zero_in_degree=False) </p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#gat_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="gat_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR GAT_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            from tensorflow.keras import layers
                            import numpy as np

                            from .... import function as fn
                            from ....base import DGLError
                            from ...functional import edge_softmax
                            from ..utils import Identity

                            # pylint: enable=W0235


                            class GATConv(layers.Layer):
                                r"""Graph Attention Layer from `Graph Attention Network
                                <https://arxiv.org/pdf/1710.10903.pdf>`__
                                .. math::
                                    h_i^{(l+1)} = \sum_{j\in \mathcal{N}(i)} \alpha_{i,j} W^{(l)} h_j^{(l)}
                                where :math:`\alpha_{ij}` is the attention score bewteen node :math:`i` and
                                node :math:`j`:
                                .. math::
                                    \alpha_{ij}^{l} &= \mathrm{softmax_i} (e_{ij}^{l})
                                    e_{ij}^{l} &= \mathrm{LeakyReLU}\left(\vec{a}^T [W h_{i} \| W h_{j}]\right)
                                Parameters
                                ----------
                                in_feats : int, or pair of ints
                                    Input feature size; i.e, the number of dimensions of :math:`h_i^{(l)}`.
                                    ATConv can be applied on homogeneous graph and unidirectional
                                    `bipartite graph <https://docs.dgl.ai/generated/dgl.bipartite.html?highlight=bipartite>`__.
                                    If the layer is to be applied to a unidirectional bipartite graph, ``in_feats``
                                    specifies the input feature size on both the source and destination nodes.  If
                                    a scalar is given, the source and destination node feature size would take the
                                    same value.
                                out_feats : int
                                    Output feature size; i.e, the number of dimensions of :math:`h_i^{(l+1)}`.
                                num_heads : int
                                    Number of heads in Multi-Head Attention.
                                feat_drop : float, optional
                                    Dropout rate on feature. Defaults: ``0``.
                                attn_drop : float, optional
                                    Dropout rate on attention weight. Defaults: ``0``.
                                negative_slope : float, optional
                                    LeakyReLU angle of negative slope. Defaults: ``0.2``.
                                residual : bool, optional
                                    If True, use residual connection. Defaults: ``False``.
                                activation : callable activation function/layer or None, optional.
                                    If not None, applies an activation function to the updated node features.
                                    Default: ``None``.
                                allow_zero_in_degree : bool, optional
                                    If there are 0-in-degree nodes in the graph, output for those nodes will be invalid
                                    since no message will be passed to those nodes. This is harmful for some applications
                                    causing silent performance regression. This module will raise a DGLError if it detects
                                    0-in-degree nodes in input graph. By setting ``True``, it will suppress the check
                                    and let the users handle it by themselves. Defaults: ``False``.
                                Note
                                ----
                                Zero in-degree nodes will lead to invalid output value. This is because no message
                                will be passed to those nodes, the aggregation function will be appied on empty input.
                                A common practice to avoid this is to add a self-loop for each node in the graph if
                                it is homogeneous, which can be achieved by:
                                >>> g = ... # a DGLGraph
                                >>> g = dgl.add_self_loop(g)
                                Calling ``add_self_loop`` will not work for some graphs, for example, heterogeneous graph
                                since the edge type can not be decided for self_loop edges. Set ``allow_zero_in_degree``
                                to ``True`` for those cases to unblock the code and handle zero-in-degree nodes manually.
                                A common practise to handle this is to filter out the nodes with zero-in-degree when use
                                after conv.
                                Examples
                                --------
                                >>> import dgl
                                >>> import numpy as np
                                >>> import tensorflow as tf
                                >>> from dgl.nn import GATConv
                                >>>
                                >>> # Case 1: Homogeneous graph
                                >>> with tf.device("CPU:0"):
                                >>>     g = dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))
                                >>>     g = dgl.add_self_loop(g)
                                >>>     feat = tf.ones((6, 10))
                                >>>     gatconv = GATConv(10, 2, num_heads=3)
                                >>>     res = gatconv(g, feat)
                                >>>     res
                                <tf.Tensor: shape=(6, 3, 2), dtype=float32, numpy=
                                array([[[ 0.75311995, -1.8093625 ],
                                        [-0.12128812, -0.78072834],
                                        [-0.49870574, -0.15074375]],
                                    [[ 0.75311995, -1.8093625 ],
                                        [-0.12128812, -0.78072834],
                                        [-0.49870574, -0.15074375]],
                                    [[ 0.75311995, -1.8093625 ],
                                        [-0.12128812, -0.78072834],
                                        [-0.49870574, -0.15074375]],
                                    [[ 0.75311995, -1.8093626 ],
                                        [-0.12128813, -0.78072834],
                                        [-0.49870574, -0.15074375]],
                                    [[ 0.75311995, -1.8093625 ],
                                        [-0.12128812, -0.78072834],
                                        [-0.49870574, -0.15074375]],
                                    [[ 0.75311995, -1.8093625 ],
                                        [-0.12128812, -0.78072834],
                                        [-0.49870574, -0.15074375]]], dtype=float32)>
                                >>> # Case 2: Unidirectional bipartite graph
                                >>> u = [0, 1, 0, 0, 1]
                                >>> v = [0, 1, 2, 3, 2]
                                >>> g = dgl.heterograph({('A', 'r', 'B'): (u, v)})
                                >>> with tf.device("CPU:0"):
                                >>>     u_feat = tf.convert_to_tensor(np.random.rand(2, 5))
                                >>>     v_feat = tf.convert_to_tensor(np.random.rand(4, 10))
                                >>>     gatconv = GATConv((5,10), 2, 3)
                                >>>     res = gatconv(g, (u_feat, v_feat))
                                >>>     res
                                <tf.Tensor: shape=(4, 3, 2), dtype=float32, numpy=
                                array([[[-0.89649093, -0.74841046],
                                        [ 0.5088224 ,  0.10908248],
                                        [ 0.55670375, -0.6811229 ]],
                                    [[-0.7905004 , -0.1457274 ],
                                        [ 0.2248168 ,  0.93014705],
                                        [ 0.12816726, -0.4093595 ]],
                                    [[-0.85875374, -0.53382933],
                                        [ 0.36841977,  0.51498866],
                                        [ 0.31893706, -0.5303393 ]],
                                    [[-0.89649093, -0.74841046],
                                        [ 0.5088224 ,  0.10908248],
                                        [ 0.55670375, -0.6811229 ]]], dtype=float32)>
                                """
                                def __init__(self,
                                            in_feats,
                                            out_feats,
                                            num_heads,
                                            feat_drop=0.,
                                            attn_drop=0.,
                                            negative_slope=0.2,
                                            residual=False,
                                            activation=None,
                                            allow_zero_in_degree=False):
                                    super(GATConv, self).__init__()
                                    self._num_heads = num_heads
                                    self._in_feats = in_feats
                                    self._out_feats = out_feats
                                    self._allow_zero_in_degree = allow_zero_in_degree
                                    xinit = tf.keras.initializers.VarianceScaling(scale=np.sqrt(
                                        2), mode="fan_avg", distribution="untruncated_normal")
                                    if isinstance(in_feats, tuple):
                                        self.fc_src = layers.Dense(
                                            out_feats * num_heads, use_bias=False, kernel_initializer=xinit)
                                        self.fc_dst = layers.Dense(
                                            out_feats * num_heads, use_bias=False, kernel_initializer=xinit)
                                    else:
                                        self.fc = layers.Dense(
                                            out_feats * num_heads, use_bias=False, kernel_initializer=xinit)
                                    self.attn_l = tf.Variable(initial_value=xinit(
                                        shape=(1, num_heads, out_feats), dtype='float32'), trainable=True)
                                    self.attn_r = tf.Variable(initial_value=xinit(
                                        shape=(1, num_heads, out_feats), dtype='float32'), trainable=True)
                                    self.feat_drop = layers.Dropout(rate=feat_drop)
                                    self.attn_drop = layers.Dropout(rate=attn_drop)
                                    self.leaky_relu = layers.LeakyReLU(alpha=negative_slope)
                                    if residual:
                                        if in_feats != out_feats:
                                            self.res_fc = layers.Dense(
                                                num_heads * out_feats, use_bias=False, kernel_initializer=xinit)
                                        else:
                                            self.res_fc = Identity()
                                    else:
                                        self.res_fc = None
                                        # self.register_buffer('res_fc', None)
                                    self.activation = activation

                                def set_allow_zero_in_degree(self, set_value):
                                    r"""Set allow_zero_in_degree flag.
                                    Parameters
                                    ----------
                                    set_value : bool
                                        The value to be set to the flag.
                                    """
                                    self._allow_zero_in_degree = set_value

                                def call(self, graph, feat, get_attention=False):
                                    r"""Compute graph attention network layer.
                                    Parameters
                                    ----------
                                    graph : DGLGraph
                                        The graph.
                                    feat : tf.Tensor or pair of tf.Tensor
                                        If a tf.Tensor is given, the input feature of shape :math:`(N, *, D_{in})` where
                                        :math:`D_{in}` is size of input feature, :math:`N` is the number of nodes.
                                        If a pair of tf.Tensor is given, the pair must contain two tensors of shape
                                        :math:`(N_{in}, *, D_{in_{src}})` and :math:`(N_{out}, *, D_{in_{dst}})`.
                                    get_attention : bool, optional
                                        Whether to return the attention values. Default to False.
                                    Returns
                                    -------
                                    tf.Tensor
                                        The output feature of shape :math:`(N, *, H, D_{out})` where :math:`H`
                                        is the number of heads, and :math:`D_{out}` is size of output feature.
                                    tf.Tensor, optional
                                        The attention values of shape :math:`(E, *, H, 1)`, where :math:`E` is the number of
                                        edges. This is returned only when :attr:`get_attention` is ``True``.
                                    Raises
                                    ------
                                    DGLError
                                        If there are 0-in-degree nodes in the input graph, it will raise DGLError
                                        since no message will be passed to those nodes. This will cause invalid output.
                                        The error can be ignored by setting ``allow_zero_in_degree`` parameter to ``True``.
                                    """
                                    with graph.local_scope():
                                        if not self._allow_zero_in_degree:
                                            if  tf.math.count_nonzero(graph.in_degrees() == 0) > 0:
                                                raise DGLError('There are 0-in-degree nodes in the graph, '
                                                              'output for those nodes will be invalid. '
                                                              'This is harmful for some applications, '
                                                              'causing silent performance regression. '
                                                              'Adding self-loop on the input graph by '
                                                              'calling `g = dgl.add_self_loop(g)` will resolve '
                                                              'the issue. Setting ``allow_zero_in_degree`` '
                                                              'to be `True` when constructing this module will '
                                                              'suppress the check and let the code run.')

                                        if isinstance(feat, tuple):
                                            src_prefix_shape = tuple(feat[0].shape[:-1])
                                            dst_prefix_shape = tuple(feat[1].shape[:-1])
                                            h_src = self.feat_drop(feat[0])
                                            h_dst = self.feat_drop(feat[1])
                                            if not hasattr(self, 'fc_src'):
                                                self.fc_src, self.fc_dst = self.fc, self.fc
                                            feat_src = tf.reshape(
                                                self.fc_src(h_src),
                                                src_prefix_shape + (self._num_heads, self._out_feats))
                                            feat_dst = tf.reshape(
                                                self.fc_dst(h_dst),
                                                dst_prefix_shape + (self._num_heads, self._out_feats))
                                        else:
                                            src_prefix_shape = dst_prefix_shape = tuple(feat.shape[:-1])
                                            h_src = h_dst = self.feat_drop(feat)
                                            feat_src = feat_dst = tf.reshape(
                                                self.fc(h_src), src_prefix_shape + (self._num_heads, self._out_feats))
                                            if graph.is_block:
                                                feat_dst = feat_src[:graph.number_of_dst_nodes()]
                                                h_dst = h_dst[:graph.number_of_dst_nodes()]
                                                dst_prefix_shape = (graph.number_of_dst_nodes(),) + dst_prefix_shape[1:]
                                        # NOTE: GAT paper uses "first concatenation then linear projection"
                                        # to compute attention scores, while ours is "first projection then
                                        # addition", the two approaches are mathematically equivalent:
                                        # We decompose the weight vector a mentioned in the paper into
                                        # [a_l || a_r], then
                                        # a^T [Wh_i || Wh_j] = a_l Wh_i + a_r Wh_j
                                        # Our implementation is much efficient because we do not need to
                                        # save [Wh_i || Wh_j] on edges, which is not memory-efficient. Plus,
                                        # addition could be optimized with DGL's built-in function u_add_v,
                                        # which further speeds up computation and saves memory footprint.
                                        el = tf.reduce_sum(feat_src * self.attn_l, axis=-1, keepdims=True)
                                        er = tf.reduce_sum(feat_dst * self.attn_r, axis=-1, keepdims=True)
                                        graph.srcdata.update({'ft': feat_src, 'el': el})
                                        graph.dstdata.update({'er': er})
                                        # compute edge attention, el and er are a_l Wh_i and a_r Wh_j respectively.
                                        graph.apply_edges(fn.u_add_v('el', 'er', 'e'))
                                        e = self.leaky_relu(graph.edata.pop('e'))
                                        # compute softmax
                                        graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))
                                        # message passing
                                        graph.update_all(fn.u_mul_e('ft', 'a', 'm'),
                                                        fn.sum('m', 'ft'))
                                        rst = graph.dstdata['ft']
                                        # residual
                                        if self.res_fc is not None:
                                            resval = tf.reshape(self.res_fc(
                                                h_dst), dst_prefix_shape + (-1, self._out_feats))
                                            rst = rst + resval
                                        # activation
                                        if self.activation:
                                            rst = self.activation(rst)

                                        if get_attention:
                                            return rst, graph.edata['a']
                                        else:
                                            return rst

                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Graph Attention Layer from<a href="https://arxiv.org/abs/1710.10903" target="_blank">“Graph Attention Network”</a> paper</p>
                  <img class="equationclass" src="assets/images/equation/gat_conv_1.jpeg" alt="">
                  <p>where &alpha;<sub>ij</sub> is the attention score bewteen node i and node j:</p>
                  <img class="equationclass" src="assets/images/equation/gat_conv_2.jpeg" alt="">
                  
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_feats   </strong>
                         – Input feature size.
                      </p>
                      </li>
                      <li>
                        <p><strong>out_feats     </strong>
                         – Output feature size; i.e, the number of dimensions.
                      </p>
                      </li>
                      <li>
                        <p><strong>num_heads    </strong>
                          – Number of heads in Multi-Head Attention.
                      </p>
                      </li>
                      <li>
                        <p><strong>feat_drop  </strong>
                          – Dropout rate on feature. Defaults: <mark>0</mark>.
                      </p>
                      </li>
                      <li>
                        <p><strong>attn_drop   </strong>
                          – Dropout rate on attention weight. Defaults: <mark>0</mark>.
                      </p>
                      </li> 
                      <li>
                        <p><strong>residual    </strong>
                          – If <mark>True</mark>, use residual connection. Defaults: <mark>False</mark>.
                      </p>
                      </li> 
                      <li>
                        <p><strong>activation     </strong>
                          – If not None, applies an activation function to the updated node features.
                      </p>
                      </li> 
                      <li>
                        <p><strong>allow_zero_in_degree  </strong>
                          – I If there are 0-in-degree nodes in the graph, output for those nodes will be invalid since no message will be passed to those nodes. This is harmful for some applications causing silent performance regression. This module will raise a DGLError if it detects 0-in-degree nodes in input graph. By setting <mark>True</mark>, it will suppress the check and let the users handle it by themselves. Defaults: <mark>False</mark>.
                      </p>
                      </li> 
                    </ul>
                  </dd>
                </dl>
               
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="ginconv">GINConv</span>( apply_func, aggregator_type, init_eps=0, learn_eps=False) </p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#gin_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="gin_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR GIN_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            from tensorflow.keras import layers

                            from .... import function as fn
                            from ....utils import expand_as_pair


                            class GINConv(layers.Layer):
                                r"""Graph Isomorphism Network layer from `How Powerful are Graph
                                Neural Networks? <https://arxiv.org/pdf/1810.00826.pdf>`__
                                .. math::
                                    h_i^{(l+1)} = f_\Theta \left((1 + \epsilon) h_i^{l} +
                                    \mathrm{aggregate}\left(\left\{h_j^{l}, j\in\mathcal{N}(i)
                                    \right\}\right)\right)
                                Parameters
                                ----------
                                apply_func : callable activation function/layer or None
                                    If not None, apply this function to the updated node feature,
                                    the :math:`f_\Theta` in the formula.
                                aggregator_type : str
                                    Aggregator type to use (``sum``, ``max`` or ``mean``).
                                init_eps : float, optional
                                    Initial :math:`\epsilon` value, default: ``0``.
                                learn_eps : bool, optional
                                    If True, :math:`\epsilon` will be a learnable parameter. Default: ``False``.
                                Example
                                -------
                                >>> import dgl
                                >>> import numpy as np
                                >>> import tensorflow as tf
                                >>> from dgl.nn import GINConv
                                >>>
                                >>> with tf.device("CPU:0"):
                                >>>     g = dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))
                                >>>     feat = tf.ones((6, 10))
                                >>>     lin = tf.keras.layers.Dense(10)
                                >>>     conv = GINConv(lin, 'max')
                                >>>     res = conv(g, feat)
                                >>>     res
                                <tf.Tensor: shape=(6, 10), dtype=float32, numpy=
                                array([[-0.1090256 ,  1.9050574 , -0.30704725, -1.995831  , -0.36399186,
                                        1.10414   ,  2.4885745 , -0.35387516,  1.3568261 ,  1.7267858 ],
                                    [-0.1090256 ,  1.9050574 , -0.30704725, -1.995831  , -0.36399186,
                                        1.10414   ,  2.4885745 , -0.35387516,  1.3568261 ,  1.7267858 ],
                                    [-0.1090256 ,  1.9050574 , -0.30704725, -1.995831  , -0.36399186,
                                        1.10414   ,  2.4885745 , -0.35387516,  1.3568261 ,  1.7267858 ],
                                    [-0.1090256 ,  1.9050574 , -0.30704725, -1.995831  , -0.36399186,
                                        1.10414   ,  2.4885745 , -0.35387516,  1.3568261 ,  1.7267858 ],
                                    [-0.1090256 ,  1.9050574 , -0.30704725, -1.995831  , -0.36399186,
                                        1.10414   ,  2.4885745 , -0.35387516,  1.3568261 ,  1.7267858 ],
                                    [-0.0545128 ,  0.9525287 , -0.15352362, -0.9979155 , -0.18199593,
                                        0.55207   ,  1.2442873 , -0.17693758,  0.67841303,  0.8633929 ]],
                                    dtype=float32)>
                                """

                                def __init__(
                                    self, apply_func, aggregator_type, init_eps=0, learn_eps=False
                                ):
                                    super(GINConv, self).__init__()
                                    self.apply_func = apply_func
                                    if aggregator_type == "sum":
                                        self._reducer = fn.sum
                                    elif aggregator_type == "max":
                                        self._reducer = fn.max
                                    elif aggregator_type == "mean":
                                        self._reducer = fn.mean
                                    else:
                                        raise KeyError(
                                            "Aggregator type {} not recognized.".format(aggregator_type)
                                        )
                                    # to specify whether eps is trainable or not.
                                    self.eps = tf.Variable(
                                        initial_value=[init_eps], dtype=tf.float32, trainable=learn_eps
                                    )

                                def call(self, graph, feat):
                                    r"""Compute Graph Isomorphism Network layer.
                                    Parameters
                                    ----------
                                    graph : DGLGraph
                                        The graph.
                                    feat : tf.Tensor or pair of tf.Tensor
                                        If a tf.Tensor is given, the input feature of shape :math:`(N, D_{in})` where
                                        :math:`D_{in}` is size of input feature, :math:`N` is the number of nodes.
                                        If a pair of tf.Tensor is given, the pair must contain two tensors of shape
                                        :math:`(N_{in}, D_{in})` and :math:`(N_{out}, D_{in})`.
                                        If ``apply_func`` is not None, :math:`D_{in}` should
                                        fit the input dimensionality requirement of ``apply_func``.
                                    Returns
                                    -------
                                    tf.Tensor
                                        The output feature of shape :math:`(N, D_{out})` where
                                        :math:`D_{out}` is the output dimensionality of ``apply_func``.
                                        If ``apply_func`` is None, :math:`D_{out}` should be the same
                                        as input dimensionality.
                                    """
                                    with graph.local_scope():
                                        feat_src, feat_dst = expand_as_pair(feat, graph)
                                        graph.srcdata["h"] = feat_src
                                        graph.update_all(fn.copy_u("h", "m"), self._reducer("m", "neigh"))
                                        rst = (1 + self.eps) * feat_dst + graph.dstdata["neigh"]
                                        if self.apply_func is not None:
                                            rst = self.apply_func(rst)
                                        return rst

                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Graph Isomorphism Network layer from<a href="https://arxiv.org/abs/1810.00826" target="_blank">“How Powerful are Graph Neural Networks?”</a> paper</p>
                  <img class="equationclass" src="assets/images/equation/ginconv.PNG" alt="">
                  
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>apply_func    </strong>
                          – If not None, apply this function to the updated node feature, the f<sub>&oplus;</sub> in the formula.
                      </p>
                      </li> 
                      <li>
                        <p><strong>aggregator_type     </strong>
                          – Aggregator type to use (<mark>sum</mark>, <mark>max</mark> or <mark>mean</mark>).
                      </p>
                      </li> 
                      <li>
                        <p><strong>init_eps     </strong>
                          – Initial &isin; value, default: <mark>0</mark>..
                      </p>
                      </li> 
                      <li>
                        <p><strong>learn_eps   </strong>
                          – If <mark>True</mark>, &isin; will be a learnable parameter. Default: <mark>False</mark>.
                      </p>
                      </li> 
                    </ul>
                  </dd>
                </dl>
               
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="appconv">APPNPConv</span>( k, alpha, edge_drop=0.0) </p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#app_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="app_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR APPNP_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import numpy as np
                            import tensorflow as tf
                            from tensorflow.keras import layers

                            from .... import function as fn


                            class APPNPConv(layers.Layer):
                                r"""Approximate Personalized Propagation of Neural Predictions
                                layer from `Predict then Propagate: Graph Neural Networks
                                meet Personalized PageRank <https://arxiv.org/pdf/1810.05997.pdf>`__
                                .. math::
                                    H^{0} & = X
                                    H^{t+1} & = (1-\alpha)\left(\hat{D}^{-1/2}
                                    \hat{A} \hat{D}^{-1/2} H^{t}\right) + \alpha H^{0}
                                Parameters
                                ----------
                                k : int
                                    Number of iterations :math:`K`.
                                alpha : float
                                    The teleport probability :math:`\alpha`.
                                edge_drop : float, optional
                                    Dropout rate on edges that controls the
                                    messages received by each node. Default: ``0``.
                                """

                                def __init__(self, k, alpha, edge_drop=0.0):
                                    super(APPNPConv, self).__init__()
                                    self._k = k
                                    self._alpha = alpha
                                    self.edge_drop = layers.Dropout(edge_drop)

                                def call(self, graph, feat):
                                    r"""Compute APPNP layer.
                                    Parameters
                                    ----------
                                    graph : DGLGraph
                                        The graph.
                                    feat : tf.Tensor
                                        The input feature of shape :math:`(N, *)` :math:`N` is the
                                        number of nodes, and :math:`*` could be of any shape.
                                    Returns
                                    -------
                                    tf.Tensor
                                        The output feature of shape :math:`(N, *)` where :math:`*`
                                        should be the same as input shape.
                                    """
                                    with graph.local_scope():
                                        degs = tf.clip_by_value(
                                            tf.cast(graph.in_degrees(), tf.float32),
                                            clip_value_min=1,
                                            clip_value_max=np.inf,
                                        )
                                        norm = tf.pow(degs, -0.5)
                                        shp = norm.shape + (1,) * (feat.ndim - 1)
                                        norm = tf.reshape(norm, shp)
                                        feat_0 = feat
                                        for _ in range(self._k):
                                            # normalization by src node
                                            feat = feat * norm
                                            graph.ndata["h"] = feat
                                            graph.edata["w"] = self.edge_drop(
                                                tf.ones(graph.number_of_edges(), 1)
                                            )
                                            graph.update_all(fn.u_mul_e("h", "w", "m"), fn.sum("m", "h"))
                                            feat = graph.ndata.pop("h")
                                            # normalization by dst node
                                            feat = feat * norm
                                            feat = (1 - self._alpha) * feat + self._alpha * feat_0
                                        return feat

                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Approximate Personalized Propagation of Neural Predictions layer from<a href="https://arxiv.org/abs/1810.05997" target="_blank">“ Predict then Propagate: Graph Neural Networks meet Personalized PageRank”</a> paper</p>
                  <img class="equationclass" src="assets/images/equation/appconv.PNG" alt="">
                  
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter"> 
                      <li>
                        <p><strong>K  </strong>
                          – Number of iterations K.
                      </p>
                      </li> 
                      <li>
                        <p><strong>alpha </strong>
                          – The teleport probability &alpha;.
                      </p>
                      </li> 
                      <li>
                        <p><strong>edge_drop </strong>
                          –  Dropout rate on edges that controls the messages received by each node.
                      </p>
                      </li> 
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="agnn_conv">AGNNConv</span>( requires_grad: bool = True, add_self_loops: bool = True, **kwargs) </p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#agnn_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="agnn_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR AGNN_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            from typing import Optional
                            import tensorflow as tf
                            from tf_typing import Adj , OptTensor 
                            from attention import tf_softmax
                            from message_passing import *
                            from loop import * 

                            class AGNNConv(MessagePassing):
                                
                                r"""The graph attentional propagation layer from the
                                `"Attention-based Graph Neural Network for Semi-Supervised Learning"
                                <https://arxiv.org/abs/1803.03735>`_ paper
                                .. math::
                                    \mathbf{X}^{\prime} = \mathbf{P} \mathbf{X},
                                where the propagation matrix :math:`\mathbf{P}` is computed as
                                .. math::
                                    P_{i,j} = \frac{\exp( \beta \cdot \cos(\mathbf{x}_i, \mathbf{x}_j))}
                                    {\sum_{k \in \mathcal{N}(i)\cup \{ i \}} \exp( \beta \cdot
                                    \cos(\mathbf{x}_i, \mathbf{x}_k))}
                                with trainable parameter :math:`\beta`.
                                Args:
                                    requires_grad (bool, optional): If set to :obj:`False`, :math:`\beta`
                                        will not be trainable. (default: :obj:`True`)
                                    add_self_loops (bool, optional): If set to :obj:`False`, will not add
                                        self-loops to the input graph. (default: :obj:`True`)
                                    **kwargs (optional): Additional arguments of
                                        :class:`MessagePassing`.
                                Shapes:
                                    - **input:**
                                      node features :math:`(|\mathcal{V}|, F)`,
                                      edge indices :math:`(2, |\mathcal{E}|)`
                                    - **output:** node features :math:`(|\mathcal{V}|, F)`
                                """
                                
                                def __init__(self, requires_grad: bool = True, add_self_loops: bool = True,
                                            **kwargs):
                                        kwargs.setdefault('aggr', 'add')
                                        super(AGNNConv , self).__init__(**kwargs)
                                        self.requires_grad = requires_grad
                                        self.add_self_loops = add_self_loops
                                        if requires_grad:
                                            self.beta = tf.Variable(tf.zeros(1))
                                        else: 
                                            self.beta = tf.ones(1)

                                
                                def __call__(self , x : tf.Tensor , edge_index : Adj) -> tf.Tensor :
                                    if self.add_self_loops:
                                        if isinstance(edge_index, tf.Tensor):
                                            edge_index, _ = remove_self_loops(tf.cast(edge_index , dtype= tf.int64))
                                            edge_index, _ = add_self_loops(edge_index , num_nodes=tf.size(x[1]).numpy())
                                    layer = tf.keras.layers.LayerNormalization(axis = -2)
                                    x_norm = layer(x)
                                    return self.propagate(edge_index, x=x, x_norm=x_norm, size=None)


                            # this is implementation :
                                    
                            #re = AGNNConv()
                            #print(re(tf.constant([[1,2],[4.,5]]) , tf.constant([[1,0],[1,0]])))
                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>The graph attentional propagation layer from the <a href="https://arxiv.org/abs/1803.03735" target="_blank"> “Attention-based Graph Neural Network for Semi-Supervised Learning”</a> paper</p>
                  <img class="equationclass" src="assets/images/equation/agnn_conv_1.PNG" alt="">
                  <p>where the propagation matrix <strong>P</strong> is computed as</p>
                  <img class="equationclass" src="assets/images/equation/agnn_conv_2.PNG" alt="">
                  
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter"> 
                      <li>
                        <p><strong>requires_grad   </strong>
                          – If set to <mark>False</mark>, &beta; will not be trainable.
                      </p>
                      </li> 
                      <li>
                        <p><strong>add_self_loops  </strong>
                          – If set to <mark>False</mark> will not add self-loops to the input graph.
                      </p>
                      </li> 
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="sgconv">SGConv</span>( in_feats, out_feats, k=1, cached=False, bias=True, norm=None, allow_zero_in_degree=False) </p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#sg_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="sg_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR SG_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import numpy as np
                            import tensorflow as tf
                            from tensorflow.keras import layers

                            from .... import function as fn
                            from ....base import DGLError


                            class SGConv(layers.Layer):
                                r"""SGC layer from `Simplifying Graph
                                Convolutional Networks <https://arxiv.org/pdf/1902.07153.pdf>`__
                                .. math::
                                    H^{K} = (\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2})^K X \Theta
                                where :math:`\tilde{A}` is :math:`A` + :math:`I`.
                                Thus the graph input is expected to have self-loop edges added.
                                Parameters
                                ----------
                                in_feats : int
                                    Number of input features; i.e, the number of dimensions of :math:`X`.
                                out_feats : int
                                    Number of output features; i.e, the number of dimensions of :math:`H^{K}`.
                                k : int
                                    Number of hops :math:`K`. Defaults:``1``.
                                cached : bool
                                    If True, the module would cache
                                    .. math::
                                        (\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}})^K X\Theta
                                    at the first forward call. This parameter should only be set to
                                    ``True`` in Transductive Learning setting.
                                bias : bool
                                    If True, adds a learnable bias to the output. Default: ``True``.
                                norm : callable activation function/layer or None, optional
                                    If not None, applies normalization to the updated node features.  Default: ``False``.
                                allow_zero_in_degree : bool, optional
                                    If there are 0-in-degree nodes in the graph, output for those nodes will be invalid
                                    since no message will be passed to those nodes. This is harmful for some applications
                                    causing silent performance regression. This module will raise a DGLError if it detects
                                    0-in-degree nodes in input graph. By setting ``True``, it will suppress the check
                                    and let the users handle it by themselves. Default: ``False``.
                                Note
                                ----
                                Zero in-degree nodes will lead to invalid output value. This is because no message
                                will be passed to those nodes, the aggregation function will be appied on empty input.
                                A common practice to avoid this is to add a self-loop for each node in the graph if
                                it is homogeneous, which can be achieved by:
                                >>> g = ... # a DGLGraph
                                >>> g = dgl.add_self_loop(g)
                                Calling ``add_self_loop`` will not work for some graphs, for example, heterogeneous graph
                                since the edge type can not be decided for self_loop edges. Set ``allow_zero_in_degree``
                                to ``True`` for those cases to unblock the code and handle zero-in-degree nodes manually.
                                A common practise to handle this is to filter out the nodes with zero-in-degree when use
                                after conv.
                                Example
                                -------
                                >>> import dgl
                                >>> import numpy as np
                                >>> import tensorflow as tf
                                >>> from dgl.nn import SGConv
                                >>>
                                >>> with tf.device("CPU:0"):
                                >>>     g = dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))
                                >>>     g = dgl.add_self_loop(g)
                                >>>     feat = tf.ones((6, 10))
                                >>>     conv = SGConv(10, 2, k=2, cached=True)
                                >>>     res = conv(g, feat)
                                >>>     res
                                <tf.Tensor: shape=(6, 2), dtype=float32, numpy=
                                array([[0.61023676, 0.5246612 ],
                                    [0.61023676, 0.5246612 ],
                                    [0.61023676, 0.5246612 ],
                                    [0.8697353 , 0.7477695 ],
                                    [0.60570633, 0.520766  ],
                                    [0.6102368 , 0.52466124]], dtype=float32)>
                                """

                                def __init__(
                                    self,
                                    in_feats,
                                    out_feats,
                                    k=1,
                                    cached=False,
                                    bias=True,
                                    norm=None,
                                    allow_zero_in_degree=False,
                                ):
                                    super(SGConv, self).__init__()
                                    self.fc = layers.Dense(out_feats, use_bias=bias)
                                    self._cached = cached
                                    self._cached_h = None
                                    self._k = k
                                    self.norm = norm
                                    self._allow_zero_in_degree = allow_zero_in_degree

                                def set_allow_zero_in_degree(self, set_value):
                                    r"""Set allow_zero_in_degree flag.
                                    Parameters
                                    ----------
                                    set_value : bool
                                        The value to be set to the flag.
                                    """
                                    self._allow_zero_in_degree = set_value

                                def call(self, graph, feat):
                                    r"""Compute Simplifying Graph Convolution layer.
                                    Parameters
                                    ----------
                                    graph : DGLGraph
                                        The graph.
                                    feat : tf.Tensor
                                        The input feature of shape :math:`(N, D_{in})` where :math:`D_{in}`
                                        is size of input feature, :math:`N` is the number of nodes.
                                    Returns
                                    -------
                                    tf.Tensor
                                        The output feature of shape :math:`(N, D_{out})` where :math:`D_{out}`
                                        is size of output feature.
                                    Raises
                                    ------
                                    DGLError
                                        If there are 0-in-degree nodes in the input graph, it will raise DGLError
                                        since no message will be passed to those nodes. This will cause invalid output.
                                        The error can be ignored by setting ``allow_zero_in_degree`` parameter to ``True``.
                                    Note
                                    ----
                                    If ``cache`` is set to True, ``feat`` and ``graph`` should not change during
                                    training, or you will get wrong results.
                                    """
                                    with graph.local_scope():
                                        if not self._allow_zero_in_degree:
                                            if tf.math.count_nonzero(graph.in_degrees() == 0) > 0:
                                                raise DGLError(
                                                    "There are 0-in-degree nodes in the graph, "
                                                    "output for those nodes will be invalid. "
                                                    "This is harmful for some applications, "
                                                    "causing silent performance regression. "
                                                    "Adding self-loop on the input graph by "
                                                    "calling `g = dgl.add_self_loop(g)` will resolve "
                                                    "the issue. Setting ``allow_zero_in_degree`` "
                                                    "to be `True` when constructing this module will "
                                                    "suppress the check and let the code run."
                                                )

                                        if self._cached_h is not None:
                                            feat = self._cached_h
                                        else:
                                            # compute normalization
                                            degs = tf.clip_by_value(
                                                tf.cast(graph.in_degrees(), tf.float32),
                                                clip_value_min=1,
                                                clip_value_max=np.inf,
                                            )
                                            norm = tf.pow(degs, -0.5)
                                            norm = tf.expand_dims(norm, 1)
                                            # compute (D^-1 A^k D)^k X
                                            for _ in range(self._k):
                                                feat = feat * norm
                                                graph.ndata["h"] = feat
                                                graph.update_all(fn.copy_u("h", "m"), fn.sum("m", "h"))
                                                feat = graph.ndata.pop("h")
                                                feat = feat * norm

                                            if self.norm is not None:
                                                feat = self.norm(feat)

                                            # cache feature
                                            if self._cached:
                                                self._cached_h = feat
                                        return self.fc(feat)

                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>SGC layer from<a href="https://arxiv.org/abs/1902.07153" target="_blank">“Simplifying Graph Convolutional Networks”</a> paper</p>
                  <img class="equationclass" src="assets/images/equation/sgconv.PNG" alt="">
                  
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_feats   </strong>
                          – Number of input features; i.e, the number of dimensions of <strong>X</strong>
                      </p>
                      </li> 
                      <li>
                        <p><strong>out_feats   </strong>
                          –  Number of output features; i.e, the number of dimensions of <strong>H<sup>k</sup></strong>.
                      </p>
                      </li> 
                      <li>
                        <p><strong>K  </strong>
                          – Number of hops K. Defaults:<mark>1</mark>.
                      </p>
                      </li> 
                      <li>
                        <p><strong>cached  </strong>
                          – If True, the module would cache.
                      </p>
                      </li> 
                      <li>
                        <p><strong>bias </strong>
                          – If True, adds a learnable bias to the output.
                      </p>
                      </li> 
                      <li>
                        <p><strong>norm      </strong>
                          – If not None, applies normalization to the updated node features.
                      </p>
                      </li> 
                      <li>
                        <p><strong>allow_zero_in_degree    </strong>
                          – If there are 0-in-degree nodes in the graph, output for those nodes will be invalid since no message will be passed to those nodes. This is harmful for some applications causing silent performance regression. This module will raise a DGLError if it detects 0-in-degree nodes in input graph. By setting <mark>True</mark>, it will suppress the check and let the users handle it by themselves. Default: <mark>False</mark>.
                      </p>
                      </li> 
                    </ul>
                  </dd>
                </dl>
               
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter"> 
                      <li>
                        <p><strong>K  </strong>
                          – Number of iterations K.
                      </p>
                      </li> 
                      <li>
                        <p><strong>alpha </strong>
                          – The teleport probability &alpha;.
                      </p>
                      </li> 
                      <li>
                        <p><strong>edge_drop </strong>
                          –  Dropout rate on edges that controls the messages received by each node.
                      </p>
                      </li> 
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="rgcconv">RelGraphConv</span>( in_feat, out_feat, num_rels, regularizer='basis', num_bases=None, bias=True, activation=None, self_loop=True, low_mem=False, dropout=0.0, layer_norm=False)</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#rel_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="rel_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR REL_GRAPH_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            from tensorflow.keras import layers

                            from .... import function as fn
                            from .. import utils


                            class RelGraphConv(layers.Layer):
                                r"""Relational graph convolution layer from `Modeling Relational Data with Graph
                                Convolutional Networks <https://arxiv.org/abs/1703.06103>`__
                                It can be described as below:
                                .. math::
                                  h_i^{(l+1)} = \sigma(\sum_{r\in\mathcal{R}}
                                  \sum_{j\in\mathcal{N}^r(i)}\frac{1}{c_{i,r}}W_r^{(l)}h_j^{(l)}+W_0^{(l)}h_i^{(l)})
                                where :math:`\mathcal{N}^r(i)` is the neighbor set of node :math:`i` w.r.t. relation
                                :math:`r`. :math:`c_{i,r}` is the normalizer equal
                                to :math:`|\mathcal{N}^r(i)|`. :math:`\sigma` is an activation function. :math:`W_0`
                                is the self-loop weight.
                                The basis regularization decomposes :math:`W_r` by:
                                .. math::
                                  W_r^{(l)} = \sum_{b=1}^B a_{rb}^{(l)}V_b^{(l)}
                                where :math:`B` is the number of bases, :math:`V_b^{(l)}` are linearly combined
                                with coefficients :math:`a_{rb}^{(l)}`.
                                The block-diagonal-decomposition regularization decomposes :math:`W_r` into :math:`B`
                                number of block diagonal matrices. We refer :math:`B` as the number of bases.
                                The block regularization decomposes :math:`W_r` by:
                                .. math::
                                  W_r^{(l)} = \oplus_{b=1}^B Q_{rb}^{(l)}
                                where :math:`B` is the number of bases, :math:`Q_{rb}^{(l)}` are block
                                bases with shape :math:`R^{(d^{(l+1)}/B)*(d^{l}/B)}`.
                                Parameters
                                ----------
                                in_feat : int
                                    Input feature size; i.e, the number of dimensions of :math:`h_j^{(l)}`.
                                out_feat : int
                                    Output feature size; i.e., the number of dimensions of :math:`h_i^{(l+1)}`.
                                num_rels : int
                                    Number of relations. .
                                regularizer : str
                                    Which weight regularizer to use "basis" or "bdd".
                                    "basis" is short for basis-diagonal-decomposition.
                                    "bdd" is short for block-diagonal-decomposition.
                                num_bases : int, optional
                                    Number of bases. If is none, use number of relations. Default: ``None``.
                                bias : bool, optional
                                    True if bias is added. Default: ``True``.
                                activation : callable, optional
                                    Activation function. Default: ``None``.
                                self_loop : bool, optional
                                    True to include self loop message. Default: ``True``.
                                low_mem : bool, optional
                                    True to use low memory implementation of relation message passing function. Default: False.
                                    This option trades speed with memory consumption, and will slowdown the forward/backward.
                                    Turn it on when you encounter OOM problem during training or evaluation. Default: ``False``.
                                dropout : float, optional
                                    Dropout rate. Default: ``0.0``
                                layer_norm: float, optional
                                    Add layer norm. Default: ``False``
                                Examples
                                --------
                                >>> import dgl
                                >>> import numpy as np
                                >>> import tensorflow as tf
                                >>> from dgl.nn import RelGraphConv
                                >>>
                                >>> with tf.device("CPU:0"):
                                >>>     g = dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))
                                >>>     feat = tf.ones((6, 10))
                                >>>     conv = RelGraphConv(10, 2, 3, regularizer='basis', num_bases=2)
                                >>>     etype = tf.convert_to_tensor(np.array([0,1,2,0,1,2]).astype(np.int64))
                                >>>     res = conv(g, feat, etype)
                                >>>     res
                                <tf.Tensor: shape=(6, 2), dtype=float32, numpy=
                                array([[-0.02938664,  1.7932655 ],
                                    [ 0.1146394 ,  0.48319   ],
                                    [-0.02938664,  1.7932655 ],
                                    [ 1.2054908 , -0.26098895],
                                    [ 0.1146394 ,  0.48319   ],
                                    [ 0.75915515,  1.1454091 ]], dtype=float32)>
                                >>> # One-hot input
                                >>> with tf.device("CPU:0"):
                                >>>     one_hot_feat = tf.convert_to_tensor(np.array([0,1,2,3,4,5]).astype(np.int64))
                                >>>     res = conv(g, one_hot_feat, etype)
                                >>>     res
                                <tf.Tensor: shape=(6, 2), dtype=float32, numpy=
                                array([[-0.24205256, -0.7922753 ],
                                    [ 0.62085056,  0.4893622 ],
                                    [-0.9484881 , -0.26546806],
                                    [-0.2163915 , -0.12585883],
                                    [-0.14293689,  0.77483284],
                                    [ 0.091169  , -0.06761569]], dtype=float32)>
                                """

                                def __init__(
                                    self,
                                    in_feat,
                                    out_feat,
                                    num_rels,
                                    regularizer="basis",
                                    num_bases=None,
                                    bias=True,
                                    activation=None,
                                    self_loop=True,
                                    low_mem=False,
                                    dropout=0.0,
                                    layer_norm=False,
                                ):
                                    super(RelGraphConv, self).__init__()
                                    self.in_feat = in_feat
                                    self.out_feat = out_feat
                                    self.num_rels = num_rels
                                    self.regularizer = regularizer
                                    self.num_bases = num_bases
                                    if (
                                        self.num_bases is None
                                        or self.num_bases > self.num_rels
                                        or self.num_bases < 0
                                    ):
                                        self.num_bases = self.num_rels
                                    self.bias = bias
                                    self.activation = activation
                                    self.self_loop = self_loop
                                    self.low_mem = low_mem

                                    assert (
                                        layer_norm is False
                                    ), "TensorFlow currently does not support layer norm."

                                    xinit = tf.keras.initializers.glorot_uniform()
                                    zeroinit = tf.keras.initializers.zeros()

                                    if regularizer == "basis":
                                        # add basis weights
                                        self.weight = tf.Variable(
                                            initial_value=xinit(
                                                shape=(self.num_bases, self.in_feat, self.out_feat),
                                                dtype="float32",
                                            ),
                                            trainable=True,
                                        )
                                        if self.num_bases < self.num_rels:
                                            # linear combination coefficients
                                            self.w_comp = tf.Variable(
                                                initial_value=xinit(
                                                    shape=(self.num_rels, self.num_bases), dtype="float32"
                                                ),
                                                trainable=True,
                                            )
                                        # message func
                                        self.message_func = self.basis_message_func
                                    elif regularizer == "bdd":
                                        if in_feat % num_bases != 0 or out_feat % num_bases != 0:
                                            raise ValueError(
                                                "Feature size must be a multiplier of num_bases."
                                            )
                                        # add block diagonal weights
                                        self.submat_in = in_feat // self.num_bases
                                        self.submat_out = out_feat // self.num_bases

                                        # assuming in_feat and out_feat are both divisible by num_bases
                                        self.weight = tf.Variable(
                                            initial_value=xinit(
                                                shape=(
                                                    self.num_rels,
                                                    self.num_bases * self.submat_in * self.submat_out,
                                                ),
                                                dtype="float32",
                                            ),
                                            trainable=True,
                                        )
                                        # message func
                                        self.message_func = self.bdd_message_func
                                    else:
                                        raise ValueError("Regularizer must be either 'basis' or 'bdd'")

                                    # bias
                                    if self.bias:
                                        self.h_bias = tf.Variable(
                                            initial_value=zeroinit(shape=(out_feat), dtype="float32"),
                                            trainable=True,
                                        )

                                    # weight for self loop
                                    if self.self_loop:
                                        self.loop_weight = tf.Variable(
                                            initial_value=xinit(shape=(in_feat, out_feat), dtype="float32"),
                                            trainable=True,
                                        )

                                    self.dropout = layers.Dropout(rate=dropout)

                                def basis_message_func(self, edges):
                                    """Message function for basis regularizer"""
                                    if self.num_bases < self.num_rels:
                                        # generate all weights from bases
                                        weight = tf.reshape(
                                            self.weight, (self.num_bases, self.in_feat * self.out_feat)
                                        )
                                        weight = tf.reshape(
                                            tf.matmul(self.w_comp, weight),
                                            (self.num_rels, self.in_feat, self.out_feat),
                                        )
                                    else:
                                        weight = self.weight

                                    # calculate msg @ W_r before put msg into edge
                                    # if src is th.int64 we expect it is an index select
                                    if edges.src["h"].dtype != tf.int64 and self.low_mem:
                                        etypes, _ = tf.unique(edges.data["type"])
                                        msg = tf.zeros([edges.src["h"].shape[0], self.out_feat])
                                        idx = tf.range(edges.src["h"].shape[0])
                                        for etype in etypes:
                                            loc = edges.data["type"] == etype
                                            w = weight[etype]
                                            src = tf.boolean_mask(edges.src["h"], loc)
                                            sub_msg = tf.matmul(src, w)
                                            indices = tf.reshape(tf.boolean_mask(idx, loc), (-1, 1))
                                            msg = tf.tensor_scatter_nd_update(msg, indices, sub_msg)
                                    else:
                                        msg = utils.bmm_maybe_select(
                                            edges.src["h"], weight, edges.data["type"]
                                        )
                                    if "norm" in edges.data:
                                        msg = msg * edges.data["norm"]
                                    return {"msg": msg}

                                def bdd_message_func(self, edges):
                                    """Message function for block-diagonal-decomposition regularizer"""
                                    if (edges.src["h"].dtype == tf.int64) and len(
                                        edges.src["h"].shape
                                    ) == 1:
                                        raise TypeError(
                                            "Block decomposition does not allow integer ID feature."
                                        )

                                    # calculate msg @ W_r before put msg into edge
                                    # if src is th.int64 we expect it is an index select
                                    if self.low_mem:
                                        etypes, _ = tf.unique(edges.data["type"])
                                        msg = tf.zeros([edges.src["h"].shape[0], self.out_feat])
                                        idx = tf.range(edges.src["h"].shape[0])
                                        for etype in etypes:
                                            loc = edges.data["type"] == etype
                                            w = tf.reshape(
                                                self.weight[etype],
                                                (self.num_bases, self.submat_in, self.submat_out),
                                            )
                                            src = tf.reshape(
                                                tf.boolean_mask(edges.src["h"], loc),
                                                (-1, self.num_bases, self.submat_in),
                                            )
                                            sub_msg = tf.einsum("abc,bcd->abd", src, w)
                                            sub_msg = tf.reshape(sub_msg, (-1, self.out_feat))
                                            indices = tf.reshape(tf.boolean_mask(idx, loc), (-1, 1))
                                            msg = tf.tensor_scatter_nd_update(msg, indices, sub_msg)
                                    else:
                                        weight = tf.reshape(
                                            tf.gather(self.weight, edges.data["type"]),
                                            (-1, self.submat_in, self.submat_out),
                                        )
                                        node = tf.reshape(edges.src["h"], (-1, 1, self.submat_in))
                                        msg = tf.reshape(tf.matmul(node, weight), (-1, self.out_feat))
                                    if "norm" in edges.data:
                                        msg = msg * edges.data["norm"]
                                    return {"msg": msg}

                                def call(self, g, x, etypes, norm=None):
                                    """Forward computation
                                    Parameters
                                    ----------
                                    g : DGLGraph
                                        The graph.
                                    x : tf.Tensor
                                        Input node features. Could be either
                                            * :math:`(|V|, D)` dense tensor
                                            * :math:`(|V|,)` int64 vector, representing the categorical values of each
                                              node. We then treat the input feature as an one-hot encoding feature.
                                    etypes : tf.Tensor
                                        Edge type tensor. Shape: :math:`(|E|,)`
                                    norm : tf.Tensor
                                        Optional edge normalizer tensor. Shape: :math:`(|E|, 1)`
                                    Returns
                                    -------
                                    tf.Tensor
                                        New node features.
                                    """
                                    assert g.is_homogeneous, (
                                        "not a homogeneous graph; convert it with to_homogeneous "
                                        "and pass in the edge type as argument"
                                    )
                                    with g.local_scope():
                                        g.ndata["h"] = x
                                        g.edata["type"] = tf.cast(etypes, tf.int64)
                                        if norm is not None:
                                            g.edata["norm"] = norm
                                        if self.self_loop:
                                            loop_message = utils.matmul_maybe_select(x, self.loop_weight)
                                        # message passing
                                        g.update_all(self.message_func, fn.sum(msg="msg", out="h"))
                                        # apply bias and activation
                                        node_repr = g.ndata["h"]
                                        if self.bias:
                                            node_repr = node_repr + self.h_bias
                                        if self.self_loop:
                                            node_repr = node_repr + loop_message
                                        if self.activation:
                                            node_repr = self.activation(node_repr)
                                        node_repr = self.dropout(node_repr)
                                        return node_repr

                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Relational graph convolution layer from<a href="https://arxiv.org/abs/1703.06103" target="_blank">“ Modeling Relational Data with Graph Convolutional Networks”</a> paper</p>
                  <p>It can be described as below:</p>
                  <img class="equationclass" src="assets/images/equation/relgraph_1.jpeg" alt="">
                  <p>where N<sup>&tau;</sup>(i) is the neighbor set of node i w.r.t. relation r. c<sub>i,r</sub> is the normalizer equal to |N<sup>&tau;</sup>(i)|. &alpha; is an activation function. W<sub>0</sub> is the self-loop weight.</p>
                  <p>The basis regularization decomposes W<sub>&tau;</sub> by:</p>
                  <img class="equationclass" src="assets/images/equation/relgraph_2.jpeg" alt="">
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_feats   </strong>
                          – Input feature size; i.e, the number of dimensions.
                      </p>
                      </li> 
                      <li>
                        <p><strong>out_feats   </strong>
                          – Output feature size; i.e., the number of dimensions.
                      </p>
                      </li> 
                      <li>
                        <p><strong>num_rels </strong>
                          – Number of relations. 
                      </p>
                      </li> 
                      <li>
                        <p><strong>regularizer   </strong>
                          – Which weight regularizer to use “basis” or “bdd”. “basis” is short for basis-diagonal-decomposition. “bdd” is short for block-diagonal-decomposition.
                      </p>
                      </li> 
                      <li>
                        <p><strong>num_bases  </strong>
                          –  Number of bases. If is none, use number of relations.
                      </p>
                      </li> 
                      <li>
                        <p><strong>bias </strong>
                          – True if bias is added
                      </li> 
                      <li>
                        <p><strong>activation </strong>
                          – Activation function.
                      </p>
                      </li> 
                      <li>
                        <p><strong>self_loop  </strong>
                          – True to include self loop message. 
                      </p>
                      </li> 
                      <li>
                        <p><strong>low_mem  </strong>
                          – True to use low memory implementation of relation message passing function. Default: False. This option trades speed with memory consumption, and will slowdown the forward/backward. Turn it on when you encounter OOM problem during training or evaluation.
                      </p>
                      </li> 
                      <li>
                        <p><strong>dropout  </strong>
                          –  Dropout rate.
                      </p>
                      </li> 
                      <li>
                        <p><strong>layer_norm  </strong>
                          – Add layer norm. 
                      </p>
                      </li> 
                    </ul>
                  </dd>
                </dl>
               
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="edgeconv">EdgeConv</span>( in_feat, out_feats, batch_norm=False, allow_zero_in_degree=False)</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#edge_conv_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="edge_conv_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR EDGE_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            from tensorflow.keras import layers

                            from .... import function as fn
                            from ....base import DGLError
                            from ....utils import expand_as_pair


                            class EdgeConv(layers.Layer):
                                r"""EdgeConv layer from `Dynamic Graph CNN for Learning on Point Clouds
                                <https://arxiv.org/pdf/1801.07829>`__
                                It can be described as follows:
                                .. math::
                                  h_i^{(l+1)} = \max_{j \in \mathcal{N}(i)} (
                                  \Theta \cdot (h_j^{(l)} - h_i^{(l)}) + \Phi \cdot h_i^{(l)})
                                where :math:`\mathcal{N}(i)` is the neighbor of :math:`i`,
                                :math:`\Theta` and :math:`\Phi` are linear layers.
                                .. note::
                                  The original formulation includes a ReLU inside the maximum operator.
                                  This is equivalent to first applying a maximum operator then applying
                                  the ReLU.
                                Parameters
                                ----------
                                in_feat : int
                                    Input feature size; i.e, the number of dimensions of :math:`h_j^{(l)}`.
                                out_feat : int
                                    Output feature size; i.e., the number of dimensions of :math:`h_i^{(l+1)}`.
                                batch_norm : bool
                                    Whether to include batch normalization on messages. Default: ``False``.
                                allow_zero_in_degree : bool, optional
                                    If there are 0-in-degree nodes in the graph, output for those nodes will be invalid
                                    since no message will be passed to those nodes. This is harmful for some applications
                                    causing silent performance regression. This module will raise a DGLError if it detects
                                    0-in-degree nodes in input graph. By setting ``True``, it will suppress the check
                                    and let the users handle it by themselves. Default: ``False``.
                                Note
                                ----
                                Zero in-degree nodes will lead to invalid output value. This is because no message
                                will be passed to those nodes, the aggregation function will be appied on empty input.
                                A common practice to avoid this is to add a self-loop for each node in the graph if
                                it is homogeneous, which can be achieved by:
                                >>> g = ... # a DGLGraph
                                >>> g = dgl.add_self_loop(g)
                                Calling ``add_self_loop`` will not work for some graphs, for example, heterogeneous graph
                                since the edge type can not be decided for self_loop edges. Set ``allow_zero_in_degree``
                                to ``True`` for those cases to unblock the code and handle zere-in-degree nodes manually.
                                A common practise to handle this is to filter out the nodes with zere-in-degree when use
                                after conv.
                                """

                                def __init__(self, out_feats, batch_norm=False, allow_zero_in_degree=False):
                                    super(EdgeConv, self).__init__()
                                    self.batch_norm = batch_norm
                                    self._allow_zero_in_degree = allow_zero_in_degree

                                    self.theta = layers.Dense(out_feats)
                                    self.phi = layers.Dense(out_feats)
                                    if batch_norm:
                                        self.bn = layers.BatchNormalization()

                                def set_allow_zero_in_degree(self, set_value):
                                    r"""Set allow_zero_in_degree flag.
                                    Parameters
                                    ----------
                                    set_value : bool
                                        The value to be set to the flag.
                                    """
                                    self._allow_zero_in_degree = set_value

                                def call(self, g, feat):
                                    """Forward computation
                                    Parameters
                                    ----------
                                    g : DGLGraph
                                        The graph.
                                    feat : tf.Tensor or pair of tf.Tensor
                                        :math:`(N, D)` where :math:`N` is the number of nodes and
                                        :math:`D` is the number of feature dimensions.
                                        If a pair of tensors is given, the graph must be a uni-bipartite graph
                                        with only one edge type, and the two tensors must have the same
                                        dimensionality on all except the first axis.
                                    Returns
                                    -------
                                    tf.Tensor or pair of tf.Tensor
                                        New node features.
                                    Raises
                                    ------
                                    DGLError
                                        If there are 0-in-degree nodes in the input graph, it will raise DGLError
                                        since no message will be passed to those nodes. This will cause invalid output.
                                        The error can be ignored by setting ``allow_zero_in_degree`` parameter to ``True``.
                                    """
                                    with g.local_scope():
                                        if not self._allow_zero_in_degree:
                                            if tf.math.count_nonzero(g.in_degrees() == 0) > 0:
                                                raise DGLError(
                                                    "There are 0-in-degree nodes in the graph, "
                                                    "output for those nodes will be invalid. "
                                                    "This is harmful for some applications, "
                                                    "causing silent performance regression. "
                                                    "Adding self-loop on the input graph by "
                                                    "calling `g = dgl.add_self_loop(g)` will resolve "
                                                    "the issue. Setting ``allow_zero_in_degree`` "
                                                    "to be `True` when constructing this module will "
                                                    "suppress the check and let the code run."
                                                )
                                        h_src, h_dst = expand_as_pair(feat, g)
                                        g.srcdata["x"] = h_src
                                        g.dstdata["x"] = h_dst
                                        g.apply_edges(fn.v_sub_u("x", "x", "theta"))
                                        g.edata["theta"] = self.theta(g.edata["theta"])
                                        g.dstdata["phi"] = self.phi(g.dstdata["x"])
                                        if not self.batch_norm:
                                            g.update_all(fn.e_add_v("theta", "phi", "e"), fn.max("e", "x"))
                                        else:
                                            g.apply_edges(fn.e_add_v("theta", "phi", "e"))
                                            # for more comments on why global batch norm instead
                                            # of batch norm within EdgeConv go to
                                            # https://github.com/dmlc/dgl/blob/master/python/dgl/nn/pytorch/conv/edgeconv.py
                                            g.edata["e"] = self.bn(g.edata["e"])
                                            g.update_all(fn.copy_e("e", "e"), fn.max("e", "x"))
                                        return g.dstdata["x"]

                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>The edge convolutional operator from the<a href="https://arxiv.org/abs/1801.07829" target="_blank">“ Modeling Relational Data with Graph Convolutional Networks”</a> paper</p>
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_feats   </strong>
                          – Input feature size; i.e, the number of dimensions.
                      </p>
                      </li> 
                      <li>
                        <p><strong>out_feats   </strong>
                          – Output feature size; i.e., the number of dimensions.
                      </p>
                      </li> 
                      <li>
                        <p><strong>batch_norm  </strong>
                          – Whether to include batch normalization on messages.
                      </p>
                      </li> 
                      <li>
                        <p><strong>allow_zero_in_degree    </strong>
                          – If there are 0-in-degree nodes in the graph, output for those nodes will be invalid since no message will be passed to those nodes. This is harmful for some applications causing silent performance regression. This module will raise a DGLError if it detects 0-in-degree nodes in input graph. By setting <mark>True</mark>, it will suppress the check and let the users handle it by themselves. Default: <mark>False</mark>.
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="film_conv">FiLMConv</span>( in_channels: Union[int, Tuple[int, int]], out_channels: int, num_relations: int = 1, nn: Optional[Callable] = None, act: Optional[Callable] = ReLU(), aggr: str = 'mean', **kwargs)</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#edge_conv_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="edge_conv_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR FILM_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import copy
                            import tensorflow as tf
                            from typing import Callable, Optional, Tuple, Union,Any
                            from tf_typing import PairTensor
                            from tensorflow.keras.layers import Dense
                            from message_passing import *


                            class FiLMConv(MessagePassing):
                                def __init__(
                                        self, in_channels: Union[int, Tuple[int, int]], out_channels: int , num_relations: int = 1,
                                        nn: Optional[Callable] = None,
                                        act: Optional[Callable] = tf.keras.layers.ReLU(),
                                        aggr: str = 'add', **kwargs):
                                        super().__init__(aggr=aggr, **kwargs)

                                        self.in_channels = in_channels
                                        self.out_channels = out_channels
                                        self.num_relations = max(num_relations, 1)
                                        self.act = act

                                        if isinstance(in_channels, int):
                                            in_channels = (in_channels, in_channels)

                                        self.lins = []
                                        self.films = []

                                        for _ in range(num_relations):                                      
                                                self.lins.append(Dense( out_channels,input_shape=(in_channels[0],),activation=None))                   
                                                if nn is None:                                                                                                                                                #
                                                    film =  Dense( out_channels,input_shape=(in_channels[1],),activation=None)
                                                else:                                                            
                                                    film = copy.deepcopy(nn)                               
                                                self.films.append(film)                                                              
                                        self.lin_skip = Dense( out_channels,input_shape=(in_channels[1],),activation=None)
                                        if nn is None:
                                            self.film_skip =  Dense( out_channels,input_shape=(in_channels[1],),activation=None)
                                        else:
                                            self.film_skip = copy.deepcopy(nn)


                                def __call__(self, x: Union[tf.Tensor, PairTensor], edge_index: Adj,
                                                    edge_type: OptTensor = None) -> tf.Tensor:
                                    
                                            if isinstance(x, tf.Tensor):
                                                x: PairTensor = (x, x)


                                            beta, gamma = self.film_skip(x[1]).numpy()
                                            out = gamma * self.lin_skip(x[1]) + beta
                                        
                                            if self.act is not None:
                                                out = self.act(out)

                                            # propagate_type: (x: Tensor, beta: Tensor, gamma: Tensor)
                                            if self.num_relations <= 1:
                                                beta, gamma = self.film_skip(x[1]).numpy()
                                                out = out + self.propagate(edge_index, x=x,size=None)
                                                return out 
                                def __repr__(self):
                                            return (f'{self.__class__.__name__}({self.in_channels}, '
                                                    f'{self.out_channels}, num_relations={self.num_relations})')

                            # this is implementation : 

                            #re = FiLMConv(2,2)
                            #print(re(tf.constant([[1,3.],[1,5]])  , tf.constant([[1,1],[1,0]])))

                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>The FiLM graph convolutional operator from the<a href="https://arxiv.org/abs/1906.12192" target="_blank"> “GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation”</a> paper</p>
                  <img class="equationclass" src="assets/images/equation/film_conv.PNG" alt="">
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_channels    </strong>
                          –  Size of each input sample, or -1 to derive the size from the first input(s) to the forward method. A tuple corresponds to the sizes of source and target dimensionalities.
                      </p>
                      </li> 
                      <li>
                        <p><strong>out_channels   </strong>
                          –  Size of each output sample.
                      </p>
                      </li> 
                      <li>
                        <p><strong>num_relations   </strong>
                          –  Number of relations.
                      </p>
                      </li> 
                      <li>
                        <p><strong>nn   </strong>
                          – The neural network g that maps node features <mark>x_i</mark> of shape <mark>[-1, in_channels]</mark> to shape <mark>[-1, 2 * out_channels]</mark>. If set to None, g will be implemented as a single linear layer.
                      </p>
                      </li> 
                      <li>
                        <p><strong>act   </strong>
                          – Activation function &alpha;.
                      </p>
                      </li> 
                      <li>
                        <p><strong>aggr     </strong>
                          –  The aggregation scheme to use (<mark>"add"</mark>, <mark>"mean"</mark>, <mark>"max"</mark>).
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
               
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="leconv">LEConv</span>( in_channels: Union[int, Tuple[int, int]], out_channels: int, bias: bool = True, **kwargs)</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#le_conv_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="le_conv_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR LE_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            from loop import add_self_loops
                            from Scatter import scatter_add
                            from loop import remove_self_loops

                            class LEConv(tf.keras.layers.Layer):
                                def __init__(self, in_channels, out_channels, bias=True):
                                        super().__init__()

                                        self.in_channels = in_channels
                                        self.out_channels = out_channels

                                        self.lin1 = tf.keras.layers.Dense(out_channels, input_shape=(in_channels,), activation=None)
                                        self.lin2 = tf.keras.layers.Dense(out_channels, input_shape=(in_channels,), activation=None)
                                        self.weight = tf.ones([in_channels, out_channels])
                                def reset_parameters(self):
                                    self.lin1.reset_parameters()
                                    self.lin2.reset_parameters()
                                def call(self, x, edge_index, edge_weight=None, size=None):
                                    """"""
                                    num_nodes = x.get_shape()[0]
                                    h = tf.matmul(x, self.weight)

                                    if edge_weight is None:
                                        edge_weight = tf.ones((edge_index.get_shape()[1], ),
                                                                dtype=x.dtype)
                                    edge_index, edge_weight = remove_self_loops(edge_index=edge_index, edge_attr=edge_weight)
                                    
                                    deg = scatter_add(edge_weight, edge_index[0], dim=0, dim_size=num_nodes) #+ 1e-10
                                    h_tensor = tf.constant(h.numpy()[edge_index.numpy()[1]])
                                    h_j = tf.reshape(edge_weight,[-1,1]) * h_tensor
                                    aggr_out = scatter_add(h_j, edge_index[0], dim=0, dim_size=num_nodes)
                                    out = (tf.reshape(deg,[-1,1]) * self.lin1(x) + aggr_out) + self.lin2(x)
                                    edge_index = tf.cast(edge_index,dtype=tf.float32)
                                    edge_index, edge_weight = add_self_loops(edge_index, edge_weight, num_nodes=num_nodes)
                                    return out
                                def __repr__(self):
                                    return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,
                                                              self.out_channels)



                            #Implementation
                            # in_channels, out_channels = (16, 32)
                            # edge_index = tf.constant([[0, 0, 0, 1, 2, 3], [1, 2, 3, 0, 0, 0]])
                            # num_nodes = int(tf.reduce_max(edge_index)) + 1
                            # x = tf.random.normal((num_nodes, in_channels))
                            # conv = LEConv(in_channels, out_channels)
                            # out = conv(x, edge_index)

                            # print(out)

                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>The local extremum graph neural network operator from the<a href="https://arxiv.org/abs/1911.07979" target="_blank">“ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations”</a> paper,which finds the importance of nodes with respect to their neighbors using the difference operator:</p>
                  <img class = "equationclass" src="assets/images/equation/leconv.PNG" alt="">
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_channels </strong>
                          –  Size of each input sample, or <mark>-1</mark> to derive the size from the first input(s) to the forward method. A tuple corresponds to the sizes of source and target dimensionalities.
                      </p>
                      </li> 
                      <li>
                        <p><strong>out_channels </strong>
                          – Size of each output sample.
                      </p>
                      </li> 
                      <li>
                        <p><strong>bias </strong>
                          – If set to <mark>False</mark>, the layer will not learn an additive bias.
                      </p>
                      </li> 
                      
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="cg_conv">CGConv</span>( channels: Union[int, Tuple[int, int]], dim: int = 0, aggr: str = 'add', batch_norm: bool = False, bias: bool = True, **kwargs)</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#cg_conv_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="cg_conv_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR CG_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            from typing import Tuple , Union
                            import tensorflow as tf 
                            from message_passing import * 
                            from tf_typing import Adj , OptTensor , PairTensor 

                            class CGConv(MessagePassing):
                                r"""The crystal graph convolutional operator from the
                                `"Crystal Graph Convolutional Neural Networks for an
                                Accurate and Interpretable Prediction of Material Properties"
                                <https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.145301>`_
                                paper
                                .. math::
                                    \mathbf{x}^{\prime}_i = \mathbf{x}_i + \sum_{j \in \mathcal{N}(i)}
                                    \sigma \left( \mathbf{z}_{i,j} \mathbf{W}_f + \mathbf{b}_f \right)
                                    \odot g \left( \mathbf{z}_{i,j} \mathbf{W}_s + \mathbf{b}_s  \right)
                                where :math:`\mathbf{z}_{i,j} = [ \mathbf{x}_i, \mathbf{x}_j,
                                \mathbf{e}_{i,j} ]` denotes the concatenation of central node features,
                                neighboring node features and edge features.
                                In addition, :math:`\sigma` and :math:`g` denote the sigmoid and softplus
                                functions, respectively.
                                Args:
                                    channels (int or tuple): Size of each input sample. A tuple
                                        corresponds to the sizes of source and target dimensionalities.
                                    dim (int, optional): Edge feature dimensionality. (default: :obj:`0`)
                                    aggr (string, optional): The aggregation operator to use
                                        (:obj:`"add"`, :obj:`"mean"`, :obj:`"max"`).
                                        (default: :obj:`"add"`)
                                    batch_norm (bool, optional): If set to :obj:`True`, will make use of
                                        batch normalization. (default: :obj:`False`)
                                    bias (bool, optional): If set to :obj:`False`, the layer will not learn
                                        an additive bias. (default: :obj:`True`)
                                    **kwargs (optional): Additional arguments of
                                        :class:`MessagePassing`.
                                Shapes:
                                    - **input:**
                                      node features :math:`(|\mathcal{V}|, F)` or
                                      :math:`((|\mathcal{V_s}|, F_{s}), (|\mathcal{V_t}|, F_{t}))`
                                      if bipartite,
                                      edge indices :math:`(2, |\mathcal{E}|)`,
                                      edge features :math:`(|\mathcal{E}|, D)` *(optional)*
                                    - **output:** node features :math:`(|\mathcal{V}|, F)` or
                                      :math:`(|\mathcal{V_t}|, F_{t})` if bipartite
                                """
                                def __init__(self , channels :Union[int ,Tuple[int , int]] , dim:int = 0,
                                            aggr :str = 'add' , batch_norm : bool = False , 
                                            bias : bool = True  , **kwargs ) :
                                    super().__init__(aggr=aggr , **kwargs)
                                    self.channels  = channels 
                                    self.dim = dim 
                                    self.batch_norm = batch_norm
                                    if isinstance(channels , int):
                                        channels = (channels , channels)
                                        self.lin_f = tf.keras.layers.Dense(sum(channels) + dim , input_shape=(channels[1]) , use_bias = bias)
                                        self.lin_s = tf.keras.layers.Dense(sum(channels) + dim , input_shape=(channels[1]) , use_bias = bias)
                                            
                                def __call__(self , x : Union[tf.Tensor , PairTensor] , edge_index : Adj , 
                                            edge_attr : OptTensor = None ) -> tf.Tensor: 
                                    if isinstance(x, tf.Tensor):
                                        x : PairTensor = (x,x)
                                    out = self.propagate(edge_index , x = x , edge_attr= edge_attr , size=None)
                                    if self.batch_norm : 
                                        self.bn = tf.keras.layers.LayerNormalization(axis= -2)
                                    else : 
                                        self.bn = None 
                                        
                                    out = out if self.bn is None else self.bn(out)
                                    out = out + x[1]
                                    return out 
                                        
                                def __repr__(self) -> str:
                                    return f'{self.__class__.__name__}({self.channels}, dim={self.dim})'


                            #this is implementation :

                            re = CGConv((1,2))
                            print(re(tf.constant([[1,2.],[2,5]]) , tf.constant([[1,1],[0,1]])))
                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>The crystal graph convolutional operator from the<a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.145301" target="_blank">“Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties"</a> paper</p>
                  <img class = "equationclass" src="assets/images/equation/CGconv.PNG" alt="">
                  <p>where z<sub>i,j</sub> =[x<sub>i</sub>, x<sub>j</sub>, e<sub>i,j</sub>]  denotes the concatenation of central node features, neighboring node features and edge features. In addition, &alpha; and g denote the sigmoid and softplus functions, respectively.</p>
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>channels  </strong>
                          – Size of each input sample. A tuple corresponds to the sizes of source and target dimensionalities.
                      </p>
                      </li> 
                      <li>
                        <p><strong>dim  </strong>
                          – Edge feature dimensionality.
                      </p>
                      </li> 
                      <li>
                        <p><strong>aggr   </strong>
                          – The aggregation operator to use (</mark>"add"</mark>, <mark>"mean"</mark>, <mark>"max"</mark>).
                      </p>
                      </li> 
                      <li>
                        <p><strong>batch_norm </strong>
                          – If set to <mark>True</mark>, will make use of batch normalization. 
                      </p>
                      </li> 
                      <li>
                        <p><strong>bias </strong>
                          – If set to <mark>False</mark>, the layer will not learn an additive bias.
                      </p>
                      </li> 
                      
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="sign_conv">SignedConv</span>( in_channels: int, out_channels: int, first_aggr: bool, bias: bool = True, **kwargs)</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#sign_conv_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="sign_conv_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR SIGNED_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import warnings
                            from typing import List, Tuple, Union, Any, Optional
                            import tensorflow as tf
                            import numpy as np
                            from message_passing import MessagePassing

                            class SignedConv(MessagePassing):
                                
                                def __init__(self, in_channels: int, out_channels: int, first_aggr: bool,
                                            bias: bool = True, **kwargs):

                                    kwargs.setdefault('aggr', 'mean')
                                    super().__init__(**kwargs)

                                    self.in_channels = in_channels
                                    self.out_channels = out_channels
                                    self.first_aggr = first_aggr

                                    if first_aggr:
                                        self.lin_pos_l = tf.keras.layers.Dense(out_channels, use_bias=False)
                                        
                                        self.lin_pos_r = tf.keras.layers.Dense(out_channels, use_bias=bias)
                                        
                                        self.lin_neg_l = tf.keras.layers.Dense(out_channels, use_bias=False)
                                        
                                        self.lin_neg_r = tf.keras.layers.Dense(out_channels, use_bias=bias)
                                    else:
                                        self.lin_pos_l = tf.keras.layers.Dense(2 * in_channels, use_bias=False)
                                        self.lin_pos_r = tf.keras.layers.Dense(in_channels, use_bias=bias)
                                        self.lin_neg_l = tf.keras.layers.Dense(2 * in_channels, use_bias=False)
                                        self.lin_neg_r = tf.keras.layers.Dense(in_channels, use_bias=bias)

                                def __call__(self, x: Union[tf.Tensor, Tuple[tf.Tensor, tf.Tensor]], pos_edge_index: Union[tf.Tensor, tf.SparseTensor], neg_edge_index: Union[tf.Tensor, tf.SparseTensor]):
                                    

                                    if isinstance(x, tf.Tensor):
                                        x: Tuple[tf.Tensor, tf.Tensor] = (x, x)

                                    # propagate_type: (x: PairTensor)
                                    if self.first_aggr:

                                        out_pos = self.propagate(pos_edge_index, x=x, size=None)
                                        
                                        out_pos = self.lin_pos_l(out_pos)
                                        out_pos = out_pos + self.lin_pos_r(x[1])
                                        

                                        out_neg = self.propagate(neg_edge_index, x=x, size=None)
                                        
                                        out_neg = self.lin_neg_l(out_neg)
                                        out_neg = out_neg + self.lin_neg_r(x[1])
                                        
                                        return tf.concat([out_pos, out_neg], axis = -1)

                                    else:
                                        F_in = self.in_channels

                                        out_pos1 = self.propagate(pos_edge_index, size=None, x=(x[0][..., :F_in], x[1][..., :F_in]))
                                        
                                        out_pos2 = self.propagate(neg_edge_index, size=None,
                                                                  x=(x[0][..., F_in:], x[1][..., F_in:]))
                                        out_pos = tf.concat([out_pos1, out_pos2], axis=-1)
                                        out_pos = self.lin_pos_l(out_pos)
                                        out_pos = out_pos + self.lin_pos_r(x[1][..., :F_in])

                                        out_neg1 = self.propagate(pos_edge_index, size=None,
                                                                  x=(x[0][..., F_in:], x[1][..., F_in:]))
                                        out_neg2 = self.propagate(neg_edge_index, size=None,
                                                                  x=(x[0][..., :F_in], x[1][..., :F_in]))
                                        out_neg = tf.concat([out_neg1, out_neg2], axis=-1)
                                        out_neg = self.lin_neg_l(out_neg)
                                        out_neg = out_neg + self.lin_neg_r(x[1][..., F_in:])
                                        
                                        return tf.concat([out_pos, out_neg], axis=-1)


                                def message(self, x_j: tf.Tensor) -> tf.Tensor:
                                    return x_j
                                    pass

                                def message_and_aggregate(self, adj_t: tf.SparseTensor,
                                                          x: Tuple[tf.Tensor, tf.Tensor]) -> tf.Tensor:
                                    adj_t = adj_t.set_value(None)
                                    return tf.linalg.matmul(adj_t, x[0], reduce=self.aggr)

                                def __repr__(self) -> str:
                                    return (f'{self.__class__.__name__}({self.in_channels}, '
                                            f'{self.out_channels}, first_aggr={self.first_aggr})')
                                            





                            #               ---------------Test--------------------


                            #s = SignedConv(4,2,True)
                            #hyperedge_index =tf.constant([[1, 0, 0, 1],[0, 1, 1, 0]])
                            #x = tf.constant([[ 2.1061,  0.3873, -0.3055, -1.3421],[ 1.4406,  2.8716,  2.2899, -2.8698]])
                            #B =tf.constant([[1, 0, 0, 1],[1, 0, 0, 1]])

                            #s(x,hyperedge_index,B)
                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>The signed graph convolutional operator from the<a href="https://arxiv.org/abs/1808.06354" target="_blank">“Signed Graph Convolutional Network"</a> paper</p>
                  <img class = "equationclass" src="assets/images/equation/sign_conv_1.PNG" alt="">
                  <p>if <mark>first_aggr</mark> is set to <mark>True</mark>, and</p>
                  <img class = "equationclass" src="assets/images/equation/sign_conv_2.PNG" alt="">
                  <p>otherwise. In case <mark>first_aggr</mark> is <mark>False</mark>, the layer expects <mark>x</mark> to be a tensor where <mark>x[:, :in_channels]</mark> denotes the positive node features <strong>X<sup>(pos)</sup></strong> and  <mark>x[:, in_channels:]</mark> denotes the negative node features <strong>X<sup>(neg)</sup></strong>.</p>
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_channels   </strong>
                          – Size of each input sample, or <mark>-1</mark> to derive the size from the first input(s) to the forward method.
                      </p>
                      </li> 
                      <li>
                        <p><strong>out_channels   </strong>
                          – Size of each output sample.
                      </p>
                      </li> 
                      <li>
                        <p><strong>first_aggr   </strong>
                          – Denotes which aggregation formula to use.
                      </p>
                      </li> 
                      <li>
                        <p><strong>bias </strong>
                          – If set to <mark>False</mark>, the layer will not learn an additive bias.
                      </p>
                      </li> 
                      
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="lg_conv">LGConv</span>( normalize: bool = True, **kwargs)</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#lg_conv_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="lg_conv_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR LG_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            from tensorflow.sparse import SparseTensor
                            from num_nodes import tf_maybe_num_nodes
                            import numpy as np
                            from Scatter import scatter_add
                            from loop import add_remaining_self_loops
                            from tf_typing import Adj, OptTensor
                            from typing import Callable, Optional, Tuple, Union
                            from message_passing import MessagePassing
                            from gcn_norm import gcn_norm


                            class LGConv(MessagePassing):
                                def __init__(self, normalize: bool = True, **kwargs):
                                    kwargs.setdefault('aggr', 'add')
                                    super().__init__(**kwargs)
                                    self.normalize = normalize

                                def reset_parameters(self):
                                    pass
                                def __call__(self, x: tf.Tensor, edge_index: Adj,
                                            edge_weight: OptTensor = None) -> tf.Tensor:
                                    """"""
                                    if self.normalize and isinstance(edge_index, tf.Tensor):
                                        out = gcn_norm(edge_index, edge_weight, x.get_shape()[self.node_dim],
                                                      add_self_loops=False, flow=self.flow, dtype=x.dtype)
                                        edge_index, edge_weight = out
                                        
                                    elif self.normalize and isinstance(edge_index, SparseTensor):
                                        edge_index = gcn_norm(edge_index, None, x.get_shape()[self.node_dim],
                                                              add_self_loops=False, flow=self.flow,
                                                              dtype=x.dtype)

                                    # propagate_type: (x: tf.Tensor, edge_weight: OptTensor)
                                    out = self.propagate(edge_index, x=x, edge_weight=edge_weight,
                                                          size=None)
                                    out = out.numpy()   
                                    return np.resize(out,x.get_shape()) 
                                def message(self, x_j: tf.Tensor, edge_weight: OptTensor) -> tf.Tensor:
                                    return x_j if edge_weight is None else tf.reshape(edge_weight,[-1, 1]) * tf.cast(x_j,dtype=tf.float32)

                                def message_and_aggregate(self, adj_t: SparseTensor, x: tf.Tensor) -> tf.Tensor:
                                    return tf.sparse.sparse_dense_matmul(adj_t, x, reduce=self.aggr)



                            #Implementation
                            # g = LGConv()
                            # x =tf.random.normal([4, 8])
                            # edge_index = tf.constant([[0, 1, 2, 3], [0, 0, 1, 1]])
                            # g(x,edge_index)


                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>The Light Graph Convolution (LGC) operator from the <a href="https://arxiv.org/abs/2002.02126" target="_blank">“LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation"</a> paper</p>
                  <img class = "equationclass" src="assets/images/equation/lg_conv.PNG" alt="">
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>normalize    </strong>
                          – If set to <mask>False</mask>, output features will not be normalized via symmetric normalization.
                      </p>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="cluster_conv">ClusterGCNConv</span>( in_channels: int, out_channels: int, diag_lambda: float = 0.0, add_self_loops: bool = True, bias: bool = True, **kwargs)</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#cluster_conv_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="cluster_conv_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR CLUSTER_GCN_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf 
                            from message_passing import * 
                            from tf_typing import Adj , OptTensor
                            from loop import * 
                            from tf_degree import * 

                            class ClusterGCNConv(MessagePassing):
                                r"""The ClusterGCN graph convolutional operator from the
                                `"Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph
                                Convolutional Networks" <https://arxiv.org/abs/1905.07953>`_ paper
                                .. math::
                                    \mathbf{X}^{\prime} = \left( \mathbf{\hat{A}} + \lambda \cdot
                                    \textrm{diag}(\mathbf{\hat{A}}) \right) \mathbf{X} \mathbf{W}_1 +
                                    \mathbf{X} \mathbf{W}_2
                                where :math:`\mathbf{\hat{A}} = {(\mathbf{D} + \mathbf{I})}^{-1}(\mathbf{A}
                                + \mathbf{I})`.
                                Args:
                                    in_channels (int): Size of each input sample, or :obj:`-1` to derive
                                        the size from the first input(s) to the forward method.
                                    out_channels (int): Size of each output sample.
                                    diag_lambda (float, optional): Diagonal enhancement value
                                        :math:`\lambda`. (default: :obj:`0.`)
                                    add_self_loops (bool, optional): If set to :obj:`False`, will not add
                                        self-loops to the input graph. (default: :obj:`True`)
                                    bias (bool, optional): If set to :obj:`False`, the layer will not learn
                                        an additive bias. (default: :obj:`True`)
                                    **kwargs (optional): Additional arguments of
                                        :class:`MessagePassing`.
                                Shapes:
                                    - **input:**
                                      node features :math:`(|\mathcal{V}|, F_{in})`,
                                      edge indices :math:`(2, |\mathcal{E}|)`
                                    - **output:** node features :math:`(|\mathcal{V}|, F_{out})`
                                """
                                def __init__(self, in_channels: int ,out_channels : int ,
                                            diag_lambda : float = 0. , add_self_loops : bool = True ,
                                            bias : bool = True , **kwargs):
                                    kwargs.setdefault('aggr' , 'add')
                                    super().__init__(**kwargs)
                                    
                                    self.in_channels = in_channels
                                    self.out_channels = out_channels
                                    self.diag_lambda = diag_lambda
                                    self.add_self_loops = add_self_loops
                                    self.lin_out = tf.keras.layers.Dense(self.out_channels , input_shape=(self.in_channels,) , use_bias= bias )
                                    self.lin_root = tf.keras.layers.Dense(self.out_channels , input_shape=(self.in_channels,) , use_bias= False )
                                def __call__(self , x: tf.Tensor , edge_index : Adj) -> tf.Tensor:
                                    edge_weight : OptTensor = None 
                                    if isinstance(edge_index , tf.Tensor):
                                        num_nodes = tf.shape(x).numpy()[0]
                                        if self.add_self_loops : 
                                            edge_index , _ = remove_self_loops(edge_index)
                                            edge_index = tf.cast(edge_index , tf.int64)
                                            edge_index , _ = add_self_loops(edge_index , num_nodes= num_nodes )
                                            edge_index = tf.cast(edge_index , tf.int32)
                                        row, col = edge_index[0], edge_index[1]
                                        deg_inv = 1. / degree(col, num_nodes=num_nodes)
                                        edge_weight = deg_inv.numpy()[col.numpy()]
                                        out = self.propagate(edge_index , x = x ,edge_weight = edge_weight , size= None)
                                        out = self.lin_out(out) + self.lin_root(x)
                                        return out
                                
                                def __repr__(self) -> str:
                                    return (f'{self.__class__.__name__}({self.in_channels}, '
                                            f'{self.out_channels}, diag_lambda={self.diag_lambda})')
                                    
                                        

                            # this is implementation: 

                            # re = ClusterGCNConv(4,3)
                            # print(re(tf.constant([[1,2,4],[4,5,5],[4,6,9],[4,8,1]]),tf.constant([[0],[0]])))

                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>The ClusterGCN graph convolutional operator from the  <a href="https://arxiv.org/abs/1905.07953" target="_blank">“Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks"</a> paper</p>
                  <img class = "equationclass" src="assets/images/equation/cluster_gcn_1.PNG" alt="">
                  <p>where</p>
                  <img class = "equationclass" src="assets/images/equation/cluster_gcn_2.PNG" alt="">
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_channels   </strong>
                          – Size of each input sample, or <mark>-1</mark> to derive the size from the first input(s) to the forward method.
                      </p>
                      </li>
                      <li>
                        <p><strong>out_channels    </strong>
                          – Size of each output sample.
                      </p>
                      </li>
                      <li>
                        <p><strong>diag_lambda    </strong>
                          – Diagonal enhancement value <strong>&lambda;</strong>.
                      </p>
                      </li>
                      <li>
                        <p><strong>add_self_loops    </strong>
                          – If set to <mark>False</mark>, will not add self-loops to the input graph.
                      </p>
                      </li>
                      <li>
                        <p><strong>bias    </strong>
                          – If set to <mark>False</mark>, the layer will not learn an additive bias.
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="res_gated_conv">ResGatedGraphConv</span>( in_channels: Union[int, Tuple[int, int]], out_channels: int, act: Optional[Callable] = Sigmoid(), root_weight: bool = True, bias: bool = True, **kwargs)</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#res_gated_conv_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="res_gated_conv_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR RES_GATED_GRAPH_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            from typing import Callable, Optional, Tuple, Union
                            from tf_typing import Adj, PairTensor
                            import numpy as np
                            import tensorflow as tf
                            from message_passing import MessagePassing
                            
                            class ResGatedGraphConv(MessagePassing):
                                def __init__(
                                    self,
                                    in_channels: Union[int, Tuple[int, int]],
                                    out_channels: int,
                                    root_weight: bool = True,
                                    bias: bool = True,
                                    **kwargs,
                                ):
                            
                                    kwargs.setdefault('aggr', 'add')
                                    super().__init__(**kwargs)
                            
                                    self.in_channels = in_channels
                                    self.out_channels = out_channels
                                    self.root_weight = root_weight
                            
                                    if isinstance(in_channels, int):
                                        in_channels = (in_channels, in_channels)
                            
                                    self.lin_key = tf.keras.layers.Dense(out_channels, input_shape=(in_channels[1],))
                                    self.lin_query = tf.keras.layers.Dense(out_channels, input_shape=(in_channels[0],))
                                    self.lin_value = tf.keras.layers.Dense(out_channels, input_shape=(in_channels[0],))
                            
                                    if root_weight:
                                        self.lin_skip = tf.keras.layers.Dense(out_channels,input_shape=(in_channels[1],), use_bias=False)
                                    else:
                                        self.register_parameter('lin_skip', None)
                            
                                    if bias:
                                        self.bias = tf.Variable(tf.zeros(out_channels,dtype=tf.float32))
                                def __call__(self, x: Union[tf.Tensor, PairTensor], edge_index: Adj) -> tf.Tensor:
                                    """"""
                                    if isinstance(x, tf.Tensor):
                                        x: PairTensor = (x, x)
                            
                                    k = self.lin_key(x[1])
                                    q = self.lin_query(x[0])
                                    v = self.lin_value(x[0])
                            
                                    # propagate_type: (k: Tensor, q: Tensor, v: Tensor)
                                    out = self.propagate(edge_index, k=k, q=q, v=v, size=None)   
                                    if self.root_weight:
                                        out = np.resize(out,self.lin_skip(x[1]).get_shape())
                                        out = tf.constant(out,dtype=self.lin_skip(x[1]).dtype)
                                        out = out + self.lin_skip(x[1])
                                        
                            
                                    if self.bias is not None:
                                        out = out + self.bias
                            
                                    return out
                            
                                def message(self, k_i: tf.Tensor, q_j: tf.Tensor, v_j: tf.Tensor) -> tf.Tensor:
                                    return tf.keras.activations.sigmoid(k_i + q_j) * v_j
                                   
                            
                            
                            #Implementation
                            
                            """
                            x1 = tf.random.normal([4, 8])
                            x2 = tf.random.normal([2, 32])
                            edge_index = tf.constant([[0, 1, 2, 3], [0, 0, 1, 1]])
                            
                            
                            conv = ResGatedGraphConv(8, 32)
                            out = conv(x1, edge_index)
                            
                            
                            conv = ResGatedGraphConv((8, 32), 32)
                            out = conv((x1, x2), edge_index)
                            """                            

                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>The residual gated graph convolutional operator from the  <a href="https://arxiv.org/abs/1711.07553" target="_blank">“Residual Gated Graph ConvNets"</a> paper</p>
                  <img class = "equationclass" src="assets/images/equation/res_graph_conv_1.PNG" alt="">
                  <p>where the gate <strong>&eta;<sub>i,j</sub></strong> is defined as</p>
                  <img class = "equationclass" src="assets/images/equation/res_graph_conv_2.PNG" alt="">
                  <p>with <strong>&sigma;</strong> denoting the sigmoid function.</p>
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_channels   </strong>
                          – Size of each input sample, or <mark>-1</mark> to derive the size from the first input(s) to the forward method. A tuple corresponds to the sizes of source and target dimensionalities.
                      </p>
                      </li>
                      <li>
                        <p><strong>out_channels    </strong>
                          – Size of each output sample.
                      </p>
                      </li>
                      <li>
                        <p><strong>act  </strong>
                          – Gating function <strong>&sigma;</strong>.
                      </p>
                      </li>
                      <li>
                        <p><strong>bias </strong>
                          – If set to <mark>False</mark>, the layer will not learn an additive bias.
                      </p>
                      </li>
                      <li>
                        <p><strong>root_weight </strong>
                          – If set to <mark>False</mark>, the layer will not add transformed root node features to the output. 
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="hyper_graph_conv">HypergraphConv</span>( in_channels, out_channels, use_attention=False, heads=1, concat=True, negative_slope=0.2, dropout=0, bias=True, **kwargs)</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#hyper_graph_conv_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="hyper_graph_conv_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR HYPER_GRAPH_CONV</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            from typing import Optional
                            import tensorflow as tf
                            from Scatter import scatter_add
                            import math
                            from typing import Any
                            from message_passing import MessagePassing
                            from tf_softmax import softmax
                            import numpy as np

                            class HypergraphConv(MessagePassing):
                                def __init__(self, in_channels, out_channels, use_attention=False, heads=1,
                                            concat=True, negative_slope=0.2, dropout=0, bias=True,
                                            **kwargs):
                                    kwargs.setdefault('aggr', 'add')
                                    super().__init__(flow='source_to_target', node_dim=0, **kwargs)

                                    self.in_channels = in_channels
                                    self.out_channels = out_channels
                                    self.use_attention = use_attention

                                    if self.use_attention:
                                        self.heads = heads
                                        self.concat = concat
                                        self.negative_slope = negative_slope
                                        self.dropout = dropout
                                        self.lin = tf.keras.layers.Dense(heads * out_channels, input_shape=(in_channels,), 
                                                                        activation=None,use_bias=False,kernel_initializer='glorot_uniform')
                                        
                                        self.att = tf.Variable(tf.zeros([1, heads, 2 * out_channels]),dtype=tf.float32)
                                    else:
                                        self.heads = 1
                                        self.concat = True
                                        self.lin = tf.keras.layers.Dense(out_channels, input_shape=(in_channels,), 
                                                                        activation=None,use_bias=False,kernel_initializer='glorot_uniform')

                                    if bias and concat:
                                        self.bias = tf.Variable(tf.zeros(heads * out_channels),dtype=tf.float32)     
                                    elif bias and not concat:
                                        self.bias = tf.Variable(tf.zeros(out_channels),dtype=tf.float32)     
                                def reset_parameters(self):
                                        self.lin.reset_parameters()
                                        if self.use_attention:
                                            glorot(self.att)
                                        zeros(self.bias)
                                def __call__(self, x: tf.Tensor, hyperedge_index: tf.Tensor,
                                            hyperedge_weight: Optional[tf.Tensor] = None,
                                            hyperedge_attr: Optional[tf.Tensor] = None) -> tf.Tensor:
                                    
                                    num_nodes, num_edges = x.get_shape()[0], 0
                                    if tf.size(hyperedge_index) > 0:
                                        num_edges = int(tf.reduce_max(hyperedge_index[1])) + 1

                                    if hyperedge_weight is None:
                                        hyperedge_weight = tf.ones(num_edges, dtype = x.dtype)

                                    x = self.lin(x)
                                    
                                    alpha = None
                                    if self.use_attention:
                                        assert hyperedge_attr is not None
                                        x = tf.reshape(x,[-1, self.heads, self.out_channels])
                                        hyperedge_attr = self.lin(hyperedge_attr)
                                        hyperedge_attr = tf.reshape(hyperedge_attr,[-1, self.heads,
                                                                            self.out_channels])
                                        x_i = x.numpy()[hyperedge_index.numpy()[0]]
                                        x_j = hyperedge_attr.numpy()[hyperedge_index.numpy()[1]]
                                        alpha = tf.reduce_sum((tf.concat([x_i, x_j], axis=-1) * self.att), axis=-1)
                                        alpha = tf.nn.leaky_relu(alpha, alpha=self.negative_slope)
                                        alpha = softmax(alpha, hyperedge_index[0], num_nodes=x.get_shape()[0])
                                        F_dropout = tf.keras.layers.Dropout(self.dropout)
                                        alpha = F_dropout(alpha, training = True)
                                    src = hyperedge_weight.numpy()[hyperedge_index.numpy()[1]]
                                    src = tf.constant(src)
                                    D = scatter_add(src,hyperedge_index[0], dim=0, dim_size=num_nodes)
                                    D = 1.0 / D
                                    D[D == float("inf")] = 0

                                    B = scatter_add(tf.ones(hyperedge_index.get_shape()[1], dtype = x.dtype),
                                                        hyperedge_index[1], dim=0, dim_size=num_edges)
                                    B = 1.0 / B
                                    B[B == float("inf")] = 0
                                    
                                    out = self.propagate(hyperedge_index, x=x, norm=B, alpha=alpha,
                                                            size=(num_nodes, num_edges))
                                    out = self.propagate(tf.reverse(hyperedge_index,[0]), x=out, norm=D,
                                                            alpha=alpha, size=(num_edges, num_nodes))
                                    
                                    if self.concat is True:
                                        out = tf.reshape(out,[-1, self.heads * self.out_channels])
                                    else:
                                        out = tf.math.reduce_mean(out,axis=1)

                                    if self.bias is not None:
                                            out = out + self.bias

                                    return out
                                def message(self, x_j: tf.Tensor, norm_i: tf.Tensor, alpha: tf.Tensor) -> tf.Tensor:
                                        H, F = self.heads, self.out_channels
                                        
                                        norm_i = np.resize(norm_i,x_j.get_shape()[0])
                                        if x_j.dtype is not norm_i.dtype:
                                            norm_i = tf.cast(norm_i,x_j.dtype)
                                        out = tf.reshape(norm_i,[-1, 1, 1]) * tf.reshape(x_j,[-1, H, F])

                                        if alpha is not None:
                                            out = tf.reshape(alpha,[-1, self.heads, 1]) * out

                                        return out


                            # Implementation
                            """"
                            in_channels, out_channels = (16, 32)
                            hyperedge_index = tf.constant([[0, 0, 1, 1, 2, 3], [0, 1, 0, 1, 0, 1]])
                            hyperedge_weight = tf.constant([1.0, 0.5])
                            num_nodes = int(tf.reduce_max(hyperedge_index[0])) + 1
                            num_edges = int(tf.reduce_max(hyperedge_index[1])) + 1
                            x = tf.random.normal(([num_nodes, in_channels]))
                            hyperedge_attr = tf.random.normal(([num_edges, in_channels]))

                            conv = HypergraphConv(in_channels, out_channels)

                            out = conv(x, hyperedge_index)

                            out = conv(x, hyperedge_index, hyperedge_weight)


                            conv = HypergraphConv(in_channels, out_channels, use_attention=True,
                                                      heads=2)
                            out = conv(x, hyperedge_index, hyperedge_attr=hyperedge_attr)

                            out = conv(x, hyperedge_index, hyperedge_weight, hyperedge_attr)

                            conv = HypergraphConv(in_channels, out_channels, use_attention=True,
                                                      heads=2, concat=False, dropout=0.5)
                            out = conv(x, hyperedge_index, hyperedge_weight, hyperedge_attr)
			   """


                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>The hypergraph convolutional operator from the <a href="https://arxiv.org/abs/1901.08150" target="_blank">“Hypergraph Convolution and Hypergraph Attention"</a> paper</p>
                  <img class = "equationclass" src="assets/images/equation/hyper_graph.PNG" alt="">
                  <p>where <strong>H &isin; {0, 1}<sup>NxM</sup></strong> is the incidence matrix, <strong>W &isin;R<sup>M</sup></strong>is the diagonal hyperedge weight matrix, and <strong>D</strong> and <strong>B</strong> are the corresponding degree matrices.</p>
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_channels   </strong>
                          – Size of each input sample, or <mark>-1</mark> to derive the size from the first input(s) to the forward method. A tuple corresponds to the sizes of source and target dimensionalities.
                      </p>
                      </li>
                      <li>
                        <p><strong>out_channels    </strong>
                          – Size of each output sample.
                      </p>
                      </li>
                      <li>
                        <p><strong>use_attention   </strong>
                          – If set to <mark>True</mark>, attention will be added to this layer. 
                      </p>
                      </li>
                      <li>
                        <p><strong>heads  </strong>
                          – Number of multi-head-attentions.
                      </p>
                      </li>
                      <li>
                        <p><strong>concat </strong>
                          – If set to <mark>False</mark>, the multi-head attentions are averaged instead of concatenated. 
                      </p>
                      </li>
                      <li>
                        <p><strong>negative_slope  </strong>
                          – LeakyReLU angle of the negative slope.  
                      </p>
                      </li>
                      <li>
                        <p><strong>dropout   </strong>
                          –  Dropout probability of the normalized attention coefficients which exposes each node to a stochastically sampled neighborhood during training.
                      </p>
                      </li>
                      <li>
                        <p><strong>bias   </strong>
                          – If set to <mark>False</mark>, the layer will not learn an additive bias.
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="wl_cont_conv">WLConvContinuous</span>( **kwargs)</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#wl_cont_conv_code">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="wl_cont_conv_code" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR WL_CONV_CONTINUOUS</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            from typing import Union
                            from message_passing import MessagePassing
                            from Scatter import scatter_add
                            from tf_typing import OptPairTensor, OptTensor,Size 
                            import numpy as np

                            def masked_fill(t, mask, value):
                                return tf.cast(t,dtype=tf.float32) * (1 - tf.cast(mask, tf.float32)) + value * tf.cast(mask, tf.float32)

                            class WLConvContinuous(MessagePassing):
                                def __init__(self, **kwargs):
                                    super().__init__(aggr='add', **kwargs)

                                def __call__(self, x: Union[tf.Tensor, OptPairTensor], edge_index: tf.Tensor,
                                            edge_weight: OptTensor = None, size: Size = None) -> tf.Tensor:
                                    """"""
                                    if isinstance(x, tf.Tensor):
                                        x: OptPairTensor = (x, x)

                                    if edge_weight is None:
                                        edge_weight = tf.ones(edge_index.get_shape()[1], dtype = x[0].dtype)

                                    # propagate_type: (x: OptPairTensor, edge_weight: Tensor)
                                    out = self.propagate(edge_index, x=x, edge_weight=edge_weight,
                                                        size=size)
                                    
                                    out = tf.constant(out, dtype=tf.float32)
                                    deg = scatter_add(edge_weight, edge_index[1],dim=0, dim_size=out.get_shape()[0])
                                    deg_inv = 1. / deg
                                    deg_inv = masked_fill(deg_inv,deg_inv == float('inf'), 0)
                                    deg_inv = deg_inv.numpy()
                                    deg_inv[np.isnan(deg_inv)] = 0
                                    deg_inv = tf.constant(deg_inv)
                                  
                                    out = tf.reshape(deg_inv,[-1, 1]) * out

                                    x_dst = x[1]
                                    if x_dst is not None:
                                        if x_dst.dtype is not out.dtype:
                                            x_dst = tf.cast(x_dst, dtype=out.dtype)
                                        out = 0.5 * (x_dst + out)

                                    return out

                                def message(self, x_j: tf.Tensor, edge_weight: tf.Tensor) -> tf.Tensor:
                                    return tf.reshape(edge_weight,[-1, 1]) * x_j



                            #Implementation
                            """
                            w = WLConvContinuous()

                            edge_index = tf.constant([[0, 1, 1, 2], [1, 0, 2, 1]], dtype=tf.int64)
                            x = tf.constant([[-1], [0], [1]], dtype=tf.float32)
                            out = w(x,edge_index)
                            print(out)


                            x1 = tf.random.normal([4, 8])
                            x2 = tf.random.normal([2, 8])
                            edge_index = tf.constant([[0, 1, 2, 3], [0, 0, 1, 1]])
                            edge_weight = tf.random.normal([edge_index.get_shape()[1]])

                            out = w((x1, None), edge_index, edge_weight, size=(4, 2))
                            print(out)

                            out = w((x1, x2), edge_index, edge_weight)
                            print(out)

                            """

                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>The Weisfeiler Lehman operator from the <a href="https://arxiv.org/abs/1906.01277" target="_blank">“Wasserstein Weisfeiler-Lehman Graph Kernels"</a> paper. Refinement is done though a degree-scaled mean aggregation and works on nodes with continuous attributes:</p>
                  <img class = "equationclass" src="assets/images/equation/wl_cont.PNG" alt="">
                  <p>where <strong>e<sub>j,i</sub></strong> denotes the edge weight from source node <mark>j</mark> to target node <mark>i</mark></p>
                </div>
                
               
            
          </section>

          <hr class="divider">

          <!-- Aggregation Operators
		============================ -->
    <section id="idocs_aggr">
      <h2>Aggregation Operators</h2>
      <p class="lead">
      <ul class="simple">
        <li>
          <p>
            <a class="reference internal" href="#aggr">Aggregation</a>
          </p>
        </li>
        <li>
          <p>
            <a class="reference internal" href="#multiaggr">MultiAggregation</a>
          </p>
        </li>
        <li>
          <p>
            <a class="reference internal" href="#sumaggr">SumAggregation</a>
          </p>
        </li>
        <li>
          <p>
            <a class="reference internal" href="#meanaggr">MeanAggregation</a>
          </p>
        </li>
        <li>
          <p>
            <a class="reference internal" href="#maxaggr">MaxAggregation</a>
          </p>
        </li>
        <li>
          <p>
            <a class="reference internal" href="#minaggr">MinAggregation</a>
          </p>
        </li>
        <li>
          <p>
            <a class="reference internal" href="#mulaggr">MulAggregation</a>
          </p>
        </li>
        <li>
          <p>
            <a class="reference internal" href="#varaggr">VarAggregation</a>
          </p>
        </li>
        <li>
          <p>
            <a class="reference internal" href="#stdaggr">StdAggregation</a>
          </p>
        </li>
        <li>
          <p>
            <a class="reference internal" href="#softaggr">SoftmaxAggregation</a>
          </p>
        </li>
        <li>
          <p>
            <a class="reference internal" href="#powaggr">PowerMeanAggregation</a>
          </p>
        </li>
        <li>
          <p>
            <a class="reference internal" href="#medaggr">MedianAggregation</a>
          </p>
        </li>
        <li>
          <p>
            <a class="reference internal" href="#quanaggr">QuantileAggregation</a>
          </p>
        </li>
        <li>
          <p>
            <a class="reference internal" href="#lstmaggr">LSTMAggregation</a>
          </p>
        </li>
        <li>
          <p>
            <a class="reference internal" href="#sortaggr">SortAggregation</a>
          </p>
        </li>
        <li>
          <p>
            <a class="reference internal" href="#attaggr">AttentionalAggregation</a>
          </p>
        </li>
        <li>
          <p>
            <a class="reference internal" href="#degreeggr">DegreeScalerAggregation</a>
          </p>
        </li>
      </ul>
        
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="aggr">Aggregation</span> </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#aggr_code">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="aggr_code" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR AGGREGATION</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>   
                    from typing import Optional, Tuple
                    import tensorflow as tf
                    import numpy as np
                    from dense import to_dense_batch
                    from Scatter import scatter

                    class Aggregation(tf.keras.layers.Layer):
                        
                        _validate = __debug__
                        
                        
                        def call(self, x: tf.Tensor, index: Optional[tf.Tensor] = None,
                                    ptr: Optional[tf.Tensor] = None, dim_size: Optional[int] = None,
                                    dim: int = -2) -> tf.Tensor:
                        
                            pass


                        def reset_parameters(self):
                            pass
                        
                        @staticmethod
                        def set_validate_args(value: bool):
                            
                            Aggregation._validate = value  #, **kwargs


                        def __call__(self, x: tf.Tensor, index: Optional[tf.Tensor] = None,
                                    ptr: Optional[tf.Tensor] = None, dim_size: Optional[int] = None, dim: int = -2) -> tf.Tensor:
                            if dim >= x.numpy().ndim or dim < -x.numpy().ndim:
                                raise ValueError(f"Encountered invalid dimension '{dim}' of "
                                                f"source tensor with {x.dim()} dimensions")
                            if index is None and ptr is None:   
                                
                                index = tf.cast(tf.zeros(x.get_shape()[dim]), dtype=x.dtype)
                            if ptr is not None:
                                if dim_size is None:
                                    dim_size = tf.size(ptr).numpy() - 1
                                elif dim_size != tf.size(ptr).numpy() - 1:
                                    raise ValueError(f"Encountered invalid 'dim_size' (got "
                                                      f"'{dim_size}' but expected "
                                                      f"'{tf.size(ptr).numpy() - 1}')")
                                    
                            if index is not None:
                                if dim_size is None:
                                    dim_size = int(tf.math.reduce_max(index)) + 1 if tf.size(index).numpy() > 0 else 0
                                elif self._validate:
                                    if tf.size(index).numpy() > 0 and dim_size <= int(tf.math.reduce_max(index)):
                                        raise ValueError(f"Encountered invalid 'dim_size' (got "
                                                        f"'{dim_size}' but expected "
                                                        f">= '{int(tf.math.reduce_max(index)) + 1}')")

                            return super().__call__(x, index, ptr, dim_size, dim)                  #  , **kwargs
                        
                        
                        
                        def __repr__(self) -> str:
                            return f'{self.__class__.__name__}()'

                        

                        def assert_index_present(self, index: Optional[tf.Tensor]):
                            
                            if index is None:
                                raise NotImplementedError(
                                    "Aggregation requires 'index' to be specified")

                        def assert_sorted_index(self, index: Optional[tf.Tensor]):
                            if index is not None and not tf.math.reduce_any(index[:-1] <= index[1:]):  
                                raise ValueError("Can not perform aggregation since the 'index' "
                                                "tensor is not sorted")

                        def assert_two_dimensional_input(self, x: tf.Tensor, dim: int):
                            if np.ndim(x) != 2:
                                raise ValueError(f"Aggregation requires two-dimensional inputs "
                                                f"(got '{np.ndim(x)}')")

                            if dim not in [-2, 0]:
                                raise ValueError(f"Aggregation needs to perform aggregation in "
                                                f"first dimension (got '{dim}')")
                                  
                        def reduce(self, x: tf.Tensor, index: Optional[tf.Tensor] = None,
                                  ptr: Optional[tf.Tensor] = None, dim_size: Optional[int] = None,
                                  dim: int = -2, reduce: str = 'add') -> tf.Tensor:

                            if ptr is not None:
                                ptr = self.expand_left(ptr, dim, dims=np.ndim(x))
                                
                                return tf.math.segment_sum(x, ptr)

                            assert index is not None
                            return scatter(x, index, reduce=reduce) 
                            
                        
                        def to_dense_batch(self, x: tf.Tensor, index: Optional[tf.Tensor] = None,
                                          ptr: Optional[tf.Tensor] = None,
                                          dim_size: Optional[int] = None, dim: int = -2,
                                          fill_value: float = 0.) -> Tuple[tf.Tensor, tf.Tensor]:

                            
                            self.assert_index_present(index)
                            self.assert_sorted_index(index)
                            self.assert_two_dimensional_input(x, dim)
                            return to_dense_batch(x, index, batch_size=dim_size,
                                                  fill_value=fill_value)
                    def expand_left(ptr: tf.Tensor, dim: int, dims: int) -> tf.Tensor:
                        for _ in range(dims + dim if dim < 0 else dim):
                                
                              
                            ptr = tf.expand_dims(ptr,0)
                        return ptr

                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div><p>An abstract base class for implementing custom aggregations.</p></div>
       
        
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="aggr">MultiAggregation</span> </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#multaggr">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="multaggr" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR MULTI_AGGREGATION</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>   
                    
                      import copy
                      import inspect
                      from typing import Any, Dict, List, Optional, Union
                      import tensorflow as tf
                      from tensorflow.keras.layers import MultiHeadAttention
                      from tensorflow.keras.layers import Dense
                      from base import Aggregation

                      def normalize_string(s: str) -> str:
                          return s.lower().replace('-', '').replace('_', '').replace(' ', '')

                      def resolver(classes: List[Any], class_dict: Dict[str, Any],
                                  query: Union[Any, str], base_cls: Optional[Any],
                                  base_cls_repr: Optional[str], *args, **kwargs):
                          if not isinstance(query, str):
                              return query

                          query_repr = normalize_string(query)
                          if base_cls_repr is None:
                              base_cls_repr = base_cls.__name__ if base_cls else ''
                          base_cls_repr = normalize_string(base_cls_repr)
                          for key_repr, cls in class_dict.items():
                              if query_repr == key_repr:
                                  if inspect.isclass(cls):
                                      obj = cls(*args, **kwargs)
                                      assert callable(obj)
                                      return obj
                                  assert callable(cls)
                                  return cls
                              for cls in classes:
                                  cls_repr = normalize_string(cls.__name__)
                                  if query_repr in [cls_repr, cls_repr.replace(base_cls_repr, '')]:
                                      if inspect.isclass(cls):
                                          obj = cls(*args, **kwargs)
                                          assert callable(obj)
                                          return obj
                                      assert callable(cls)
                                      return cls

                          choices = set(cls.__name__ for cls in classes) | set(class_dict.keys())
                          raise ValueError(f"Could not resolve '{query}' among choices {choices}")

                      def swish(x: tf.Tensor) -> tf.Tensor:
                          x = tf.cast(x,dtype=tf.
                                      float32)
                          return x * tf.math.sigmoid(x)

                      def activation_resolver(query: Union[Any, str] = 'relu', *args, **kwargs):
                          base_cls = tf.keras.layers.Layer
                          base_cls_repr = 'Act'
                          acts = [
                              act for act in vars(tf.keras.layers.Activation).values()
                              if isinstance(act, type) and issubclass(act, base_cls)
                          ]
                          acts += [
                              swish,
                          ]
                          act_dict = {}
                          return resolver(acts, act_dict, query, base_cls, base_cls_repr, *args,
                                          **kwargs)



                      def aggregation_resolver(query: Union[Any, str], *args, **kwargs):
                          import base as aggr
                          import basic
                          base_cls = aggr.Aggregation
                          aggrs = [
                              aggr for aggr in vars(aggr).values()
                              if isinstance(aggr, type) and issubclass(aggr, base_cls)
                          ]
                          aggr_dict = {
                              'sum'   : basic.SumAggregation,
                              'mean'  : basic.PowerMeanAggregation,
                              'max'   : basic.MaxAggregation,
                              'min'   : basic.MinAggregation,
                              'add'   : basic.SumAggregation
                          }
                          return resolver(aggrs, aggr_dict, query, base_cls, None, *args, **kwargs)
                  

                      class MultiAggregation(Aggregation):
                          def __init__(
                              self,
                              aggrs: List[Union[Aggregation, str]],
                              aggrs_kwargs: Optional[List[Dict[str, Any]]] = None,
                              mode: Optional[str] = 'cat',
                              mode_kwargs: Optional[Dict[str, Any]] = None,
                          ):

                              super().__init__()

                              if not isinstance(aggrs, (list, tuple)):
                                  raise ValueError(f"'aggrs' of '{self.__class__.__name__}' should "
                                                  f"be a list or tuple (got '{type(aggrs)}').")

                              if len(aggrs) == 0:
                                  raise ValueError(f"'aggrs' of '{self.__class__.__name__}' should "
                                                  f"not be empty.")

                              if aggrs_kwargs is None:
                                  aggrs_kwargs = [{}] * len(aggrs)
                              elif len(aggrs) != len(aggrs_kwargs):
                                  raise ValueError(f"'aggrs_kwargs' with invalid length passed to "
                                                  f"'{self.__class__.__name__}' "
                                                  f"(got '{len(aggrs_kwargs)}', "
                                                  f"expected '{len(aggrs)}'). Ensure that both "
                                                  f"'aggrs' and 'aggrs_kwargs' are consistent.")

                              self.aggrs = [
                                  aggregation_resolver(aggr, **aggr_kwargs)
                                  for aggr, aggr_kwargs in zip(aggrs, aggrs_kwargs)
                              ]

                              self.mode = mode
                              mode_kwargs = copy.copy(mode_kwargs or {})
                              self.in_channels = mode_kwargs.pop('in_channels', None)
                              self.out_channels = mode_kwargs.pop('out_channels', None)
                              if mode == 'proj' or mode == 'attn':
                                  if len(aggrs) == 1:
                                      raise ValueError("Multiple aggregations are required for "
                                                      "'proj' or 'attn' combine mode.")

                                  if (self.in_channels and self.out_channels) is None:
                                      raise ValueError(
                                          f"Combine mode '{mode}' must have `in_channels` "
                                          f"and `out_channels` specified.")

                                  if isinstance(self.in_channels, int):
                                      self.in_channels = (self.in_channels, ) * len(aggrs)

                                  if mode == 'proj':
                                      self.lin = tensorflow.keras.layers.Dense(
                                          sum(self.in_channels),
                                          self.out_channels,
                                          **mode_kwargs,
                                      )

                                  if mode == 'attn':
                                      self.lin_heads = [
                                          tensorflow.keras.layers.Dense(channels, self.out_channels)
                                          for channels in self.in_channels
                                      ]
                                      num_heads = mode_kwargs.pop('num_heads', 1)
                                      self.multihead_attn = MultiheadAttention(
                                          self.out_channels,
                                          num_heads,
                                          **mode_kwargs,
                                      )

                              dense_combine_modes = [
                                  'sum', 'mean', 'max', 'min', 'logsumexp', 'std', 'var'
                              ]
                              if mode in dense_combine_modes:
                                  self.dense_combine = getattr(tf, mode)
                      ####
                          def reset_parameters(self):
                              for aggr in self.aggrs:
                                  aggr.reset_parameters()
                              if hasattr(self, 'lin'):
                                  self.lin.reset_parameters()
                              if hasattr(self, 'lin_heads'):
                                  for lin in self.lin_heads:
                                      lin.reset_parameters()
                              if hasattr(self, 'multihead_attn'):
                                  self.multihead_attn._reset_parameters()

                          def get_out_channels(self, in_channels: int) -> int:
                              if self.out_channels is not None:
                                  return self.out_channels
                              # TODO: Support having customized `out_channels` in each aggregation
                              if self.mode == 'cat':
                                  return in_channels * len(self.aggrs)
                              return in_channels

                          def call(self, x: tf.Tensor, index: Optional[tf.Tensor] = None,
                                      ptr: Optional[tf.Tensor] = None, dim_size: Optional[int] = None,
                                      dim: int = -2) -> tf.Tensor:
                              outs = []
                              for aggr in self.aggrs:
                                  outs.append(aggr(x, index))
                                  print(aggr)
                              print(outs)
                              return self.combine(outs) if len(outs) > 1 else outs[0]

                          def combine(self, inputs: List[tf.Tensor]) -> tf.Tensor:
                              if self.mode in ['cat', 'proj']:
                                  out = tf.concat(inputs, -1)
                                  return self.lin(out) if hasattr(self, 'lin') else out

                              if hasattr(self, 'multihead_attn'):
                                  x = tf.stack(
                                      [head(x) for x, head in zip(inputs, self.lin_heads)],
                                      0,
                                  )
                                  attn_out, _ = self.multihead_attn(x, x, x)
                                  return tf.math.reduce_mean (attn_out)

                              if hasattr(self, 'dense_combine'):
                                  out = self.dense_combine(tf.stack(inputs, 0), dim=0)
                                  return out if isinstance(out, tf.Tensor) else out[0]

                              raise ValueError(f"Combine mode '{self.mode}' is not supported.")

                          def __repr__(self) -> str:
                              args = [f'  {aggr}' for aggr in self.aggrs]
                              return '{}([\n{}\n], mode={})'.format(
                                  self.__class__.__name__,
                                  ',\n'.join(args),
                                  self.mode,
                              )


                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div><p>Performs aggregations with one or more aggregators and combines aggregated results, as described in the
          <a href="https://arxiv.org/abs/2004.05718"  target="_blank">"Principal Neighbourhood Aggregation for Graph Nets"</a>and <a href="https://arxiv.org/abs/2104.01481" target="_blank">"Do We Need Anisotropic Graph Neural Networks?</a>
        papers</p>
        </div>
  
        <dl class="field-list simple">
          <dt class="field-odd">
            <h4><strong>Parameters</strong></h4>
          </dt>
          <dd class="field-odd">
            <ul class="simple_parameter">
              <li>
                <p><strong>aggrs</strong>
                 – The list of aggregation schemes to use.
              </p>
              </li>
              <li>
                <p><strong>aggrs_kwargs</strong>
                 –  Arguments passed to the respective aggregation function in case it gets automatically resolved. 
              </p>
              </li>
              <li>
                <p><strong>mode</strong>
                 – The combine mode to use for combining aggregated results from multiple aggregations. 
              </p>
              </li>
              <li>
                <p><strong>mode_kwargs  </strong>
                  Arguments passed for the combine <mark>mode</mark>. When <mark>"proj"</mark> or <mark>"attn"</mark> is used as the combine <mark>mode</mark>, <mark>in_channels</mark> (int or tuple) and <mark>out_channels</mark> (int) are needed to be specified respectively for the size of each input sample to combine from the respective aggregation outputs and the size of each output sample after combination. When <mark>"attn"</mark> mode is used, <mark>num_heads</mark> (int) is needed to be specified for the number of parallel attention heads. . 
              </p>
              </li>
              
            </ul>
          </dd>
        </dl>
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="sumaggr">SumAggregation</span> </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#basic">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="basic" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR BASIC</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>  
                    import tensorflow as tf
                    from base import Aggregation
                    from typing import Optional
                    from attention import *

                    def generate_index_tensor(num):
                        li = []
                        i,x = 0,0 
                        while i < num:
                            li.append(x)
                            i += 1
                        return li 

                    class SumAggregation(Aggregation):
                        def __call__(self , x : tf.Tensor, index :tf.Tensor = None , dim = -2):
                            if index is None : 
                                index = generate_index_tensor(tf.shape(x).numpy()[0])
                            return self.reduce(x,index,reduce='sum')

                    class MeanAggregation(Aggregation):
                          def __call__(self, x: tf.Tensor, index: Optional[tf.Tensor] = None,
                                    ptr: Optional[tf.Tensor] = None, dim_size: Optional[int] = None,
                                    dim: int = -2) -> tf.Tensor:
                                if index is None :
                                    index = generate_index_tensor(tf.shape(x).numpy()[0])
                                    index = tf.constant(index)
                                if dim_size is None:
                                    dim_size = 1
                                print(index)
                                return self.reduce(x, index, ptr, dim_size, dim, reduce='mean')

                    class MaxAggregation(Aggregation):  
                        def __call__(self, x: tf.Tensor, index: Optional[tf.Tensor] = None,
                                    ptr: Optional[tf.Tensor] = None, dim_size: Optional[int] = None,
                                    dim: int = -2) -> tf.Tensor:
                            if index is None and ptr is None: 
                                index = tf.cast(tf.ones(x.get_shape()[dim]), dtype=x.dtype)
                            return self.reduce(x, index, ptr, dim_size, dim, reduce='max')

                    class MinAggregation(Aggregation):
                      
                        def call(self, x: tf.Tensor, index: Optional[tf.Tensor] = None,
                                    ptr: Optional[tf.Tensor] = None, dim_size: Optional[int] = None,
                                    dim: int = -2) -> tf.Tensor:
                            return self.reduce(x, index, ptr, dim_size, dim, reduce='min')

                    class MulAggregation(Aggregation):
                        def call(self, x: tf.Tensor, index: Optional[tf.Tensor] = None,
                                    ptr: Optional[tf.Tensor] = None, dim_size: Optional[int] = None,
                                    dim: int = -2) -> tf.Tensor:
                            self.assert_index_present(index)

                            return self.reduce(x, index, None, dim_size, dim, reduce='mul')

                    class VarAggregation(Aggregation):
                        def call(self, x: tf.Tensor, index: Optional[tf.Tensor] = None,
                                    ptr: Optional[tf.Tensor] = None, dim_size: Optional[int] = None,
                                    dim: int = -2) -> tf.Tensor: 
                            mean = self.reduce(x, index, ptr, dim_size, dim,reduce = 'mean')
                            mean_2 = self.reduce(x * x, index, ptr, dim_size, dim, reduce='mean')
                            return mean_2 - mean * mean

                    class StdAggregation(Aggregation):
                        var_aggr = VarAggregation()

                        def call(self, x: tf.Tensor, index: Optional[tf.Tensor] = None,
                                    ptr: Optional[tf.Tensor] = None, dim_size: Optional[int] = None,
                                    dim: int = -2) -> tf.Tensor:
                            var = tf.cast(self.var_aggr(x, index, ptr, dim_size, dim),dtype=tf.float32)
                            
                            return tf.math.sqrt(var + 1e-5).numpy()

                    class SoftmaxAggregation(Aggregation):
                        def __init__(self, t: float = 1.0, learn: bool = False,
                                    semi_grad: bool = False, channels: int = 1):
                            super(SoftmaxAggregation , self).__init__()

                            if learn and semi_grad:
                                raise ValueError(
                                    f"Cannot enable 'semi_grad' in '{self.__class__.__name__}' in "
                                    f"case the temperature term 't' is learnable")

                            if not learn and channels != 1:
                                raise ValueError(f"Cannot set 'channels' greater than '1' in case "
                                                f"'{self.__class__.__name__}' is not trainable")

                            self._init_t = t
                            self.learn = learn
                            self.semi_grad = semi_grad
                            self.channels = channels

                            self.t = tf.Variable(tf.constant(channels)) if learn else t
                            self.reset_parameters()

                        def reset_parameters(self):
                            if isinstance(self.t, tf.Tensor):
                                    tf.fill(tf.size(self._init_t),self._init_t)

                        def __call__(self, x: tf.Tensor, index: tf.Tensor,
                                    ptr: Optional[tf.Tensor] = None, dim_size: int = None,
                                    dim: int = -2) -> tf.Tensor:

                            t = self.t
                            if self.channels != 1:
                                self.assert_two_dimensional_input(x, dim)
                                assert isinstance(t, tf.Tensor)
                                t = tf.reshape(t,-1)
                            alpha=x
                            if not isinstance(t, (int, float)) or t != 1:
                                alpha = x * t
                          
                            if not self.learn and self.semi_grad:
                                with tf.stop_gradient(x):
                                    alpha = tf_softmax(alpha, index)
                            
                            else:
                                alpha = tf_softmax(alpha, index)
                            return self.reduce(x * alpha, index, reduce='sum').numpy()

                        def __repr__(self) -> str:
                            return (f'{self.__class__.__name__}(learn={self.learn})')

                    class PowerMeanAggregation(Aggregation):
                        def __init__(self, p: float = 1.0, learn: bool = False, channels: int = 1):
                            super().__init__()

                            if not learn and channels != 1:
                                raise ValueError(f"Cannot set 'channels' greater than '1' in case "
                                                f"'{self.__class__.__name__}' is not trainable")

                            self._init_p = p
                            self.learn = learn
                            self.channels = channels

                            self.p = tf.Variable(tf.constant(channels)) if learn else p
                            self.reset_parameters()

                        def reset_parameters(self):
                            if isinstance(self.p, tf.Tensor):
                                tf.fill(self.p,self._init_p)
                        def __call__(self, x: tf.Tensor, index: Optional[tf.Tensor] = None,
                                    ptr: Optional[tf.Tensor] = None, dim_size: Optional[int] = None,
                                    dim: int = -2) -> tf.Tensor:

                            p = self.p
                            if self.channels != 1:
                                assert isinstance(p, tf.Tensor)
                                self.assert_two_dimensional_input(x, dim)
                                p = tf.reshape(p,[-1, self.channels])

                            if not isinstance(p, (int, float)) or p != 1:
                                x = tf.clip_by_value(x,clip_value_min=0, clip_value_max=100).pow(p)
                            if index is None:
                                if x.numpy().ndim > 2:
                                    index = tf.zeros((x.get_shape()[1]),dtype=tf.int32)
                                else:
                                    index = tf.zeros((x.get_shape()[0]),dtype=tf.int32)
                            out = self.reduce(x, index, ptr, dim_size, dim, reduce='mean')

                            if not isinstance(p, (int, float)) or p != 1:
                                out = tf.clip_by_value(out,clip_value_min=0, clip_value_max=100).pow(1. / p)

                            return out 
                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div>
          <p>An aggregation operator that computes the sum of features across a set of elements.</p>
          <img class="equationclass" src="assets\images\equation/sumaggr.PNG" alt="batch">
        </div>
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="meanaggr">MeanAggregation</span> </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#basic">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="basic" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR BASIC</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>   
                  
                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div>
          <p>An aggregation operator that computes the average of features across a set of elements.Averaging features across a set of elements</p>
          <img class="equationclass" src="assets\images\equation/meanaggr.PNG" alt="batch">
        </div>
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="maxaggr">MaxAggregation</span> </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#basic">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="basic" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR BASIC</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>   
                  
                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div>
          <p>An aggregation operator that computes the max of features across a set of elements.</p>
          <img class="equationclass" src="assets\images\equation/maxaggr.PNG" alt="batch">
        </div>
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="minaggr">MinAggregation</span> </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#basic">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="basic" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR BASIC</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>   
                  
                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div>
          <p>An aggregation operator that finds the feature-wise smallest set of elements.</p>
          <img class="equationclass" src="assets\images\equation/minaggr.PNG" alt="batch">
        </div>
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="mulaggr">MulAggregation</span> </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#basic">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="basic" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR BASIC</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>   
                  
                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div>
          <p>An aggregation operator that computes the multiples  of features across a set of elements.</p>
          <img class="equationclass" src="assets\images\equation/mulaggr.PNG" alt="batch">
        </div>
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="varaggr">VarAggregation</span> </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#basic">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="basic" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR BASIC</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>   
                  
                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div>
          <p>An aggregation operator that computes the variance of a set of elements based on their features.</p>
          <img class="equationclass" src="assets\images\equation/varaggr.PNG" alt="batch">
        </div>
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="srdaggr">StdAggregation</span> </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#basic">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="basic" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR BASIC</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>   
                  
                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div>
          <p>An aggregation operator that computes the feature-wise standard deviation across a set of elements.</p>
          <img class="equationclass" src="assets\images\equation/stdaggr.PNG" alt="batch">
        </div>
        
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="softaggr">SoftmaxAggregation</span>( t: float = 1.0, learn: bool = False, semi_grad: bool = False, channels: int = 1) </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#basic">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="basic" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR BASIC</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>   
                  
                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div>
          <p>The softmax aggregation operator based on a temperature term, as described in the <a href="https://arxiv.org/abs/2006.07739" target="_blank">
            DeeperGCN: All You Need to Train Deeper GCNs</a> paper</p>
          <img class="equationclass" src="assets\images\equation/softaggr.PNG" alt="batch">
        </div>
        <dl class="field-list simple">
          <dt class="field-odd">
            <h4><strong>Parameters</strong></h4>
          </dt>
          <dd class="field-odd">
            <ul class="simple_parameter">
              <li>
                <p><strong>t</strong>
                 –  Initial inverse temperature for softmax aggregation.
              </p>
              </li>
              <li>
                <p><strong>learn </strong>
                 – If set to <mark>True</mark>, will learn the value <mark>t</mark> for softmax aggregation dynamically. 
              </p>
              </li>
              <li>
                <p><strong>semi_grad </strong>
                 – If set to <mark>True</mark>, will turn off gradient calculation during softmax computation. Therefore, only semi-gradient is used during backpropagation. Useful for saving memory when <mark>t</mark> is not learnable. 
              </p>
              </li>
              <li>
                <p><strong>channels   </strong>
                  Number of channels to learn from . If set to a value greater than <mark>1</mark>, t will be learned per input feature channel. This requires compatible shapes for the input to the forward calculation.  
              </p>
              </li>
              
            </ul>
          </dd>
        </dl>
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="powaggr">PowerMeanAggregation</span>( p: float = 1.0, learn: bool = False, channels: int = 1) </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#basic">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="basic" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR BASIC</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>   
                  
                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div>
          <p>The powermean aggregation operator based on a power term, as described in the <a href="https://arxiv.org/abs/2006.07739" target="_blank">DeeperGCN: All You Need to Train Deeper GCNs</a> paper</p>
          <img class="equationclass" src="assets\images\equation/powermeaggr.PNG" alt="batch">
        </div>
        <dl class="field-list simple">
          <dt class="field-odd">
            <h4><strong>Parameters</strong></h4>
          </dt>
          <dd class="field-odd">
            <ul class="simple_parameter">
              <li>
                <p><strong>p</strong>
                 –   Initial power for powermean aggregation. 
              </p>
              </li>
              <li>
                <p><strong>learn </strong>
                 – If set to <mark>True</mark>, will learn the value <mark>t</mark> for softmax aggregation dynamically. 
              </p>
              </li>
             
              <li>
                <p><strong>channels   </strong>
                  Number of channels to learn from . If set to a value greater than <mark>1</mark>, t will be learned per input feature channel. This requires compatible shapes for the input to the forward calculation.  
              </p>
              </li>
              
            </ul>
          </dd>
        </dl>
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="medaggr">MedianAggregation</span>( fill_value: float = 0.0) </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#quantile">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="quantile" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR QUANTILE</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>   
                    from typing import List, Optional, Union
                    import tensorflow as tf
                    import numpy as np
                    import math
                    from base import Aggregation

                    def tf_index_select(input_, dim, indices):
                        """
                        input_(tensor): input tensor
                        dim(int): dimension
                        indices(list): selected indices list
                        """
                        shape = input_.get_shape().as_list()
                        if dim == -1:
                            dim = len(shape)-1
                        shape[dim] = 1
                        
                        tmp = []
                        for idx in indices:
                            begin = [0]*len(shape)
                            begin[dim] = idx
                            tmp.append(tf.slice(input_, begin, shape))
                        res = tf.concat(tmp, axis=dim)
                        
                        return res

                    def fraction(input):
                        input = input.tolist()
                        out = []
                        for i in input:
                            if i > 0:
                                sign = 1
                            else:
                                sign = -1
                            res =  i - math.floor(abs(i)) * sign
                            out.append(res)
                        return tf.convert_to_tensor(out)

                    def masked_fill(t, mask, value):
                        return t * (1 - tf.cast(mask, tf.float32)) + value * tf.cast(mask, tf.float32)


                    class QuantileAggregation(Aggregation):
                        interpolations = {'linear', 'lower', 'higher', 'nearest', 'midpoint'}

                        def __init__(self, q: Union[float, List[float]],
                                    interpolation: str = 'linear', fill_value: float = 0.0):
                            super().__init__()

                            qs = [q] if not isinstance(q, (list, tuple)) else q
                            if len(qs) == 0:
                                raise ValueError("Provide at least one quantile value for `q`.")
                            if not all(0. <= quantile <= 1. for quantile in qs):
                                raise ValueError("`q` must be in the range [0, 1].")
                            if interpolation not in self.interpolations:
                                raise ValueError(f"Invalid interpolation method "
                                                f"got ('{interpolation}')")

                            self.q = q
                            self.q = tf.Variable(tf.reshape(tf.constant(qs),[-1, 1]))
                            self.interpolation = interpolation
                            self.fill_value = fill_value
                        def __call__(self, x: tf.Tensor, index: Optional[tf.Tensor] = None,
                                    ptr: Optional[tf.Tensor] = None, dim_size: Optional[int] = None,
                                    dim: int = -2) -> tf.Tensor:
                            if index is None:
                                if x.numpy().ndim > 2:
                                    index = tf.zeros((x.get_shape()[1]),dtype=tf.int32)
                                else:
                                    index = tf.zeros((x.get_shape()[0]),dtype=tf.int32)

                            dim = x.numpy().ndim + dim if dim < 0 else dim
                            

                            assert index is not None  # Required for TorchScript.
                            count = tf.math.bincount(index, minlength=dim_size or 0)
                            cumsum = tf.math.cumsum(count, axis=0) - count
                            count = tf.cast(count, dtype = tf.float32)
                            cumsum = tf.cast(cumsum, dtype=tf.float32)
                            q_point = self.q * (count - 1) + cumsum
                            q_point = tf.reshape(tf.transpose(q_point),[-1])
                            print(q_point)
                            shape = [1] * x.numpy().ndim
                            
                            shape[dim] = -1

                            index = tf.broadcast_to(tf.reshape(index,shape),tf.shape(x))
                            print(index)

                            # Two sorts: the first one on the value,
                            # the second (stable) on the indices:
                            x= tf.sort(x, axis=dim)
                            x_perm  = tf.argsort(x, axis=dim)
                            index = np.take_along_axis(index.numpy(), x_perm.numpy(), axis=dim)
                            index= tf.sort(index, axis=dim)
                            index_perm  = tf.argsort(index, axis=dim)
                            
                            x = np.take_along_axis(x.numpy(), index_perm.numpy(), axis=dim)
                            x = tf.constant(x)

                            # Compute the quantile interpolations:
                            if self.interpolation == 'lower':
                                quantile = tf_index_select(x, dim, tf.cast(np.floor(q_point.numpy()),dtype=tf.int64))
                            elif self.interpolation == 'higher':
                                quantile = tf_index_select(x, dim, tf.cast(np.ceil(q_point.numpy()),dtype=tf.int64))
                            elif self.interpolation == 'nearest':
                                quantile = tf_index_select(x, dim, tf.cast(np.round(q_point.numpy()),dtype=tf.int64))
                            else:
                                l_quant = tf.cast(tf_index_select(x, dim, tf.cast(np.floor(q_point.numpy()),dtype=tf.int64)),dtype=tf.float32)
                                r_quant = tf.cast(tf_index_select(x, dim, tf.cast(np.ceil(q_point.numpy()),dtype=tf.int64)),dtype=tf.float32)
                                
                                if self.interpolation == 'linear':
                                    q_frac = tf.reshape(fraction(q_point.numpy()),shape)
                                    quantile = l_quant + (r_quant - l_quant) * q_frac
                                else:  # 'midpoint'
                                    quantile = 0.5 * l_quant + 0.5 * r_quant

                            # If the number of elements is zero, fill with pre-defined value:
                            mask = tf.reshape(tf.repeat((count == 0),tf.size(q)),shape)
                            out = masked_fill(quantile,mask, self.fill_value)
                          

                            if tf.size(q) > 1:
                                shape = list(out.shape)
                                shape = (shape[:dim] + [shape[dim] //tf.size(q), -1] +
                                        shape[dim + 2:])
                                out = tf.reshape(out,shape)

                            return out

                        def __repr__(self) -> str:
                            return (f'{self.__class__.__name__}(q={self._q})')
                        
                    class MedianAggregation(QuantileAggregation):
                        def __init__(self, fill_value: float = 0.0):
                            super().__init__(0.5, 'lower', fill_value)

                        def __repr__(self) -> str:
                                return f"{self.__class__.__name__}()"
                  
                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div>
          <p>An aggregation operator that returns the set's feature-wise median.That is, for every feature <strong>d</strong> , it computes</p>
          <img class="equationclass" src="assets\images\equation/medaggr.PNG" alt="batch">
        </div>
        <dl class="field-list simple">
          <dt class="field-odd">
            <h4><strong>Parameters</strong></h4>
          </dt>
          <dd class="field-odd">
            <ul class="simple_parameter">
              <li>
                <p><strong>fill_value</strong>
                 –   The default value in the case no entry is found for a given index. 
              </p>
              </li>
 
            </ul>
          </dd>
        </dl>
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="quanaggr">QuantileAggregation</span>( q: Union[float, List[float]], interpolation: str = 'linear', fill_value: float = 0.0) </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#quantile">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="quantile" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR QUANTILE</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>   
                  
                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div>
          <p>An aggregation operator which returns the <strong>q-th</strong> quantile of a feature-wise <strong>X</strong> set.</p>
          <img class="equationclass" src="assets\images\equation/quanaggr.PNG" alt="batch">
        </div>
        <dl class="field-list simple">
          <dt class="field-odd">
            <h4><strong>Parameters</strong></h4>
          </dt>
          <dd class="field-odd">
            <ul class="simple_parameter">
              <li>
                <p><strong>q</strong>
                 – The quantile value(s) q. Can be a scalar or a list of scalars in the range [0,1] . If more than a quantile is passed, the results are concatenated.
              </p>
              </li>
              <li>
                <p><strong>interpolation </strong>
                 – Interpolation method applied if the quantile point q.n lies between two values a &le; b . Can be one of the following:
                 <ul>
                  <li>
                    <mark>"lower"</mark>: Returns the one with lowest value.
                  </li>
                  <li>
                    <mark>"higher"</mark>: Returns the one with highest value.
                  </li>
                  <li>
                    <mark>"midpoint"</mark>: Returns the average of the two values.
                  </li>
                  <li>
                    <mark>"nearest"</mark>: Returns the one whose index is nearest to the quantile point.
                  </li>
                  <li>
                    <mark>"linear"</mark>: Returns a linear combination of the two elements.
                  </li>
                 </ul> 
              </p>
              </li>
             
              <li>
                <p><strong>fill_value    </strong>
                  –The default value in the case no entry is found for a given index.  
              </p>
              </li>
              
            </ul>
          </dd>
        </dl>
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="lstmaggr">LSTMAggregation</span>( in_channels: int, out_channels: int, **kwargs) </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#lstm">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="lstm" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR LSTM</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>   
                    from typing import Optional
                    import tensorflow as tf
                    from base import Aggregation
                    from dense import to_dense_batch

                    class LSTMAggregation(Aggregation):
                        def __init__(self, in_channels: int, out_channels: int, **kwargs):
                            super().__init__()
                            self.in_channels = in_channels
                            self.out_channels = out_channels
                            self.lstm = tf.keras.layers.RNN(
                                          tf.keras.layers.LSTMCell(out_channels),
                                          return_sequences=True,
                                          return_state=True)

                        def reset_parameters(self):
                            self.lstm.reset_parameters()
                        def __call__(self, x: tf.Tensor, index: Optional[tf.Tensor] = None,
                                    ptr: Optional[tf.Tensor] = None, dim_size: Optional[int] = None,
                                    dim: int = -2) -> tf.Tensor:
                            if index is None:
                                if x.numpy().ndim > 2:
                                    index = tf.zeros((x.get_shape()[1]),dtype=tf.int32)
                                else:
                                    index = tf.zeros((x.get_shape()[0]),dtype=tf.int32)
                            if ptr is None:
                                ptr = 0
                            x, _ = to_dense_batch(x, index, ptr, dim_size, abs(dim))
                            x = tf.cast(x, dtype=tf.float32)
                            res = self.lstm(x)[0][:, -1][0]
                            res = tf.reshape(res,[1,self.out_channels])
                            
                            return res

                        def __repr__(self) -> str:
                            return (f'{self.__class__.__name__}({self.in_channels}, '
                                    f'{self.out_channels})')


                  
                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div><p>Performs LSTM-style aggregation in which the elements to aggregate are interpreted as a sequence, as described in the<a href="https://arxiv.org/abs/1706.02216" target="_blank">Inductive Representation Learning on Large Graphs</a></p></div>
        <dl class="field-list simple">
          <dt class="field-odd">
            <h4><strong>Parameters</strong></h4>
          </dt>
          <dd class="field-odd">
            <ul class="simple_parameter">
              <li>
                <p><strong>in_channels </strong>
                 – Size of each input sample.
              </p>
              </li>
              <li>
                <p><strong>out_channels  </strong>
                 – Size of each output sample.
                 
              </p>
              </li> 
            </ul>
          </dd>
        </dl>
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="sortaggr">SortAggregation</span>( k: int) </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#sort">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="sort" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR SORT_AGGREGATION</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>
                    from typing import Optional

                    import tensorflow as tf

                    from base import Aggregation

                    import numpy as np

                    class SortAggregation(Aggregation):
                        def __init__(self, k: int):
                            super().__init__()
                            self.k = k
                        def __call__(self, x: tf.Tensor, index: Optional[tf.Tensor] = None,
                                    ptr: Optional[tf.Tensor] = None, dim_size: Optional[int] = None,
                                    dim: int = -2) -> tf.Tensor:
                            if index is None and ptr is None:     
                                index = tf.cast(tf.zeros(x.get_shape()[dim]), dtype=x.dtype)
                            fill_value = int(tf.reduce_min(x))- 1
                          
                            batch_x, _ = self.to_dense_batch(x, index, ptr, dim_size, dim,
                                                            fill_value=fill_value)
                          
                            B, N, D = batch_x.get_shape()
                            
                            _  = tf.sort(batch_x[:, :, -1], axis=-1,direction='DESCENDING')
                            perm = tf.argsort(batch_x[:, :, -1], axis=-1,direction='DESCENDING')

                            arange = tf.range(B, dtype=tf.int32) * N
                            perm = perm + tf.reshape(arange,[-1, 1])
                            batch_x = tf.reshape(batch_x,[B * N, D])
                            batch_x = batch_x.numpy()[perm.numpy()]
                            
                        
                            batch_x = tf.reshape(batch_x,[B, N, D])
                            if N >= self.k:
                                batch_x = batch_x.numpy()
                                batch_x = np.ascontiguousarray(batch_x[:, :self.k])
                                batch_x = tf.constant(batch_x)
                            else:
                                expand_batch_x = np.full((B, self.k - N, D), fill_value)
                                batch_x = tf.concat([batch_x, expand_batch_x], axis=1)
                            batch_x = batch_x.numpy()
                            batch_x[batch_x == fill_value] = 0
                            batch_x = tf.constant(batch_x)
                            x = tf.reshape(batch_x,[B, self.k * D])

        return x   
                  
                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div>
          <p>The pooling operator from the <a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://muhanzhang.github.io/papers/AAAI_2018_DGCNN.pdf" target="_blank"> “An End-to-End Deep Learning Architecture for Graph Classification”</a> paper, where The features of nodes are sorted in descending order based on their most recent feature channel.The first <strong>k</strong> nodes form the output of the layer.</p>
          <img class="equationclass" src="assets\images\equation/softaggr.PNG" alt="batch">
        </div>
        <dl class="field-list simple">
          <dt class="field-odd">
            <h4><strong>Parameters</strong></h4>
          </dt>
          <dd class="field-odd">
            <ul class="simple_parameter">
              <li>
                <p><strong>k</strong>
                 – The number of nodes to hold for each graph.
              </p>
              </li>
              
              
            </ul>
          </dd>
        </dl>
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="attaggr">AttentionalAggregation</span>( gate_nn: Module, nn: Optional[Module] = None) </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#attention">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="attention" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR ATTENTIONAL_AGGREGATION</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>  
                    import tensorflow as tf 
                    from base import Aggregation

                    def maybe_num_nodes(index: tf.Tensor, num_nodes:int = None):
                        """
                        tf.argmax(row).numpy() -> int 
                        num_nodes -> int
                        need type conversion 
                        return tf.argmax(row).numpy() + 1 if num_nodes is None else num_nodes 
                        """
                        return tf.reduce_max(index).numpy() + 1 if num_nodes is None else int(num_nodes)

                    def tf_index_select(input_, dim, indices):
                        """
                        input_(tensor): input tensor
                        dim(int): dimension
                        indices(list): selected indices list
                        """
                        shape = input_.get_shape().as_list()
                        if dim == -1:
                            dim = len(shape)-1
                        shape[dim] = 1
                        
                        tmp = []
                        for idx in indices:
                            begin = [0]*len(shape)
                            begin[dim] = idx
                            tmp.append(tf.slice(input_, begin, shape))
                        res = tf.concat(tmp, axis=dim)
                        
                        return res

                    def tf_softmax( src: tf.Tensor,index: tf.Tensor , num_nodes: int = None, ptr: tf.Tensor = None, dim: int = 0) :
                            N = maybe_num_nodes(index, num_nodes)
                            src_max = tf.math.unsorted_segment_max(src, index, N)
                            src_max = tf_index_select(src_max , dim ,index)
                            out = tf.exp(src - src_max)
                            out_sum = tf.math.unsorted_segment_sum(out , index , N)
                            out_sum = tf_index_select(out_sum , dim , index)
                            return (out / (out_sum + 1e-16)).numpy()
                    class tf_AttentionalAggregation(Aggregation):
                        def __init__(self,gate_nn : tf.keras.layers.Layer , 
                                    nn : tf.keras.layers.Layer = None) : 
                            super(tf_AttentionalAggregation , self).__init__()
                            self.gate_nn = gate_nn
                            self.nn = nn
                            
                        def call(self , x:tf.Tensor , index:tf.Tensor = None , ptr : tf.Tensor = None , dim_size:int= None, dim :int = -2):
                            self.assert_two_dimensional_input(x,dim)
                            gate = self.gate_nn(x)
                            x = self.nn(x) if self.nn is not None else x 
                            gate = tf_softmax(x,index)
                            return self.reduce(gate * x, index,reduce='sum')

                        def __repr__(self) :
                            return (f'{self.__class__.__name__}(gate_nn={self.gate_nn}, '
                                    f'nn={self.nn})')

                    # Implementation : 
                    # re = tf_AttentionalAggregation(tf.keras.Sequential())
                    # print(re(tf.constant([[2.,2,300],[5,4,3]]),tf.constant([0,0])))
                    # print(re) # this representation for this class  
                  
                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div>
          <p>The soft attention aggregation layer from the <a href="https://arxiv.org/abs/1904.12787" target="_blank">Graph Matching Networks for Learning the Similarity of Graph Structured Objects</a> paper</p>
          <img class="equationclass" src="assets\images\equation/attaggr.PNG" alt="batch">
        </div>
        <dl class="field-list simple">
          <dt class="field-odd">
            <h4><strong>Parameters</strong></h4>
          </dt>
          <dd class="field-odd">
            <ul class="simple_parameter">
              <li>
                <p><strong>gate_nn </strong>
                 – A neural network h<sub>gate</sub> that computes attention scores by mapping node features <mask>x</mask> of shape <mask>[-1, in_channels]</mask> to shape <mask>[-1, 1]</mask> (for node-level gating) or <mask>[1, out_channels]</mask>.
              </p>
              </li>
              <li>
                <p><strong>nn  </strong>
                 – A neural network h<sub>&Theta;</sub>  that maps node features <mask>x</mask> of shape <mask>[-1, in_channels]</mask> to shape <mask>[-1, out_channels]<mask> before combining them with the attention scores.
                 </p>
              </li>
              
            </ul>
          </dd>
        </dl>
        <div>
          <pre class="classes">
            <code>
              <p>class <span class="className" id="degreeaggr">DegreeScalerAggregation</span>( aggr: Union[str, List[str], Aggregation], scaler: Union[str, List[str]], deg: Tensor, aggr_kwargs: Optional[List[Dict[str, Any]]] = None) </p>
            </code>
            <span class="preSource" data-toggle="modal" data-target="#degree_code">[source]</span>
          </pre>
        </div>
        <div class="modal fade" id="degree_code" tabindex="-1" role="dialog" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR DEGREE_SCALER_AGGREGATION</strong></h5>
                <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                  <span aria-hidden="true">&times;</span>
                </button>
              </div>
              <div class="modal-body">
                <pre>
                  <code>  
                    import tensorflow as tf
                    from typing import Any, Dict, List, Optional, Union
                    from base import Aggregation
                    from tf_degree import degree
                    from Multi import aggregation_resolver as aggr_resolver
                    from Multi import MultiAggregation

                    class DegreeScalerAggregation(Aggregation):
                        def __init__(self,
                                    aggr: Union[str, List[str],Aggregation],
                                    scaler: Union[str, List[str]],
                                    deg: tf.Tensor,
                                    aggr_kwargs: Optional[List[Dict[str, Any]]] = None):
                            
                            super().__init__()

                            if isinstance(aggr, (str, Aggregation)):
                                self.aggr = aggr_resolver(aggr, **(aggr_kwargs or {}))
                            elif isinstance(aggr, (tuple, list)):
                                self.aggr = MultiAggregation(aggr, aggr_kwargs)
                            else:
                                raise ValueError(f"Only strings, list, tuples and instances of"
                                                f"`torch_geometric.nn.aggr.Aggregation` are "
                                                f"valid aggregation schemes (got '{type(aggr)}')")

                            self.scaler = [scaler] if isinstance(aggr, str) else scaler
                            deg=tf.cast(deg,dtype=tf.double)
                            num_nodes = int(tf.reduce_sum(deg))
                            bin_degrees = tf.experimental.numpy.arange(float(tf.size(deg).numpy()))
                            self.avg_deg: Dict[str, float] = {
                                'lin': float(tf.reduce_sum(bin_degrees * deg))/ num_nodes,
                                'log': float(tf.reduce_sum(tf.math.log(bin_degrees + 1) * deg))/ num_nodes,
                    #             'log': float(((bin_degrees + 1).log() * deg).sum()) / num_nodes,
                                # 'exp': float(tf.reduce_sum(tf.math.exp(bin_degrees) * deg)) / num_nodes,
                            }

                        def __call__(self, x: tf.Tensor, index: tf.Tensor , ptr = None , dim_size = None , dim = -2 ) -> tf.Tensor:

                            # TODO Currently, degree can only operate on index:
                            self.assert_index_present(index)

                            out = self.aggr(x, index)
                            
                            
                            assert index is not None
                            deg = tf.clip_by_value(degree(index, num_nodes=dim_size, dtype=out.dtype),0,1)
                            size = [1] * (tf.size(out).numpy())
                            size[dim] = -1
                            deg = tf.reshape(deg,size)

                            outs = []
                            for scaler in self.scaler:
                                if scaler == 'identity':
                                    out_scaler = out
                                elif scaler == 'amplification':
                                    out_scaler = out * (tf.math.log(deg + 1) / self.avg_deg['log'])
                                elif scaler == 'attenuation':
                                    out_scaler = out * (self.avg_deg['log'] / tf.math.log(deg + 1))
                                elif scaler == 'linear':
                                    out_scaler = out * (deg / self.avg_deg['lin'])
                                elif scaler == 'inverse_linear':
                                    out_scaler = out * (self.avg_deg['lin'] / deg)
                                else:
                                    raise ValueError(f"Unknown scaler '{scaler}'")
                                outs.append(out_scaler)

                                x= tf.concat(outs, axis=-1) if len(outs) > 1 else outs[0]
                            return tf.cast(x ,dtype=tf.float32).numpy()

                    # implementation
                    #x=DegreeScalerAggregation('sum','amplification',tf.constant([[2,6,5]]))
                    #x(tf.constant([[5,2,5.],[1,5,3]]) , tf.constant([0,0]))
                  
                          </code>
                </pre>
              </div>
            </div>
          </div>
        </div>
        <div>
          <p>Combines one or more aggregators and transforms its output with one or more scalers as introduced in the <a href="https://arxiv.org/abs/2004.05718" target="_blank">Principal Neighbourhood Aggregation for Graph Nets</a> paper</p>
        </div>
        <dl class="field-list simple">
          <dt class="field-odd">
            <h4><strong>Parameters</strong></h4>
          </dt>
          <dd class="field-odd">
            <ul class="simple_parameter">
              <li>
                <p><strong>aggr  </strong>
                 – The aggregation scheme to use. See <mark>MessagePassing</mark> for more information.
              </p>
              </li>
              <li>
                <p><strong>scaler   </strong>
                 – Set of scaling function identifiers, namely one or more of <mark>"identity"</mark>, <mark>"amplification"</mark>, <mark>"attenuation"</mark>, <mark>"linear"</mark> and <mark>"inverse_linear"</mark>
                 </p>
              </li>
              <li>
                <p><strong>deg  </strong>
                 – Histogram of in-degrees of nodes in the training set, used by scalers to normalize.
                 </p>
              </li>
             
              
            </ul>
          </dd>
        </dl>
        
          </section>

          <hr class="divider">

          <!-- Normalization Layers
		============================ -->
          <section id="idocs_norm">
            <h2>Normalization Layers</h2>
            <p class="lead">
            <ul class="simple">
              <li>
                <p>
                  <a class="reference internal" href="#classBatch">Batch Normalization</a>
                </p>
              </li>
              <li>
                <p>
                  <a class="reference internal" href="#instance_norm">Instance Normalization</a>
                </p>
              </li>
              <li>
                <p>
                  <a class="reference internal" href="#layer_norm">Layer Normalization</a>
                </p>
              </li>
              <li>
                <p>
                  <a class="reference internal" href="#graph_norm">Graph Normalization</a>
                </p>
              </li>
              <li>
                <p>
                  <a class="reference internal" href="#graph_size_norm">Graph Size Normalization</a>
                </p>
              </li>
              <li>
                <p>
                  <a class="reference internal" href="#pair_norm">Pair Normalization</a>
                </p>
              </li>
              <li>
                <p>
                  <a class="reference internal" href="#mean_sub_norm">Mean Subtraction Normalization</a>
                </p>
              </li>
              <li>
                <p>
                  <a class="reference internal" href="#message_norm">Message Normalization</a>
                </p>
              </li>
              </p>
              <div>
                <pre class="classes">
                  <code>
                    <p>class <span class="className" id="classBatch">Batch Norm</span> ( in_channels: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = True, track_running_stats: bool = True, allow_single_element: bool = False)</p>
                  </code>
                  <span class="preSource" data-toggle="modal" data-target="#sourceData">[source]</span>
                </pre>
              </div>
              <div class="modal fade" id="sourceData" tabindex="-1" role="dialog" aria-hidden="true">
                <div class="modal-dialog modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR BATCH_NORM</strong></h5>
                      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                      </button>
                    </div>
                    <div class="modal-body">
                      <pre>
                        <code>
                            import tensorflow as tf
                            class BatchNorm(tf.keras.layers.Layer):
                              def _init_(self, eps: float = 1e-5,
                                momentum: float = 0.1, affine: bool = True,
                                track_running_stats: bool = True):
                                  super()._init_()
                              def call(self , x : tf.Tensor):
                                module =  tf.keras.layers.LayerNormalization(axis = 0 )
                                output = module(x).numpy()
                                return output
                                </code>
                      </pre>
                    </div>
                  </div>
                </div>
              </div>
              <div>
                <p>Applies batch normalization over a batch of node features as described in the <a href="https://arxiv.org/abs/1502.03167" target="_blank">"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> paper</p>
                <img class="equationclass" src="assets\images\equation/batch_equation.PNG" alt="batch">
                <p>The mean and standard-deviation are calculated per-dimension over all nodes inside the mini-batch.</p>
              </div>
              <dl class="field-list simple">
                <dt class="field-odd">
                  <h4><strong>Parameters</strong></h4>
                </dt>
                <dd class="field-odd">
                  <ul class="simple_parameter">
                    <li>
                      <p><strong>in_channels</strong>
                       – Size of each input sample.
                    </p>
                    </li>
                    <li>
                      <p><strong>eps</strong>
                       – A value added to the denominator for numerical stability. 
                    </p>
                    </li>
                    <li>
                      <p><strong>momentum</strong>
                       – The value used for the running mean and running variance computation. 
                    </p>
                    </li>
                    <li>
                      <p><strong>affine </strong>
                       – If set to True, this module has learnable affine parameters &gamma; and &beta;. 
                    </p>
                    </li>
                    <li>
                      <p><strong>track_running_stats </strong>
                       – If set to True, this module tracks the running mean and variance, and when set to False, this module
                        does not track such statistics and always uses batch statistics in both training and eval modes.
                    </p>
                    </li>
                    <li>
                      <p><strong>allow_single_element</strong>
                        If set to True, batches with only a single element will work as during in evaluation. That is the running
                        mean and variance will be used. Requires track_running_stats=True.  
                    </p>
                    </li>
                  </ul>
                </dd>
              </dl>
              <div>
                <pre class="classes">
                  <code>
                    <p>class <span class="className" id ="instance_norm">InstanceNorm</span> ( in_channels: int, eps: float = 1e-05, momentum: float = 0.1, affine: bool = False, track_running_stats: bool = False)</p>
                  </code>
                  <span class="preSource" data-toggle="modal" data-target="#pair_source">[source]</span>
                </pre>
              </div>
              <div class="modal fade" id="pair_source" tabindex="-1" role="dialog" aria-hidden="true">
                <div class="modal-dialog modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR NORM.INSTANCE_NORM</strong></h5>
                      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                      </button>
                    </div>
                    <div class="modal-body">
                      <pre>
                        <code>
                          import tensorflow as tf 
                          import numpy as np 
                          
                          
                          class InstanceNorm(tf.keras.layers.Layer):
                            def __init__(
                                self , 
                                in_channels : int,
                                epsilon : float = 1e-5 ,
                                momentum : float = 0.1 ,
                                affine : bool = False , 
                                track_running_stats : bool = False ,
                                **kwargs
                                ):
                              super(InstanceNorm , self).__init__(**kwargs)
                              self.in_channels = in_channels 
                              self.epsilon = epsilon 
                              self.momentum = momentum
                              self.affine = affine
                              self.track_running_stats = track_running_stats 
                          
                            def call(self , x:tf.Tensor , batch = None) -> tf.Tensor:
                               if batch is None :
                                  if(tf.shape(x).shape[0] == 2):
                                      out = tf.keras.layers.LayerNormalization(axis=0 , epsilon= self.epsilon)
                                      output = out(x)
                                      return output.numpy()
                                  else:
                                      raise Exception("tensor should be '= 2' dimensions")
                          
                            def __repr__(self):
                              return f"{self.__class__.__name__}({self.in_channels})"
                          
                          
                          
                          
                          
                          #gr = InstanceNorm(64 , 0.001)
                          # print(gr(tf.constant([[1. ,2. ,2.5]])))
                          #v = tf.constant([[6.6 , 9.7] , [1.1 , 8.4] , [1.2 , 5.9] , [3.2 , 4.4]])
                          #print(gr(v))
                          # print(gr(tf.constant([[[6.6 , 9.7] , [1.1 , 8.4] , [1.2 , 5.9] , [3.2 , 4.4]]])))
                          # print(tf.shape(v).shape[0])
                                </code>
                      </pre>
                    </div>
                  </div>
                </div>
              </div>
              <div>
                <p>Applies instance normalization over each individual example in a batch of node features as described in the <a href="https://arxiv.org/abs/1607.08022" target="_blank">"Instance Normalization: The Missing Ingredient for Fast Stylization"</a> paper.</p>
                <img class="equationclass" src="assets\images\equation/batch_equation.PNG" alt="batch">
              </div>
              <dl class="field-list simple">
                <dt class="field-odd">
                  <h4><strong>Parameters</strong></h4>
                </dt>
                <dd class="field-odd">
                  <ul class="simple_parameter">
                    <li>
                      <p><strong>in_channels</strong>
                       – Size of each input sample.
                    </p>
                    </li>
                    <li>
                      <p><strong>eps</strong>
                       – A value added to the denominator for numerical stability. 
                    </p>
                    </li>
                    <li>
                      <p><strong>momentum</strong>
                       – The value used for the running mean and running variance computation. 
                    </p>
                    </li>
                    <li>
                      <p><strong>affine </strong>
                       – If set to True, this module has learnable affine parameters &gamma; and &beta;. 
                    </p>
                    </li>
                    <li>
                      <p><strong>track_running_stats </strong>
                       – If set to True, this module tracks the running mean and variance, and when set to False, this module
                       does not track such statistics and always uses batch statistics in both training and eval modes.
                    </p>
                    </li>
                  </ul>
                </dd>
              </dl>
              <div>
                <pre class="classes">
                  <code>
                    <p>class <span class="className" id="layer_norm">LayerNorm</span> ( in_channels: int, eps: float = 1e-05, affine: bool = True, mode: str = 'graph')</p>
                  </code>
                  <span class="preSource" data-toggle="modal" data-target="#layer_source">[source]</span>
                </pre>
              </div>
              <div class="modal fade" id="layer_source" tabindex="-1" role="dialog" aria-hidden="true">
                <div class="modal-dialog modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR LAYER_NORM                      </strong></h5>
                      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                      </button>
                    </div>
                    <div class="modal-body">
                      <pre>
                        <code>
                          import tensorflow as tf
                          from tf_degree import degree
                          from Scatter import scatter
                        
                          
                          def tf_index_select(input_, dim, indices):
                              """
                              input_(tensor): input tensor
                              dim(int): dimension
                              indices(list): selected indices list
                              """
                              shape = input_.get_shape().as_list()
                              if dim == -1:
                                  dim = len(shape)-1
                              shape[dim] = 1
                              
                              tmp = []
                              for idx in indices:
                                  begin = [0]*len(shape)
                                  begin[dim] = idx
                                  tmp.append(tf.slice(input_, begin, shape))
                              res = tf.concat(tmp, axis=dim)
                              
                              return res
                         
                          
                          class LayerNorm(tf.keras.layers.Layer):
                              def __init__(self, in_channels:int, eps:float= 1e-5, affine: bool = True, mode: str = 'graph'):
                                  super().__init__()
                                  self.in_channels = in_channels
                                  self.eps = eps
                                  self.mode = mode
                                  if affine:
                                      self.weight = tf.ones(in_channels, dtype=tf.float32)
                                      self.bias = tf.zeros(in_channels , dtype=tf.float32)
                              def call(self, x:tf.Tensor, batch = None) ->tf.Tensor:
                                  x = tf.cast(x,dtype=tf.float32)
                                  if self.mode == 'graph':
                                      if batch is None:
                                          x = x - tf.math.reduce_mean(x)
                                          out = x / (tf.math.reduce_std(x,  keepdims=False) + self.eps)
                                      else:
                                          batch_size = int(tf.reduce_max(batch)) + 1
                                          norm = tf.clip_by_value(degree (batch, batch_size, dtype=x.dtype),clip_value_min=1,clip_value_max=100000)
                                          
                                          norm = tf.reshape(tf.math.multiply(norm,x.get_shape()[-1]),[-1, 1])
                              
                                          mean = tf.math.reduce_sum(scatter(x, batch, dim=0, dim_size=batch_size,
                                                         reduce='add'),axis=-1, keepdims=True) / norm
                                          x = tf.cast(x,dtype=tf.float32)
                                          x = x - tf_index_select(mean,0, batch)
                                          src = x * x 
                                          var = tf.math.reduce_sum( scatter(x * x, batch, dim=0, dim_size=batch_size,
                                                        reduce='add'),axis=-1, keepdims=True)
                                          
                                          var = var / norm
                                          
                          
                                          out = x / tf_index_select(tf.math.sqrt(var + self.eps),0, batch)
                          
                                      if self.weight is not None and self.bias is not None:
                                          
                                          out = out * self.weight +self.bias
                          
                                      return out.numpy()
                                  if self.mode == 'node':
                                      return tf.keras.layers.Normalization(x, (self.in_channels, ), self.weight,
                                                          self.bias, self.eps)
                          
                                  raise ValueError(f"Unknow normalization mode: {self.mode}")
                              def __repr__(self):
                                  return (f'{self.__class__.__name__}({self.in_channels}, '
                                          f'mode={self.mode})')
                          
                                </code>
                      </pre>
                    </div>
                  </div>
                </div>
              </div>
              <div>
                <p>Applies layer normalization over each individual example in a batch of node features as described in the <a href="https://arxiv.org/abs/1607.06450" target="_blank">"Layer Normalization"</a> paper</p>
                <img class="equationclass" src="assets\images\equation/batch_equation.PNG" alt="batch">
              </div>
              <dl class="field-list simple">
                <dt class="field-odd">
                  <h4><strong>Parameters</strong></h4>
                </dt>
                <dd class="field-odd">
                  <ul class="simple_parameter">
                    <li>
                      <p><strong>in_channels</strong>
                       – Size of each input sample.
                    </p>
                    </li>
                    <li>
                      <p><strong>eps</strong>
                       – A value added to the denominator for numerical stability. 
                    </p>
                    </li>
                    <li>
                      <p><strong>affine </strong>
                       – If set to True, this module has learnable affine parameters &gamma; and &beta;. 
                    </p>
                    </li>
                    <li>
                      <p><strong>mode</strong>
                       – The normalization mode to use for layer normalization. ("graph" or "node"). If "graph" is used, each graph will be 
                        considered as an element to be normalized. If “node” is used, each node will be considered as an element to be normalized.
                       (default: "graph")</p>
                    </li>
                  </ul>
                </dd>
              </dl>
              <div>
                <pre class="classes">
                  <code>
                    <p>class <span class="className" id="graph_norm">GraphNorm </span> ( in_channels: int, eps: float = 1e-05)</p>
                  </code>
                  <span class="preSource" data-toggle="modal" data-target="#graph_norm_source">[source]</span>
                </pre>
              </div>
              <div class="modal fade" id="graph_norm_source" tabindex="-1" role="dialog" aria-hidden="true">
                <div class="modal-dialog modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR GRAPH_NORM</strong></h5>
                      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                      </button>
                    </div>
                    <div class="modal-body">
                      <pre>
                        <code>
                          import tensorflow as tf 
                          import numpy as np 

                          class GraphNorm(tf.keras.layers.Layer):
                              def __init__(self, in_channel: int , eps : float = 0.001) -> None:
                                  super().__init__()
                                  self.in_channel = in_channel
                                  self.eps = eps 
                                  self.weight = tf.ones(in_channel, dtype=tf.float32)
                                  self.bias = tf.zeros(in_channel , dtype=tf.float32)
                                  self.mean_scale = tf.ones(in_channel , dtype=tf.float32)
                              
                              def call(self , x:tf.Tensor , batch = None):

                                  if batch is None : 
                                      batch = tf.zeros(tf.size(x).numpy() , dtype=tf.float32)
                                  
                                  mean = tf.reduce_mean(x).numpy()
                                  out = (mean - x ) * -1
                                  var = np.array([tf.reduce_mean(tf.math.pow(out,2)).numpy()])
                                  std = tf.pow((var + self.eps),0.5) + tf.zeros(self.in_channel).numpy()
                                  return self.weight * out / std + self.bias


                          #v = tf.constant([4.,1.,3.])
                          #gr = GraphNorm(3)
                          #print(gr(v))
                                </code>
                      </pre>
                    </div>
                  </div>
                </div>
              </div>
              <div>
                <p>Applies graph normalization over individual graphs as described in the <a href="https://arxiv.org/abs/2009.03294" target="_blank">"GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training"</a> paper</p>
                <img class="equationclass" src="assets\images\equation/graphnormequatyion.PNG" alt="batch">
              </div>
              
              <dl class="field-list simple">
                <dt class="field-odd">
                  <h4><strong>Parameters</strong></h4>
                </dt>
                <dd class="field-odd">
                  <ul class="simple_parameter">
                    <li>
                      <p><strong>in_channels  </strong>
                       – Size of each input sample.
                    </p>
                    </li>
                    <li>
                      <p><strong>eps  </strong>
                       –  A value added to the denominator for numerical stability.
                    </p>
                    </li>
                  </ul>
                </dd>
              </dl>
              
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="graph_size_norm">GraphSizeNorm </span> </p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#graph_size_norm_souce">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="graph_size_norm_souce" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR GRAPH_SIZE_NORM</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>
                            import tensorflow as tf
                            import numpy as np 
                            from tf_degree import degree
                            
                            class GraphSizeNorm(tf.keras.layers.Layer):
                              def __init__(self):
                                super().__init__()
                            
                              def call(self , x: tf.Tensor, batch = None):
                                if batch is None:
                                  batch = tf.zeros(tf.size(x).numpy(),dtype=tf.int32)
                                if tf.shape(x).shape[0] > 1 : 
                                    return x.numpy()
                                else:
                                  inv_sqrt_deg = tf.pow(degree(batch),-0.5)
                                  new_x = tf.cast(x,dtype=tf.float32)
                                  z = (new_x * tf.reshape(inv_sqrt_deg ,(-1,1))).numpy()
                                  c = tf.size(x)
                                  return (np.repeat(z,c).reshape(c,c)).T
                            
                            
                            
                            
                            # for testing : 
                            # gr =GraphSizeNorm()
                            # print(gr(tf.constant([3,5,4,1])))
                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Applies Graph Size Normalization over each individual graph in a batch of node features as described in the <a href="https://arxiv.org/abs/2003.00982" target="_blank">"Benchmarking Graph Neural Networks"</a> paper</p>
                  <img class="equationclass" src="assets\images\equation/graph_size.PNG" alt="batch">
                  <p>where &alpha; denotes parameters that learn how much information to keep in the mean.</p>
                </div>
              <div>
                <pre class="classes">
                  <code>
                    <p>class <span class="className" id="pair_norm">PairNorm </span> ( scale: float = 1.0, scale_individually: bool = False, eps: float = 1e-05)</p>
                  </code>
                  <span class="preSource" data-toggle="modal" data-target="#pair_norm_source">[source]</span>
                </pre>
              </div>
              <div class="modal fade" id="pair_norm_source" tabindex="-1" role="dialog" aria-hidden="true">
                <div class="modal-dialog modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR PAIR_NORM</strong></h5>
                      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                      </button>
                    </div>
                    <div class="modal-body">
                      <pre>
                        <code>
                          import tensorflow as tf
                          from tf_scatter import scatter_mean
                          
          
                          class PairNorm(tf.keras.layers.Layer):
                             
                              def __init__(self, scale: float = 1., scale_individually: bool = False,
                                           eps: float = 1e-5):
                                  super().__init__()
                          
                                  self.scale = scale
                                  self.scale_individually = scale_individually
                                  self.eps = eps
                          
                          
                              def call(self, x: tf.Tensor, batch= None) -> tf.Tensor:
                                  
                                  scale = self.scale
                          
                                  if batch is None:
                                       
                                      x = x - tf.math.reduce_mean(x ,axis = 0, keepdims= True)
                          
                          
                                      if not self.scale_individually:
                                          
                                          return scale * x / tf.math.sqrt(self.eps + tf.math.reduce_mean(tf.math.reduce_sum(tf.math.pow (x,2),-1))) 
                          
                                      else:
                                           
                                          return scale * x / (self.eps + tf.norm(x, ord=2, axis=-1, keepdims=True))
                          
                                  else:
                                      
                                      mean = scatter_mean(x , batch,n_nodes=0)
                                      
                                      x = x - tf.gather(mean, indices= batch, axis=0).numpy()
                          
                          
                                      if not self.scale_individually:
                                          
                                          return scale * x /  tf.math.sqrt(self.eps + scatter_mean(x.tf.math.pow(2).reduce_sum(-1, keepdims=True), batch, n_nodes=0).tf.gather(indices= batch, axis=0))  
                          
                          
                                      else:
                                          return scale * x / (self.eps + tf.norm(x, ord=2, axis=-1, keepdims=True))
                          
                          
                              def __repr__(self):
                                  return f'{self.__class__.__name__}()'
                                </code>
                      </pre>
                    </div>
                  </div>
                </div>
              </div>
              <div>
                <p>Applies pair normalization over node features as described in the <a href="https://arxiv.org/abs/1909.12223" target="_blank">"
                  PairNorm: Tackling Oversmoothing in GNNs"</a> paper</p>
                <img class="equationclass" src="assets\images\equation/pairnorm.PNG" alt="batch">
                
              </div>
              <div>
                <img class="equationclass" src="assets\images\equation/pair_2.PNG" alt="batch">
              </div>
              
              <dl class="field-list simple">
                <dt class="field-odd">
                  <h4><strong>Parameters</strong></h4>
                </dt>
                <dd class="field-odd">
                  <ul class="simple_parameter">
                    <li>
                      <p><strong>scale </strong>
                       – Scaling factor &delta; of normalization. 
                    </p>
                    </li>
                    <li>
                      <p><strong>scale_individually </strong>
                       – If set to True, will compute the scaling step as
                    </p>
                    </li>
                    <li>
                      <p><strong>eps</strong>
                       – A value added to the denominator for numerical stability. 
                    </p>
                    </li>
                  </ul>
                </dd>
              </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="mean_sub_norm">MeanSubtractionNorm </span> </p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#mean_sub_norm_source">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="mean_sub_norm_source" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR MEAN_SUB_NORM</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>
                              
                              import tensorflow as tf
                              from Scatter import scatter

                              class MeanSubtractionNorm(tf.keras.layers.Layer):
                                  def reset_parameters(self):
                                      pass

                                  def call(self, x:tf.Tensor, batch= None,dim_size= None) -> tf.Tensor:
                                      if batch is None:
                                          return x - tf.math.reduce_mean(x,axis=0, keepdims=True)

                                      mean = scatter(x, batch, dim=0, dim_size=dim_size, reduce='mean')
                                      return x - mean.numpy()[batch.numpy()]


                                  def __repr__(self) -> str:
                                      return f'{self.__class__.__name__}()'

                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Applies layer normalization by subtracting the mean from the inputs as described in the <a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/2003.13663.pdf" target="_blank">“Revisiting ‘Over-smoothing’ in Deep GCNs” </a> paper</p>
                  <img class="equationclass" src="assets\images\equation/mean_sub.PNG" alt="batch">
                </div>
    
              <div>
                <pre class="classes">
                  <code>
                    <p>class <span class="className" id="message_norm">MessageNorm </span> ( learn_scale: bool = False)</p>
                  </code>
                  <span class="preSource" data-toggle="modal" data-target="#message_norm_source">[source]</span>
                </pre>
              </div>
              <div class="modal fade" id="message_norm_source" tabindex="-1" role="dialog" aria-hidden="true">
                <div class="modal-dialog modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR MESSAGE_NORM</strong></h5>
                      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                      </button>
                    </div>
                    <div class="modal-body">
                      <pre>
                        <code>
                            import tensorflow as tf


                            class MessageNorm(tf.keras.layers.Layer):
                            
                                def __init__(self, learn_scale: bool = False):
                                    super().__init__()
                                

                                    self.scale = tf.Variable(tf.constant([1.0]), trainable=learn_scale)

                                def reset_parameters(self):
                                    self.scale = tf.fill((1,) , 1.0 , name ="r")
                                    return self.scale
                                    




                                def call(self, x: tf.Tensor, msg: tf.Tensor, p: int = 2) -> tf.Tensor:
                                  
                                    
                                    msg = tf.keras.utils.normalize(msg, axis=-1, order=p)

                                    x_norm = tf.norm(x, ord=p, axis=-1, keepdims=True)
                                    return msg * x_norm * self.scale


                                def __repr__(self) -> str:
                                    return (f'{self.__class__.__name__}'
                                            f'(learn_scale={self.scale.requires_grad})')


                                </code>
                      </pre>
                    </div>
                  </div>
                </div>
              </div>
              <div>
                <p>Applies message normalization over the aggregated messages as described in the <a href="https://arxiv.org/abs/2006.07739" target="_blank">"DeeperGCN: All You Need to Train Deeper GCNs"</a> paper</p>
                <img class="equationclass" src="assets\images\equation/msgnorm.PNG" alt="batch">
              </div>
              <dl class="field-list simple">
                <dt class="field-odd">
                  <h4><strong>Parameters</strong></h4>
                </dt>
                <dd class="field-odd">
                  <ul class="simple_parameter">
                    <li>
                      <p><strong>learn_scale  </strong>
                       – If set to True, will learn the scaling factor  of message normalization. (default: False) 
                    </p>
                    </li>
                  </ul>
                </dd>
              </dl>
          </section>

          <!-- Pooling Layer
		============================ -->
          <section id="idocs_pool">
            <h2>Pooling Layers</h2>
            <p class="lead">
              <ul class="simple">
                <li>
                  <p>
                    <a class="reference internal" href="#globaladdpool">Global Add Pool</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#globalmeanpool">Global Mean Pool</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#globalmaxpool">Global Max Pool</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#topk">TopK Pooling</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#edgepool">Edge Pooling</a>
                  </p>
                </li>
                <li>
                  <p>
                    <a class="reference internal" href="#mempool">Mem Pooling</a>
                  </p>
                </li>
                </p>
                <li>
                  <p>
                    <a class="reference internal" href="#maxpool">Max Pool</a>
                  </p>
                </li>
                </p>
                <li>
                  <p>
                    <a class="reference internal" href="#avgpool">Avg Pooling</a>
                  </p>
                </li>
                </p>
                <li>
                  <p>
                    <a class="reference internal" href="#maxpoolx">Max Pool X</a>
                  </p>
                </li>
                </p>
                <li>
                  <p>
                    <a class="reference internal" href="#maxpoolnx">Max Pool Neighbor X</a>
                  </p>
                </li>
                </p>
                <li>
                  <p>
                    <a class="reference internal" href="#avgpoolx">Avg Pool X</a>
                  </p>
                </li>
                </p>
                <li>
                  <p>
                    <a class="reference internal" href="#avgpoolnx">Avg Pool Neighbor X</a>
                  </p>
                </li>
                </p>
                <li>
                  <p>
                    <a class="reference internal" href="#graclus">Graclus</a>
                  </p>
                </li>
                </p>
                <li>
                  <p>
                    <a class="reference internal" href="#fps">fps</a>
                  </p>
                </li>
                </p>
                <li>
                  <p>
                    <a class="reference internal" href="#knn">knn</a>
                  </p>
                </li>
                </p>
                <li>
                  <p>
                    <a class="reference internal" href="#knng">knn graph</a>
                  </p>
                </li>
                </p>
                <li>
                  <p>
                    <a class="reference internal" href="#radius">radius</a>
                  </p>
                </li>
                </p>
                <li>
                  <p>
                    <a class="reference internal" href="#radiusg">radius graph</a>
                  </p>
                </li>
                </p>
                <li>
                  <p>
                    <a class="reference internal" href="#near">nearest</a>
                  </p>
                </li>
                </p>
                <div>
                  <pre class="classes">
                    <code>
                      <p>def <span class="className" id="globaladdpool">tf_global_add_pool</span> ( x : tf.Tensor , batch : tf.Tensor = None , size:int = None) -> tf.Tensor</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#global_add">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="global_add" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR GLOBAL_ADD_POOL</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>
                            import tensorflow as tf
                            from scatter_global import scatter_add
                            
                            def tf_global_add_pool(x : tf.Tensor , batch : tf.Tensor = None , size:int = None) -> tf.Tensor:
                                if batch is None:
                                    return [tf.reduce_sum(x).numpy()]
                                size = int(tf.reduce_max(batch.numpy())+1) if size is None else size
                                out = tf.zeros((size ,),dtype= tf.int32)
                                sum = tf.reduce_sum(x).numpy()
                                return scatter_add(size , sum)
                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Returns graph-level outputs in batches by adding node features across node dimensions.so that for a single graph G<sub>i</sub> its output is computed by</p>
                  <img class="equationclass" src="assets\images\equation/gloabal_add.PNG" alt="batch">
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>X</strong>
                         – Node feature matrix &#88; 	&#8712; 	&#82;<sup>(N<sub>i</sub>+...+N<sub>B</sub>) X F</sup>
                      </p>
                      </li>
                      <li>
                        <p><strong>batch </strong>
                         – Batch vector b &#8712; {0,....,B-1}<sup>N</sup> , which assigns each node to a specific example.
                      </p>
                      </li>
                      <li>
                        <p><strong>size </strong>
                         – Batch-size B
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>def <span class="className" id="globalmeanpool">tf_mean_add_pool</span> ( x : tf.Tensor , batch : tf.Tensor = None , size:int = None) -> tf.Tensor</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#global_mean">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="global_mean" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR GLOBAL_MEAN_POOL</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            from scatter_global import scatter_mean

                            def tf_global_mean_pool(x : tf.Tensor , batch : tf.Tensor = None , size:int = None) -> tf.Tensor:

                                if batch is None:
                                    return [tf.reduce_mean (x).numpy()]
                                size = int(tf.reduce_max(batch.numpy())+1) if size is None else size
                                out = tf.zeros((size ,),dtype= tf.int32)
                                mean= tf.reduce_sum(x).numpy()
                                return scatter_mean(size , mean)
                                  </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Returns graph-level outputs in batches by averaging node features across node dimensions.so that for a single graph G<sub>i</sub> its output is computed by

                  </p>
                  <img class="equationclass" src="assets\images\equation/global_mean.PNG" alt="batch">
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>X</strong>
                         – Node feature matrix &#88; 	&#8712; 	&#82;<sup>(N<sub>i</sub>+...+N<sub>B</sub>) X F</sup>
                      </p>
                      </li>
                      <li>
                        <p><strong>batch </strong>
                         – Batch vector b &#8712; {0,....,B-1}<sup>N</sup> , which assigns each node to a specific example.
                      </p>
                      </li>
                      <li>
                        <p><strong>size </strong>
                         – Batch-size B
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>def <span class="className" id="globalmaxpool">tf_global_max_pool</span> ( x : tf.Tensor , batch : tf.Tensor = None , size:int = None) -> tf.Tensor</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#global_max">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="global_max" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR GLOBAL_MAX_POOL</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            from scatter_global import scatter_add,scatter_max

                            def tf_global_max_pool(x : tf.Tensor , batch : tf.Tensor = None , size:int = None) -> tf.Tensor:
                                if batch is None:
                                    return [tf.reduce_max (x).numpy()]
                                size = int(tf.reduce_max(batch.numpy())) if size is None else size
                                s=tf.size(x)
                                out = tf.zeros((size ,),dtype= tf.int32)
                                mean= tf.reduce_max(x).numpy()
                                return scatter_max(s, mean,size)
                           </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Returns batch-wise graph-level-outputs by taking the channel-wise maximum across the node dimension, so that for a single graph G<sub>i</sub>  its output is computed by</p>
                  <img class="equationclass" src="assets\images\equation/global_max.PNG" alt="batch">
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>X</strong>
                         – Node feature matrix &#88; 	&#8712; 	&#82;<sup>(N<sub>i</sub>+...+N<sub>B</sub>) X F</sup>
                      </p>
                      </li>
                      <li>
                        <p><strong>batch </strong>
                         – Batch vector b &#8712; {0,....,B-1}<sup>N</sup> , which assigns each node to a specific example.
                      </p>
                      </li>
                      <li>
                        <p><strong>size </strong>
                         – Batch-size B
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="topk">TopKPooling</span> ( in_channels: int, ratio: ~typing.Union[int, float] = 0.5, min_score: ~typing.Optional[float] = None, multiplier: float = 1.0, nonlinearity: ~typing.Callable)</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#tf_tppk">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="tf_tppk" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR TOP_K_POOLING</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            import numpy as np
                            from tf_softmax import softmax
                            from tf_topk import tf_topk
                            from filter_adj_tensorflow import filter_adj

                            class TopKPooling(tf.keras.layers.Layer):
                                def __init__(self, in_channels: int, ratio:float= 0.5,min_score:float= None, multiplier: float = 1., nonlinearity= tf.math.tanh):
                                    super().__init__()

                                    self.in_channels = in_channels
                                    self.ratio = ratio
                                    self.min_score = min_score
                                    self.multiplier = multiplier
                                    self.nonlinearity = nonlinearity
                                    size = self.in_channels
                                    self.weight =tf.Variable(tf.ones([1,in_channels],dtype=tf.float32))

                                def call(self, x, edge_index, edge_attr=None, batch=None, attn=None):
                                    if batch is None:
                                        batch = tf.zeros(x.get_shape()[0],edge_index.dtype)
                                    attn = x if attn is None else attn
                                    attn = tf.expand_dims(attn,axis=-1) if attn.numpy().ndim == 1 else attn
                                    attn = tf.cast(attn,dtype=tf.float64)
                                    self.weight=tf.cast(self.weight,dtype=tf.float64)
                                    score = tf.reduce_sum((attn * self.weight),axis=-1)
                                    if self.min_score is None:
                                        score = self.nonlinearity(score / tf.norm(self.weight, axis=-1))
                                    else:
                                        score =softmax(score, batch)

                                    perm = tf_topk(score, self.ratio, batch, self.min_score)
                                    if isinstance (perm,tf.Tensor):
                                        perm = perm.numpy()
                                  
                                
                                    x = x.numpy()[perm] * tf.reshape(score.numpy()[perm],[-1, 1])
                                    
                                    x = self.multiplier * x if self.multiplier != 1 else x
                                    batch = batch.numpy()[perm]
                                    
                                    edge_index, edge_attr = filter_adj(edge_index, edge_attr, perm,num_nodes=score.get_shape()[0])
                                    return x, edge_index, edge_attr, batch, perm, score.numpy()[perm]

                                def __repr__(self) -> str:
                                    if self.min_score is None:
                                        ratio = f'ratio={self.ratio}'
                                    else:
                                        ratio = f'min_score={self.min_score}'

                                    return (f'{self.__class__.__name__}({self.in_channels}, {ratio}, '
                                            f'multiplier={self.multiplier})')


                            #implementation

                            """
                            in_channels = 16
                            edge_index = tf.constant([[0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],
                                                          [1, 2, 3, 0, 2, 3, 0, 1, 3, 0, 1, 2]])
                            num_nodes = int(tf.reduce_max(edge_index)) + 1
                            x = tf.random.normal((num_nodes, in_channels))
                            pool1 = TopKPooling(in_channels, ratio=0.5)
                            out1 = pool1(x, edge_index)
                            pool2 = TopKPooling(in_channels, ratio=None, min_score=0.1)
                            out2 = pool2(x, edge_index)
                            pool3 = TopKPooling(in_channels, ratio=2)
                            out3 = pool3(x, edge_index)
                            """

                           </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>top<sub>k</sub> pooling operator from the <a href="https://arxiv.org/abs/1905.05178" target="_blank">"Graph U-Nets"</a> , <a href="https://arxiv.org/abs/1905.02850" target="_blank">"Understanding Attention and Generalization in Graph Neural Networks"</a> and <a href="https://arxiv.org/abs/1811.01287" target="_blank">"Towards Sparse Hierarchical Graph Classifiers"</a> papers</p>
                  <p><strong>if <mark>min_score</mark> &alpha; is <mark>None</mark>:</strong></p>
                  <img class="equationclass" src="assets\images\equation/topk_1.PNG" alt="batch">
                </div>
                <div>
                  <p><strong>if <mark>min_score</mark> &alpha; is a value in [0, 1]:</strong></p>
                  <img class="equationclass" src="assets\images\equation/topk_2.PNG" alt="batch">
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_channels </strong>
                         – Size of each input sample. 
                      </p>
                      </li>
                      <li>
                        <p><strong>ratio  </strong>
                         – Graph pooling ratio, which is used to compute k , or the value of  itself, depending on whether 
                         the type of ratio is <mark> float</mark> or <mark>int</mark> value is ignored if min_score is not None.
                      </p>
                      </li>
                      <li>
                        <p><strong>GNN  </strong>
                         – A graph neural network layer for calculating projection scores
                      </p>
                      </li>
                      <li>
                        <p><strong>min_score  </strong>
                         – Minimal node score &alpha; which is used to compute indices of pooled nodes 
                      </p>
                      </li>
                      <li>
                        <p><strong>multiplier   </strong>
                         – Coefficient by which features gets multiplied after pooling. 
                         <p>This can be useful for large graphs and when <mark>min_score</mark> is used.</p> 
                      </p>
                      </li>
                      <li>
                        <p><strong>nonlinearity    </strong>
                         – The nonlinearity to use. 
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="edgepool">EdgePooling</span> ( in_channels: int, edge_score_method: Optional[Callable] = None, dropout: Optional[float] = 0.0, add_to_edge_score: float = 0.5)</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#edge_pool">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="edge_pool" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR EDGE_POOLING</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            from typing import List, Optional, Tuple, Union
                            import tensorflow as tf
                            import numpy as np
                            from collections import namedtuple
                            from typing import List, Optional, Tuple, Union
                            import tensorflow as tf
                            import numpy
                            from Scatter import scatter_add
                            from consecutive import consecutive_cluster
                            from coalesce import tf_coalesce


                            def tf_tolist(tensor):
                                arr = np.array(tensor)
                                list = arr.tolist()
                                return list

                            class EdgePooling(tf.keras.layers.Layer):
                                unpool_description = namedtuple(
                                    "UnpoolDescription",
                                    ["edge_index", "cluster", "batch", "new_edge_score"])


                                def __init__(self, in_channels, edge_score_method=None, dropout=0, add_to_edge_score=0.5):

                                    super().__init__()

                                    self.in_channels = in_channels
                                    if edge_score_method is None:
                                        edge_score_method = self.compute_edge_score_softmax
                                    self.compute_edge_score = edge_score_method
                                    self.add_to_edge_score = add_to_edge_score
                                    self.dropout = dropout

                                    
                                    self.lin = tf.keras.layers.Dense(1, input_shape=(2 * in_channels,), activation=None)

                                    #self.reset_parameters()


                                #def reset_parameters(self):
                                    #self.lin.reset_parameters()


                                @staticmethod
                                def compute_edge_score_softmax(raw_edge_score, edge_index, num_nodes):
                                    
                                    return tf.nn.softmax(raw_edge_score)



                                @staticmethod
                                def compute_edge_score_tanh(raw_edge_score, edge_index, num_nodes):

                                    
                                    return tf.math.tanh(raw_edge_score)


                                @staticmethod
                                def compute_edge_score_sigmoid(raw_edge_score, edge_index, num_nodes):
                                    
                                    return tf.math.sigmoid(raw_edge_score) 


                                def call(self, x, edge_index, batch):
                                  
                                    
                                    e = tf.concat([x.numpy()[edge_index.numpy()[0]], x.numpy()[edge_index.numpy()[1]]],axis = -1 )
                                    
                                    
                                    e = tf.reshape(self.lin(e), -1)
                                  
                                    
                                    e = tf.nn.dropout(e,self.dropout )
                                    
                                    e = self.compute_edge_score(e, edge_index, tf.size(x).numpy())
                                    e = e + self.add_to_edge_score
                                    
                                    x, edge_index, batch, unpool_info = self.__merge_edges__(
                                        x, edge_index, batch, e)

                                    return x, edge_index, batch, unpool_info 


                                def __merge_edges__(self, x, edge_index, batch, edge_score):
                                    nodes_remaining = set(range(x.get_shape()[0]))

                                    cluster = np.empty_like(batch.numpy())
                                    cluster = tf.constant(cluster)
                                    edge_argsort = tf.argsort(edge_score, direction='DESCENDING')


                                    # Iterate through all edges, selecting it if it is not incident to
                                    # another already chosen edge.
                                    i = 0
                                    new_edge_indices = []
                                    edge_index_cpu = edge_index.cpu()

                                    for edge_idx in edge_argsort.numpy().tolist():
                                            source = edge_index_cpu[0, edge_idx].numpy()

                                            if source not in nodes_remaining:
                                                continue

                                            target = edge_index_cpu[1, edge_idx].numpy()

                                            if target not in nodes_remaining:
                                                continue

                                            new_edge_indices.append(edge_idx)
                                            cluster = cluster.numpy()
                                            cluster[source] = i

                                            nodes_remaining.remove(source)

                                            if source != target:
                                                cluster[target] = i
                                                nodes_remaining.remove(target)

                                            i += 1

                                    for node_idx in nodes_remaining:

                                        cluster[node_idx] = i
                                        i += 1
                                    cluster = tf.constant(cluster)
                                    new_x = scatter_add(x, cluster, i)
                                    new_edge_score = edge_score[new_edge_indices]
                                    if len(nodes_remaining) > 0:
                                        remaining_score = tf.cast(tf.ones(
                                            (x.get_shape()[0] - len(edge_index), )), dtype = x.dtype)
                                        new_edge_score = tf.concat([new_edge_score, remaining_score])
                                    new_x = new_x * tf.reshape(new_edge_score,[-1, 1])
                                    N = new_x.get_shape()[0]
                                    new_edge_index = tf_coalesce(tf.constant(cluster.numpy()[edge_index.numpy()]), None, N, N)

                                    new_batch = tf.zeros(new_x.get_shape()[0], dtype=tf.int64)
                                    new_batch = consecutive_cluster(new_batch)
                                  

                                    unpool_info = self.unpool_description(edge_index=edge_index,
                                                                              cluster=cluster, batch=batch,
                                                                              new_edge_score=new_edge_score)

                                    return new_x, new_edge_index, new_batch, unpool_info
                                def unpool(self, x, unpool_info):
                                

                                    
                                    new_x = x / tf.reshape(unpool_info.new_edge_score,[-1,1])
                                    new_x = new_x[unpool_info.cluster]
                                    return new_x, unpool_info.edge_index, unpool_info.batch


                                def __repr__(self) -> str:
                                    return f'{self.__class__.__name__}({self.in_channels})'

                           </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>The edge pooling operator from the <a href="https://arxiv.org/abs/1905.10990" target="_blank">"Edge Contraction Pooling for Graph Neural Networks"</a> paper</p>
                  <p>In short, each edge is assigned a score. Edges are iteratively contracted based on that score unless one of their nodes has previously been part of a contracted edge.</p>
                  <p><strong>if <mark>min_score</mark> &alpha; is <mark>None</mark>:</strong></p>
                  <img class="equationclass" src="assets\images\equation/topk_1.PNG" alt="batch">
                </div>
                <div>
                  <p><strong>if <mark>min_score</mark> &alpha; is a value in [0, 1]:</strong></p>
                  <img class="equationclass" src="assets\images\equation/topk_2.PNG" alt="batch">
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_channels </strong>
                         – Size of each input sample. 
                      </p>
                      </li>
                      <li>
                        <p><strong>edge_score_method   </strong>
                         -  The function to apply to compute the edge score from raw edge scores. By default, this is the softmax over all incoming edges for each node. This function takes in a <mark>raw_edge_score</mark> tensor of shape <mark>[num_nodes]</mark>, an <mark>edge_index</mark> tensor and the number of nodes <mark>num_nodes</mark>, and produces a new tensor of the same size as <mark>raw_edge_score</mark> describing normalized edge scores. Included functions are <mark>EdgePooling.compute_edge_score_softmax()</mark>, <mark>EdgePooling.compute_edge_score_tanh()</mark>, and <mark>EdgePooling.compute_edge_score_sigmoid()</mark>.
                      </p>
                      </li>
                      <li>
                        <p><strong>dropout   </strong>
                         – The probability with which to drop edge scores during training.
                      </p>
                      </li>
                      <li>
                        <p><strong>add_to_edge_score  </strong>
                         – This is added to each computed edge score. Adding this greatly helps with unpool stability.
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p>class <span class="className" id="mempool">MemPooling</span> ( in_channels: int, out_channels: int, heads: int, num_clusters: int, tau: float = 1.0)</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#mem_pool">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="mem_pool" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR MEM_POOLING</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf 
                            from scipy.spatial import distance
                            from tf_dense_to_betch import tf_to_dense_batch 
                            class MemPooling(tf.keras.layers.Layer):
                                  def __init__(self , in_channels : int , out_channels : int ,heads : int ,
                                              num_clusters : int , tau:float = 1.):
                                          super(MemPooling , self).__init__()
                                          self.in_channels = in_channels
                                          self.out_channels = out_channels
                                          self.heads = heads
                                          self.num_clusters = num_clusters
                                          self.tau = tau

                                          self.k = tf.random.uniform((self.heads , self.num_clusters , self.in_channels) , minval= -1 , maxval= 1).numpy()
                                          self.conv = tf.keras.layers.Conv2D(num_clusters, kernel_size= 1 , use_bias= False)
                                          self.lin = tf.keras.layers.Dense(out_channels ,activation='linear' , use_bias= False)
                                  def call(self , x : tf.Tensor , batch : tf.Tensor = None , mask : tf.Tensor = None):
                                    if tf.shape(x).shape[0] <= 2 :
                                      x , mask = tf_to_dense_batch(x,batch)
                                    elif mask is None : 
                                      mask = tf.ones((1,tf.shape(x).numpy()[0]) , dtype= tf.bool).numpy()
                                    
                                    B , N , H , K = 1,tf.shape(x).numpy()[0] , self.heads , self.num_clusters
                                    dist = distance.cdist(tf.reshape(self.k , (H*K , -1)).numpy() , tf.cast(tf.reshape(x , (B * N ,- 1)) , dtype= tf.float32).numpy()) ** 2
                                    dist = tf.pow((1. + dist / self.tau),(-(self.tau + 1.0) / 2.0))
                                    dist = tf.reshape(dist , (H,K,B,N)).numpy()
                                  
                                    S = dist / tf.reduce_sum(dist).numpy()

                                    S = tf.nn.softmax(tf.squeeze(self.conv(S)),axis= -1).numpy()
                                    
                                    S = S * tf.reshape(mask ,(B,N,1)).numpy()
                                    return self.lin(S.T @ x).numpy()[0]

                                  def __repr__(self) -> str :
                                      return (f'{self.__class__.__name__}({self.in_channels}), ' 
                                              f'{self.out_channels}, heads={self.heads}, '
                                              f'num_clusters={self.num_clusters})')


                            # this is implementation 
                            # v = tf.constant([[3,4,3,5,9,3]])
                            # mm = MemPooling(6,1,1,4)
                            # print(mm(v))

                           </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Memory based pooling layer from <a href="https://arxiv.org/abs/2002.09518" target="_blank">"Memory-Based Graph Networks"</a> paper, which learns a coarsened graph representation based on soft cluster assignments</p>
                  <img class="equationclass" src="assets\images\equation/mempool_1.PNG" alt="batch">
                </div>
                <div>
                  <img class="equationclass" src="assets\images\equation/mempool_2.PNG" alt="batch">
                </div>
                <div>
                  <img class="equationclass" src="assets\images\equation/mempool_3.PNG" alt="batch">
                </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>in_channels </strong>
                         – Size of each input sample.
                      </p>
                      </li>
                      <li>
                        <p><strong>out_channels    </strong>
                         - Size of each output sample.
                      </p>
                      </li>
                      <li>
                        <p><strong>dropout   </strong>
                         – The probability with which to drop edge scores during training.
                      </p>
                      </li>
                      <li>
                        <p><strong>heads </strong>
                         – The number of heads H.
                      </p>
                      </li>
                      <li>
                        <p><strong>num_clusters  </strong>
                         –  number of clusters K per head.
                      </p>
                      </li>
                      <li>
                        <p><strong>tau  </strong>
                         – The temperature &tau;.
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                
                <div>
                  <pre class="classes">
                    <code>
                      <p><span class="className" id="maxpool">max_pool</span> ( cluster: tf.Tensor, data: Data, transform: Optional[Callable] = None)→ Data</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#max_pool">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="max_pool" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR MAX_POOL</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            from tf_scatter import scatter_max, scatter_sum
                            from typing import Optional
                            from pool import pool_batch, pool_edge, pool_pos
                            from consecutive import consecutive_cluster
                            from loop import add_self_loops
                            from typing import Optional, Tuple

                            def _max_pool_x(cluster, x, size: int = None):
                                if size is None:
                                    return tf.cast(scatter_max(x, cluster, x.get_shape()[0]),dtype= tf.int32)
                                else:
                                    return tf.cast(scatter_max(x, cluster,size),dtype= tf.int32)

                            def max_pool_x(cluster, x, batch, size: int = None):
                                if size is not None:
                                    batch_size = int(tf.reduce_max(batch))+1
                                    return _max_pool_x(cluster, x, batch_size * size), None
                                
                                cluster, perm = consecutive_cluster(cluster)
                                x = _max_pool_x(cluster, x)
                                batch = pool_batch(perm, batch)
                                
                                return x, batch

                            def max_pool(cluster, data, transform=None):
                                cluster, perm = consecutive_cluster(cluster)

                                x = None if data['x'] is None else _max_pool_x(cluster, data['x'])
                                index, attr = pool_edge(cluster, data['edge_index'], data['edge_attr'])
                                
                                batch = None if data['y'] is None else pool_batch(perm, data['y'])
                                pos = None if data['pos'] is None else pool_pos(cluster, data['pos'])
                                return pos

                                data = Batch(batch=batch, x=x, edge_index=index, edge_attr=attr, pos=pos)

                                if transform is not None:
                                    data = transform(data)

                                return data

                            

                            def max_pool_neighbor_x(data, flow='source_to_target'):
                                x, edge_index = data['x'], data['edge_index']

                                edge_index, _ = add_self_loops(edge_index, num_nodes=x.get_shape()[0])
                                row, col = edge_index
                                row, col = (row, col) if flow == 'source_to_target' else (col, row)
                                x_row = tf.constant(x.numpy()[row.numpy()])
                                data['x'] = scatter_max(x_row, col,tf.size(x))
                                return data

                           </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Each node in the same cluster will be represented as a single node. The maximum features of all nodes within the same cluster are used to define final node features, node positions are averaged, and edge indices are defined as the union of all node edge indices within the same cluster.</p>
                </div>
               
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>cluster  </strong>
                         – Cluster vector c &#8712; {0,...N-1}<sup>N</sup> , which assigns each node to a specific cluster.
                      </p>
                      </li>
                      <li>
                        <p><strong>data </strong>
                         - Graph data object.
                      </p>
                      </li>
                      <li>
                        <p><strong>transform    </strong>
                         –  A function/transform that takes in the coarsened and pooled <mark>Data</mark> object and returns a transformed version.
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p><span class="className" id="avgpool">avg_pool</span> ( cluster: tf.Tensor, data: Data, transform: Optional[Callable] = None)→ Data</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#avg_pool">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="avg_pool" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR AVG_POOL</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            
                            import tensorflow as tf
                            from tf_scatter import scatter_mean, scatter_sum
                            from typing import Optional
                            from pool import pool_batch, pool_edge, pool_pos
                            from consecutive import consecutive_cluster
                            from loop import add_self_loops
                            from typing import Optional, Tuple

                            def _avg_pool_x(cluster, x, size: Optional[int] = None):
                                if size is None:
                                    return tf.cast(scatter_mean(x, cluster, x.get_shape()[0]),dtype= tf.int32)
                                else:
                                    return tf.cast(scatter_mean(x, cluster,size),dtype= tf.int32)



                            def avg_pool_x(cluster, x, batch, size: Optional[int] = None):
                                if size is not None:
                                    batch_size = int(tf.reduce_max(batch))+1
                                    return _avg_pool_x(cluster, x, batch_size * size), None
                                
                                cluster, perm = consecutive_cluster(cluster)
                                x = _avg_pool_x(cluster, x)
                                batch = pool_batch(perm, batch)
                                
                                return x, batch



                            def avg_pool(cluster, data, transform=None):
                                cluster, perm = consecutive_cluster(cluster)

                                x = None if data['x'] is None else _avg_pool_x(cluster, data['x'])
                                index, attr = pool_edge(cluster, data['edge_index'], data['edge_attr'])
                                
                                batch = None if data['y'] is None else pool_batch(perm, data['y'])
                                pos = None if data['pos'] is None else pool_pos(cluster, data['pos'])
                                return pos

                                data = Batch(batch=batch, x=x, edge_index=index, edge_attr=attr, pos=pos)

                                if transform is not None:
                                    data = transform(data)

                                return data

                           ''' 
                            implementation
                            x = tf.constant([1,7,8])
                            c= tf.constant([1,2,3])
                            data = {'x' : tf.constant([1,2,3]),
                                  'edge_index':tf.constant([6,7,8]),
                                  'edge_attr':None,
                                  'y':tf.constant([1,7,8]),
                                  'pos':None}
                            avg_pool(x,data)
                            ''' 
                            def avg_pool_neighbor_x(data, flow='source_to_target'):
                                x, edge_index = data['x'], data['edge_index']

                                edge_index, _ = add_self_loops(edge_index, num_nodes=x.get_shape()[0])
                                row, col = edge_index
                                row, col = (row, col) if flow == 'source_to_target' else (col, row)
                                x_row = tf.constant(x.numpy()[row.numpy()])
                                data['x'] = scatter_mean(x_row, col,tf.size(x))
                                return data


                           </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div><p>
                  Final node features are defined by the average features of all nodes within the same cluster.
                </p></div>
               
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>cluster  </strong>
                         – Cluster vector c &#8712; {0,...N-1}<sup>N</sup> , which assigns each node to a specific cluster.
                      </p>
                      </li>
                      <li>
                        <p><strong>data </strong>
                         - Graph data object.
                      </p>
                      </li>
                      <li>
                        <p><strong>transform    </strong>
                         –  A function/transform that takes in the coarsened and pooled <mark>Data</mark> object and returns a transformed version.
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p><span class="className" id="maxpoolx">max_pool_x</span> ( cluster: tf.Tensor, x: tf.Tensor, batch: tf.Tensor, size: Optional[int] = None)→ Tuple[tf.Tensor, Optional[tf.Tensor]]</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#max_pool">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="max_pool" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR MAX_POOL</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            
                  

                           </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Max-Pools node features according to the clustering defined in <mark>cluster</mark>.</p>
                </div>
               
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>cluster  </strong>
                         – Cluster vector c &#8712; {0,...N-1}<sup>N</sup> , which assigns each node to a specific cluster.
                      </p>
                      </li>
                      <li>
                        <p><strong>X </strong>
                         - Node feature matrix X &#8712; R <sup>(N<sub>1</sub>,..,N<sub>B</sub>)&times;F</sup>
                      </p>
                      </li>
                      <li>
                        <p><strong>batch </strong>
                         – Batch vector b &#8712; {0,..,B-1}<sup>N</sup> , which assigns each node to a specific example.
                      </p>
                      </li>
                      <li>
                        <p><strong>Size </strong>
                         – The maximum number of clusters in a single example. This property is useful to obtain a batch-wise dense representation, e.g. for applying FC layers, but should only be used if the size of the maximum number of clusters per example is known in advance.
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p><span class="className" id="maxpoolxn">max_pool_neighbor_x</span> ( data: Data, flow: Optional[str] = 'source_to_target')→ Data</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#max_pool">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="max_pool" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR MAX_POOL</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                           </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                  <div><p>Max pools neighboring node features, where each feature in data['x'] is replaced by the feature value with the maximum value from the central node and its neighbors.</p></div>
                </div>
                <div>
                  <pre class="classes">
                    <code>
                      <p><span class="className" id="avgpool">avg_pool_x</span> ( cluster: tf.Tensor, x: tf.Tensor, batch: tf.Tensor, size: Optional[int] = None)→ Tuple[tf.Tensor, Optional[tf.Tensor]]</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#avg_poolx">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="avg_poolx" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR AVG_POOL</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            
                            import tensorflow as tf
                            from tf_scatter import scatter_mean, scatter_sum
                            from typing import Optional
                            from pool import pool_batch, pool_edge, pool_pos
                            from consecutive import consecutive_cluster
                            from loop import add_self_loops
                            from typing import Optional, Tuple

                            def _avg_pool_x(cluster, x, size: Optional[int] = None):
                                if size is None:
                                    return tf.cast(scatter_mean(x, cluster, x.get_shape()[0]),dtype= tf.int32)
                                else:
                                    return tf.cast(scatter_mean(x, cluster,size),dtype= tf.int32)



                            def avg_pool_x(cluster, x, batch, size: Optional[int] = None):
                                if size is not None:
                                    batch_size = int(tf.reduce_max(batch))+1
                                    return _avg_pool_x(cluster, x, batch_size * size), None
                                
                                cluster, perm = consecutive_cluster(cluster)
                                x = _avg_pool_x(cluster, x)
                                batch = pool_batch(perm, batch)
                                
                                return x, batch



                            def avg_pool(cluster, data, transform=None):
                                cluster, perm = consecutive_cluster(cluster)

                                x = None if data['x'] is None else _avg_pool_x(cluster, data['x'])
                                index, attr = pool_edge(cluster, data['edge_index'], data['edge_attr'])
                                
                                batch = None if data['y'] is None else pool_batch(perm, data['y'])
                                pos = None if data['pos'] is None else pool_pos(cluster, data['pos'])
                                return pos

                                data = Batch(batch=batch, x=x, edge_index=index, edge_attr=attr, pos=pos)

                                if transform is not None:
                                    data = transform(data)

                                return data

                           ''' 
                            implementation
                            x = tf.constant([1,7,8])
                            c= tf.constant([1,2,3])
                            data = {'x' : tf.constant([1,2,3]),
                                  'edge_index':tf.constant([6,7,8]),
                                  'edge_attr':None,
                                  'y':tf.constant([1,7,8]),
                                  'pos':None}
                            avg_pool(x,data)
                            ''' 
                            def avg_pool_neighbor_x(data, flow='source_to_target'):
                                x, edge_index = data['x'], data['edge_index']

                                edge_index, _ = add_self_loops(edge_index, num_nodes=x.get_shape()[0])
                                row, col = edge_index
                                row, col = (row, col) if flow == 'source_to_target' else (col, row)
                                x_row = tf.constant(x.numpy()[row.numpy()])
                                data['x'] = scatter_mean(x_row, col,tf.size(x))
                                return data


                           </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Average pools node features according to the clustering defined in <mark>cluster</mark>.</p>
                </div>
               
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>cluster  </strong>
                         – Cluster vector c &#8712; {0,...N-1}<sup>N</sup> , which assigns each node to a specific cluster.
                      </p>
                      </li>
                      <li>
                        <p><strong>X </strong>
                         - Node feature matrix X &#8712; R <sup>(N<sub>1</sub>,..,N<sub>B</sub>)&times;F</sup>
                      </p>
                      </li>
                      <li>
                        <p><strong>batch </strong>
                         – Batch vector b &#8712; {0,..,B-1}<sup>N</sup> , which assigns each node to a specific example.
                      </p>
                      </li>
                      <li>
                        <p><strong>Size </strong>
                         – The maximum number of clusters in a single example. This property is useful to obtain a batch-wise dense representation, e.g. for applying FC layers, but should only be used if the size of the maximum number of clusters per example is known in advance.
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p><span class="className" id="avgpool">avg_pool_neighbor_x</span> ( data: Data, flow: Optional[str] = 'source_to_target')→ Data</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#avg_poolxn">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="avg_poolxn" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR AVG_POOL</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                           </code>
                        </pre>
                        </div>
                        </div>
                  </div>
                </div>
                <div>
                  <p>Average pools neighboring node features, where each feature in data['x'] is replaced by the average feature values from the central node and its neighbors.</p>
                </div>
                <div>
                  <pre class="classes">
                    <code>
                      <p><span class="className" id="graclus">graclus</span> ( edge_index, weight: Optional[tf.Tensor] = None, num_nodes: Optional[int] = None)</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#graclus_co">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="graclus_co" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR GRACLUS</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf

                            try :
                              from tf_cluster import graclus_cluster
                            except:
                              graclus_cluster = None

                            def graclus(edge_index , weight : tf.Tensor = None , num_nodes : int = None):
                              
                                r"""A greedy clustering algorithm from the `"Weighted Graph Cuts without
                                    Eigenvectors: A Multilevel Approach" <http://www.cs.utexas.edu/users/
                                    inderjit/public_papers/multilevel_pami.pdf>`_ paper of picking an unmarked
                                    vertex and matching it with one of its unmarked neighbors (that maximizes
                                    its edge weight).
                                    The GPU algorithm is adapted from the `"A GPU Algorithm for Greedy Graph
                                    Matching" <http://www.staff.science.uu.nl/~bisse101/Articles/match12.pdf>`_
                                    paper.
                                    Args:
                                        edge_index (LongTensor): The edge indices.
                                        weight (Tensor, optional): One-dimensional edge weights.
                                            (default: :obj:`None`)
                                        num_nodes (int, optional): The number of nodes, i.e.
                                            :obj:`max_val + 1` of :attr:`edge_index`. (default: :obj:`None`)
                                    :rtype: :class:`LongTensor`
                                    """

                                if graclus_cluster is None : 
                                        raise ImportError("'graclus' requires 'tf_cluster'")

                                return graclus_cluster(edge_index[0] , edge_index[1] , weight , num_nodes)



                           </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div><p>
                  A greedy clustering algorithm from the <a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://www.cs.utexas.edu/users/inderjit/public_papers/multilevel_pami.pdf" target="_blank"> “Weighted Graph Cuts without Eigenvectors: A Multilevel Approach”</a> paper of picking an unmarked vertex and matching it with one of its unmarked neighbors (that maximizes its edge weight).
                </p></div>
               
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>edge_index  </strong>
                         – The edge indices.
                      </p>
                      </li>
                      <li>
                        <p><strong>weight  </strong>
                         - One-dimensional edge weights.
                      </li>
                      <li>
                        <p><strong>num_nodes </strong>
                         –  The number of nodes, i.e. <mark>max_val + 1</mark> of <mark>edge_index</mark>.
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p><span class="className" id="fps">fps</span> ( x: tf.Tensor, batch: Optional[tf.Tensor] = None, ratio: float = 0.5, random_start: bool = True)→ tf.Tensor</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#fps_co">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="fps_co" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR FPS</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            from typing import Optional


                            def fps(src, batch=None, ratio=None, random_start=True): 
                                    r: Optional[tf.Tensor] = None
                                    if ratio is None:
                                        r = tf.constant(0.5, dtype=src.dtype)
                                    elif isinstance(ratio, float):
                                        r = tf.cast(tf.constant(ratio), dtype=src.dtype)
                                    else:
                                        r = ratio
                                    assert r is not None

                                    if batch is not None:
                                        assert src.get_shape()[0] == tf.size(batch)
                                        batch_size = int(tf.reduce_max(batch)) + 1

                                        deg = tf.Variable(tf.zeros((batch_size,), dtype= tf.int32))
                                        ones = tf.ones((tf.size(batch).numpy(),),dtype=tf.int32)
                                        out = tf.compat.v1.scatter_add(deg,batch,ones)
                                      
                                
                                        ptr = tf.zeros([batch_size + 1], dtype = deg.dtype)
                                        tf.cumsum(deg, 0, bool(ptr[1:]))
                                    else:
                                        ptr = tf.constant([0, src.get_shape()[0]])

                                    return ptr



                           </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div><p>
                  A sampling algorithm from the <a href="https://arxiv.org/abs/1706.02413" target="_blank"> “PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space”</a> paper, which iteratively samples the most distant point with regard to the rest points.
                </p></div>
               
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>X  </strong>
                         – Node feature matrix X &isin; R<sup>N&times;F</sup>.
                      </p>
                      </li>
              
                      <li>
                        <p><strong>batch </strong>
                         – Batch vector b &isin; {0,..B-1}<sup>N</sup>, which assigns each node to a specific example.
                      </p>
                      </li>
                      <li>
                        <p><strong>ratio  </strong>
                         – Sampling ratio.
                      </p>
                      </li>
                      <li>
                        <p><strong>random_start </strong>
                         –  If set to <mark>False</mark>, use the first node in X as starting node.
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p><span class="className" id="knn">knn</span> ( x: tf.Tensor, y: tf.Tensor, k: int, batch_x: Optional[tf.Tensor] = None, batch_y: Optional[tf.Tensor] = None, cosine: bool = False, num_workers: int = 1)→ tf.Tensor</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#knn_co">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="knn_co" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR KNN</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            from typing import Optional
                            import tensorflow as tf
                            import scipy.spatial
                            import numpy as np

                            def knn(x: tf.Tensor, y: tf.Tensor, k: int,
                                    batch_x: Optional[tf.Tensor] = None,
                                    batch_y: Optional[tf.Tensor] = None, cosine: bool = False,
                                    num_workers: int = 1) -> tf.Tensor:
                                
                                if batch_x is None:
                                    batch_x = tf.zeros(x.get_shape()[0], dtype=tf.int64)

                                if batch_y is None:
                                    batch_y = tf.zeros(y.get_shape()[0], dtype=tf.int64)
                                
                                x = tf.reshape(x,[-1, 1]) if x.numpy().ndim == 1 else x
                                y = tf.reshape(y,[-1, 1]) if y.numpy().ndim == 1 else y
                                assert x.numpy().ndim == 2 and batch_x.numpy().ndim == 1
                                assert y.numpy().ndim == 2 and batch_y.numpy().ndim == 1
                                assert x.get_shape()[1] == y.get_shape()[1]
                                assert x.get_shape()[0] == tf.size(batch_x)
                                assert y.get_shape()[0] == batch_y.get_shape()[0]
                                
                                min_xy = min(tf.reduce_min(x).numpy(), tf.reduce_min(y).numpy())
                                x, y = x - min_xy, y - min_xy

                                max_xy = max(tf.reduce_max(x).numpy(), tf.reduce_max(y).numpy())
                                x, y, = x / max_xy, y / max_xy
                                
                                x = tf.concat([x, 2 * x.get_shape()[1] * tf.cast(tf.reshape(batch_x,[-1, 1]),dtype=x.dtype)], axis=-1)
                                y = tf.concat([y, 2 * y.get_shape()[1] * tf.cast(tf.reshape(batch_y,[-1, 1]),dtype=y.dtype)], axis=-1)
                                
                                tree = scipy.spatial.cKDTree(tf.stop_gradient(x).numpy())
                                dist, col = tree.query(
                                    tf.stop_gradient(y).cpu(), k=k, distance_upper_bound=x.get_shape()[1])
                                dist = tf.cast(dist,dtype = x.dtype).numpy()
                                col = tf.cast(col,dtype =tf.int64).numpy()
                              
                                row = np.tile(tf.reshape(tf.range(col.shape[0], dtype=tf.int64),[-1, 1]).numpy(),[1, k])
                                mask = tf.reshape(tf.constant(~np.isinf(dist)),[-1])
                                
                                row, col = tf.reshape(row,[-1]).numpy()[mask.numpy()], tf.reshape(tf.constant(col),[-1]).numpy()[mask.numpy()]

                                return tf.stack([row, col], axis=0)


                            def knn_graph(x, k, batch=None, loop=False, flow='source_to_target'):
                                assert flow in ['source_to_target', 'target_to_source']
                                row, col = knn(x, x, k if loop else k + 1, batch, batch)
                                row, col = (col, row) if flow == 'source_to_target' else (row, col)
                                if not loop:
                                    mask = row != col
                                    row, col = row[mask], col[mask]
                                return tf.stack([row, col], axis=0)

                           </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Finds for each element in <mark>y</mark> the <mark>k</mark> nearest points in <mark>x</mark>.</p>
                </div>
               
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>X  </strong>
                         – Node feature matrix X &isin; R<sup>N&times;F</sup>.
                      </p>
                      </li>
                      <li>
                        <p><strong>Y  </strong>
                         - Node feature matrix Y &isin; R<sup>M&times;F</sup>
                      </li>
                      <li>
                        <p><strong>batch_x </strong>
                         – Batch vector b &isin; {0,..B-1}<sup>N</sup>, which assigns each node to a specific example.
                      </p>
                      </li>
                      <li>
                        <p><strong>batch_y </strong>
                         – Batch vector b &isin; {0,..B-1}<sup>M</sup>, which assigns each node to a specific example.
                      </p>
                      </li>
                      <li>
                        <p><strong>K  </strong>
                         - The number of neighbors.
                      </li>
                      
                      <li>
                        <p><strong>cosine  </strong>
                         – If <mark>True</mark>, will use the cosine distance instead of euclidean distance to find nearest neighbors.
                      </p>
                      </li>
          
                      <li>
                        <p><strong>num_workers  </strong>
                         –  Number of workers to use for computation. Has no effect in case <mark>batch</mark> is not <mark>None</mark>, or the input lies on the GPU
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p><span class="className" id="knng">knn_graph</span> ( x: tf.Tensor, k: int, batch: Optional[tf.Tensor] = None, loop: bool = False, flow: str = 'source_to_target', cosine: bool = False, num_workers: int = 1)→ tf.Tensor</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#knn_co">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="knn_co" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR KNN_GRAPH</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            

                           </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>
                    Computes graph edges to the nearest <mark>k</mark> points.
                  </p>
                </div>
               
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>X  </strong>
                         – Node feature matrix X &isin; R<sup>N&times;F</sup>.
                      </p>
                      </li>
                      <li>
                        <p><strong>Y  </strong>
                         - Node feature matrix Y &isin; R<sup>M&times;F</sup>
                      </li>
                      <li>
                        <p><strong>batch_x </strong>
                         – Batch vector b &isin; {0,..B-1}<sup>N</sup>, which assigns each node to a specific example.
                      </p>
                      </li>
                      <li>
                        <p><strong>batch_y </strong>
                         – Batch vector b &isin; {0,..B-1}<sup>M</sup>, which assigns each node to a specific example.
                      </p>
                      </li>
                      <li>
                        <p><strong>K  </strong>
                         - The number of neighbors.
                      </li>
                      
                      <li>
                        <p><strong>cosine  </strong>
                         – If <mark>True</mark>, will use the cosine distance instead of euclidean distance to find nearest neighbors.
                      </p>
                      </li>
                      <li>
                        <p><strong>loop   </strong>
                         - The maximum number of neighbors to return for each element in <mark>y</mark>.
                      </li>
                      <li>
                        <p><strong>flow </strong>
                         – The flow direction when using in combination with message passing (<mark>"source_to_target"</mark> or <mark>"target_to_source"</mark>).
                      </p>
                      </li>
                    
                      <li>
                        <p><strong>num_workers  </strong>
                         –  Number of workers to use for computation. Has no effect in case <mark>batch</mark> is not <mark>None</mark>, or the input lies on the GPU
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p><span class="className" id="radius">radius</span> ( x: tf.Tensor, y: tf.Tensor, r: float, batch_x: Optional[tf.Tensor] = None, batch_y: Optional[tf.Tensor] = None, max_num_neighbors: int = 32, num_workers: int = 1)→ tf.Tensor</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#radius_co">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="radius_co" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR RADIUS</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            import numpy as np
                            import scipy.spatial
          
                            def radius(x, y, r, batch_x=None, batch_y=None, max_num_neighbors=32):
                                
                                if batch_x is None:
                                    batch_x = tf.zeros(x.get_shape()[0], dtype=tf.int64)
                            
                                if batch_y is None:
                                    batch_y = tf.zeros(y.get_shape()[0], dtype=tf.int64)
                                    
                                x = tf.reshape(x,[-1, 1]) if x.numpy().ndim == 1 else x
                                y = tf.reshape(y,[-1, 1]) if y.numpy().ndim == 1 else y
                                
                                assert x.numpy().ndim == 2 and batch_x.numpy().ndim == 1
                                assert y.numpy().ndim == 2 and batch_y.numpy().ndim == 1
                                assert x.get_shape()[1] == y.get_shape()[1]
                                assert x.get_shape()[0] == batch_x.get_shape()[0]
                                assert y.get_shape()[0] == batch_y.get_shape()[0]
                                
                                x = tf.cast(x, dtype= tf.float64)
                                y = tf.cast(y, dtype= tf.float64)
                                
                                x = tf.concat([x, 2 * r * tf.cast(tf.reshape(batch_x,[-1, 1]),dtype = x.dtype)], axis=-1)
                                y = tf.concat([y, 2 * r * tf.cast(tf.reshape(batch_y,[-1, 1]),dtype = y.dtype)], axis=-1)
                                
                                tree = scipy.spatial.cKDTree(tf.stop_gradient(x).numpy())
                                _, col = tree.query(
                                    tf.stop_gradient(y).numpy(), k=max_num_neighbors, distance_upper_bound=r + 1e-8)
                                
                                col = [tf.cast(tf.constant(c), dtype = tf.int64) for c in col]
                                 
                                row = [np.full_like(c,i) for i, c in enumerate(col)]
                                
                                
                                row, col = tf.concat(row, axis=0), tf.concat(col, axis=0)
                                    
                                mask = col < int(tree.n)
                                return tf.stack([row[mask], col[mask]], axis=0)
                            
                            
                            def radius_graph(x,
                                             r,
                                             batch=None,
                                             loop=False,
                                             max_num_neighbors=32,
                                             flow='source_to_target'):
                                assert flow in ['source_to_target', 'target_to_source']
                                row, col = radius(x, x, r, batch, batch, max_num_neighbors + 1)
                                row, col = (col, row) if flow == 'source_to_target' else (row, col)
                                if not loop:
                                    mask = row != col
                                    row, col = row[mask], col[mask]
                                return tf.stack([row, col], axis=0)

                           </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
               <div>
                <p>Finds for each element in <mark>y</mark> all points in <mark>x</mark> within distance <mark>r</mark>.</p>
               </div>
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                  
                        <li>
                          <p><strong>X  </strong>
                           – Node feature matrix X &isin; R<sup>N&times;F</sup>.
                        </p>
                        </li>
                        <li>
                          <p><strong>Y  </strong>
                           - Node feature matrix Y &isin; R<sup>M&times;F</sup>
                        </li>
                        <li>
                          <p><strong>batch_x </strong>
                           – Batch vector b &isin; {0,..B-1}<sup>N</sup>, which assigns each node to a specific example.
                        </p>
                        </li>
                        <li>
                          <p><strong>batch_y </strong>
                           – Batch vector b &isin; {0,..B-1}<sup>M</sup>, which assigns each node to a specific example.
                        </p>
                        </li>
                        <li>
                          <p><strong>r  </strong>
                           - The radius.
                        </li>
                        
                        <li>
                          <p><strong>max_num_neighbors  </strong>
                           – The maximum number of neighbors to return for each element in <mark>y</mark>
                        </p>
                        </li>
                      
                        <li>
                          <p><strong>num_workers  </strong>
                           –  Number of workers to use for computation. Has no effect in case <mark>batch</mark> is not <mark>None</mark>, or the input lies on the GPU
                        </p>
                        </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p><span class="className" id="radiusg">radius_graph</span> ( x: tf.Tensor, r: float, batch: Optional[tf.Tensor] = None, loop: bool = False, max_num_neighbors: int = 32, flow: str = 'source_to_target', num_workers: int = 1)→ tf.Tensor</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#radius_co">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="radius_co" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR RADIUS_GRAPH</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                          </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <p>Computes graph edges to all points within a given distance.</p>
                </div>
               
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>X  </strong>
                         – Node feature matrix X &isin; R<sup>N&times;F</sup>.
                      </p>
                      </li>
                      <li>
                        <p><strong>r  </strong>
                         - The radius.
                      </li>
                      <li>
                        <p><strong>batch </strong>
                         – Batch vector b &isin; {0,..B-1}<sup>N</sup>, which assigns each node to a specific example.
                      </p>
                      </li>
                     
                      <li>
                        <p><strong>loop   </strong>
                         - The maximum number of neighbors to return for each element in <mark>y</mark>.
                      </li>
                      
                      <li>
                        <p><strong>max_num_neighbors  </strong>
                         – The maximum number of neighbors to return for each element in <mark>y</mark>
                      </p>
                      </li>
                      <li>
                        <p><strong>flow </strong>
                         – The flow direction when using in combination with message passing (<mark>"source_to_target"</mark> or <mark>"target_to_source"</mark>).
                      </p>
                      </li>
                    
                      <li>
                        <p><strong>num_workers  </strong>
                         –  Number of workers to use for computation. Has no effect in case <mark>batch</mark> is not <mark>None</mark>, or the input lies on the GPU
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
                <div>
                  <pre class="classes">
                    <code>
                      <p><span class="className" id="near">nearest</span> ( x: tf.Tensor, y: tf.Tensor, batch_x: Optional[tf.Tensor] = None, batch_y: Optional[tf.Tensor] = None)→ tf.Tensor</p>
                    </code>
                    <span class="preSource" data-toggle="modal" data-target="#nearest_co">[source]</span>
                  </pre>
                </div>
                <div class="modal fade" id="nearest_co" tabindex="-1" role="dialog" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR NEAREST</strong></h5>
                        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                          <span aria-hidden="true">&times;</span>
                        </button>
                      </div>
                      <div class="modal-body">
                        <pre>
                          <code>  
                            import tensorflow as tf
                            import scipy.cluster

                            def nearest(x, y, batch_x=None, batch_y=None):
                                
                                if batch_x is None:
                                    batch_x = tf.zeros(x.get_shape()[0], dtype=tf.int64)

                                if batch_y is None:
                                    batch_y = tf.zeros(y.get_shape()[0], dtype=tf.int64)
                                    
                                x = tf.reshape(x,[-1, 1]) if x.numpy().ndim == 1 else x
                                y = tf.reshape(y,[-1, 1]) if y.numpy().ndim == 1 else y
                                
                                assert x.numpy().ndim == 2 and batch_x.numpy().ndim == 1
                                assert y.numpy().ndim == 2 and batch_y.numpy().ndim == 1
                                assert x.get_shape()[1] == y.get_shape()[1]
                                assert x.get_shape()[0] == batch_x.get_shape()[0]
                                assert y.get_shape()[0] == batch_y.get_shape()[0]
                                
                                # Rescale x and y.
                                min_xy = min(tf.reduce_min(x).numpy(), tf.reduce_min(y).numpy())
                                x, y = x - min_xy, y - min_xy

                                max_xy = max(tf.reduce_max(x).numpy(), tf.reduce_max(y).numpy())
                                x, y, = x / max_xy, y / max_xy
                                
                                x = tf.concat([x, 2 * x.get_shape()[1] * tf.cast(tf.reshape(batch_x,[-1, 1]),dtype = x.dtype)], axis=-1)
                                y = tf.concat([y, 2 * y.get_shape()[1] * tf.cast(tf.reshape(batch_y,[-1, 1]), dtype = y.dtype)], axis=-1)
                                
                                return tf.cast(tf.constant(scipy.cluster.vq.vq(tf.stop_gradient(x).cpu(),
                                                        tf.stop_gradient(y).cpu())[0]),dtype = tf.int64)



                           </code>
                        </pre>
                      </div>
                    </div>
                  </div>
                </div>
                <div><p>Clusters points in <mark>x</mark> together which are nearest to a given query point in <mark>y</mark>.</p></div>
               
                <dl class="field-list simple">
                  <dt class="field-odd">
                    <h4><strong>Parameters</strong></h4>
                  </dt>
                  <dd class="field-odd">
                    <ul class="simple_parameter">
                      <li>
                        <p><strong>X  </strong>
                         – Node feature matrix X &isin; R<sup>N&times;F</sup>.
                      </p>
                      </li>
                      <li>
                        <p><strong>Y  </strong>
                         - Node feature matrix Y &isin; R<sup>M&times;F</sup>
                      </li>
                      <li>
                        <p><strong>batch_x </strong>
                         – Batch vector b &isin; {0,..B-1}<sup>N</sup>, which assigns each node to a specific example.
                      </p>
                      </li>
                      <li>
                        <p><strong>batch_y </strong>
                         – Batch vector b &isin; {0,..B-1}<sup>M</sup>, which assigns each node to a specific example.
                      </p>
                      </li>
                    </ul>
                  </dd>
                </dl>
            
          </section>

          <hr class="divider">

          <!-- UnPooling layers
		============================ -->
          <section id="idocs_un_pool">
            <h2>UnPooling Layers</h2>
            <p class="lead">
            <ul class="simple">
              <li>
                <p>
                  <a class="reference internal" href="#knninter">Knn Interpolate</a>
                </p>
              </li>
              <div>
                <pre class="classes">
                  <code>
                    <p><span class="className" id="knninter">knn_interpolate Norm</span> ( x: tf.Tensor, pos_x: tf.Tensor, pos_y: tf.Tensor, batch_x: Optional[tf.Tensor] = None, batch_y: Optional[tf.Tensor] = None, k: int = 3, num_workers: int = 1)</p>
                  </code>
                  <span class="preSource" data-toggle="modal" data-target="#knninco">[source]</span>
                </pre>
              </div>
              <div class="modal fade" id="knninco" tabindex="-1" role="dialog" aria-hidden="true">
                <div class="modal-dialog modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <h5 class="modal-title" id="exampleModalLongTitle"><strong>SOURCE CODE FOR KNN_INTERPOLATE</strong></h5>
                      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                      </button>
                    </div>
                    <div class="modal-body">
                      <pre>
                        <code>
                          import tensorflow as tf
                          from knn import knn
                          from Scatter import scatter_add
                       
                          def knn_interpolate(x: tf.Tensor, pos_x: tf.Tensor, pos_y: tf.Tensor,
                                              batch_x= None, batch_y = None,
                                              k: int = 3, num_workers: int = 1):
                                  assign_index = knn(pos_x, pos_y, k, batch_x=batch_x, batch_y=batch_y,
                                                         num_workers=num_workers)
                                  y_idx, x_idx = assign_index[0], assign_index[1]
                                  diff = pos_x.numpy()[x_idx.numpy()] - pos_y.numpy()[y_idx.numpy()]
                                  squared_distance = tf.reduce_sum((diff * diff), axis=-1, keepdims=True)
                                  squared_distance = tf.cast(squared_distance, dtype = tf.float32)
                                  weights = 1.0 / tf.clip_by_value(squared_distance, clip_value_min=1e-16, clip_value_max= 1e+16)
                                  res = x.numpy()[x_idx.numpy()]* weights
                                  y = scatter_add(res, index=y_idx, dim=0, dim_size=pos_y.get_shape()[0])
                                  y = y / scatter_add(weights, dim=0, index=y_idx)
                                  return y
                                </code>
                      </pre>
                    </div>
                  </div>
                </div>
              </div>
              <div>
                <p>The k-NN interpolation from the <a href="https://arxiv.org/abs/1706.02413" target="_blank"> “PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space”</a> paper. For each point <strong>y</strong> with position <strong>P(y)</strong>, its interpolated features <strong>F(y)</strong> are given by</p>
                <img class="equationclass" src="assets\images\equation/knn_inter.PNG" alt="batch">
              </div>
              <dl class="field-list simple">
                <dt class="field-odd">
                  <h4><strong>Parameters</strong></h4>
                </dt>
                <dd class="field-odd">
                  <ul class="simple_parameter">
                    <li>
                      <p><strong>X</strong>
                       – Node feature matrix X &isin; R<sup>N&times;F</sup>.
                    </p>
                    </li>
                    <li>
                      <p><strong>pos_x </strong>
                       – Node position matrix &isin; R<sup>N&times;d</sup>.
                    </p>
                    </li>
                    <li>
                      <p><strong>pos_y </strong>
                       – Node position matrix &isin; R<sup>M&times;d</sup>.
                    </p>
                    </li>
                    <li>
                      <p><strong>batch_x  </strong>
                       – Batch vector b<sub>x</sub> &isin; {0,..B-1}<sup>N</sup>, which assigns node from X to a specific example. 
                    </p>
                    </li>
                    <li>
                      <p><strong>batch_y </strong>
                       –  Batch vector b<sub>y</sub> &isin; {0,..B-1}<sup>N</sup>,which assigns node from Y to a specific example.
                    </p>
                    </li>
                    <li>
                      <p><strong>k </strong>
                        – Number of neighbors.   
                    </p>
                    </li>
                    <li>
                      <p><strong>num_workers </strong>
                        – Number of workers to use for computation. Has no effect in case <mark>batch_x</mark> or <mark>batch_y</mark> is not <mark>None</mark>, or the input lies on the GPU.   
                    </p>
                    </li>
                  </ul>
                </dd>
              </dl>
            
          </section>

          <hr class="divider">

        </div>
      </div>

    </div>
    <!-- Content end -->

    <!-- Footer
  ============================ -->
    <footer id="footer" class="section bg-dark footer-text-light">
      <div class="container">
        <ul class="social-icons social-icons-lg social-icons-muted justify-content-center mb-3">
          
          <li><a data-toggle="tooltip" href="https://github.com/Ritaj19Zamel/Layers" target="_blank" title=""
              data-original-title="GitHub"><i class="fab fa-github"></i></a></li>
        </ul>
        <p class="text-2 text-center mb-0">Design &amp; Develop by <a class="btn-link" target="_blank"
            href="#">Binary Brains Team</a>.</p>
      </div>
    </footer>
    <!-- Footer end -->

  </div>
  <!-- Document Wrapper end -->

  <!-- Back To Top -->
  <a id="back-to-top" data-toggle="tooltip" title="Back to Top" href="javascript:void(0)"><i
      class="fa fa-chevron-up"></i></a>

  <!-- JavaScript
============================ -->
  <script src="assets/vendor/jquery/jquery.min.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <!-- Highlight JS -->
  <script src="assets/vendor/highlight.js/highlight.min.js"></script>
  <!-- Easing -->
  <script src="assets/vendor/jquery.easing/jquery.easing.min.js"></script>
  <!-- Magnific Popup -->
  <script src="assets/vendor/magnific-popup/jquery.magnific-popup.min.js"></script>
  <!-- Custom Script -->
  <script src="assets/js/theme.js"></script>
</body>

</html>
